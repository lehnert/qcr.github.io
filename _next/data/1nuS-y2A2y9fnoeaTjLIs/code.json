{"pageProps":{"listData":"[{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>Using the Abstract Map in a 2D Stage simulation</h1>\\n<p><img src=\\\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\\\" alt=\\\"2D Stage simulation with with simulated tags\\\"></p>\\n<p>Package contains everything needed to simulate the zoo experiments performed in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The package includes:</p>\\n<ul>\\n<li>world &amp; launch files for a stage simulation of the GP-S11 environment on QUT's Gardens Point campus</li>\\n<li>a tool for creating simulated tags in an environment &amp; saving them to file,</li>\\n<li>launch &amp; config files for using the move_base navigation stack with gmapping to explore unseen simulated environments</li>\\n</ul>\\n<h2>Installing the abstract map simulator</h2>\\n<p><em>Note: this is just the simulator; to use the abstract map with the simulator please make sure you use the <a href=https://github.com/btalb/abstract_map>abstract_map</a> package</em></p>\\n<p>Clone the repo &amp; install all Python dependencies:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/btalb/abstract_map_simulator\\npip install -r abstract_map_simulator/requirements.txt\\n</code></pre>\\n<p>Add the new package to your ROS workspace at <code class=\\\"language-none\\\">&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/\\n</code></pre>\\n<p>Install all of the listed ROS dependencies, and build the package:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">cd &lt;ROS_WS&gt;/src/\\nrosdep install abstract_map_simulator\\ncd &lt;ROS_WS&gt;\\ncatkin_make\\n</code></pre>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"2D Simulator for Zoo Experiments\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_simulator\",\"src\":\"/content/human_cues/abstract-map-simulator.md\",\"id\":\"abstract-map-simulator\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\"},{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>The Abstract Map - using symbols to navigate</h1>\\n<p><img src=\\\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\\\" alt=\\\"The abstract map in action\\\"></p>\\n<p>This repository provides the implementation of the abstract map used in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The implementation, done in Python, includes the following features:</p>\\n<ul>\\n<li>a novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, &amp; collision models)</li>\\n<li>a visualiser &amp; text-based commentator for introspection of your navigation system (both shown in videos on the <a href=\\\"https://btalb.github.io/abstract_map/\\\">repository website</a>)</li>\\n<li>easy ROS bindings for getting up &amp; running in simulation or on a real robot</li>\\n<li>tag readers &amp; interpreters for extracting symbolic spatial information from <a href=\\\"http://wiki.ros.org/apriltag_ros\\\">AprilTags</a></li>\\n<li>configuration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">the paper</a> for further details)</li>\\n<li>serialisation methods for passing an entire abstract map state between machines, or saving to file</li>\\n</ul>\\n<p>Please see our other related repositories for further resources, and related parts of the abstract map studies:</p>\\n<ul>\\n<li><strong><a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a>:</strong> all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT</li>\\n<li><strong><a href=https://github.com/btalb/abstract_map_app>abstract_map_app</a>:</strong> mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags &amp; present the mapped symbolic spatial information in real time)</li>\\n</ul>\\n<h2>Getting up &amp; running with the abstract map</h2>\\n<p><em>Note: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the <a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a> package</em></p>\\n<h3>Setting up your environment</h3>\\n<p>Clone the repo &amp; install all Python dependencies:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/btalb/abstract_map\\npip install -r abstract_map/requirements.txt\\n</code></pre>\\n<p>Add the new package to your ROS workspace at <code class=\\\"language-none\\\">&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/\\n</code></pre>\\n<p>Install all of the listed ROS dependencies, and build the package:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">cd &lt;ROS_WS&gt;/src/\\nrosdep install abstract_map\\ncd &lt;ROS_WS&gt;\\ncatkin_make\\n</code></pre>\\n<h3>Running the Zoo experiments</h3>\\n<p>Start the experiment (this will try &amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">roslaunch abstract_map experiment.launch\\n</code></pre>\\n<p><em>(please see <a href=https://github.com/btalb/abstract_map_simulator/issues/1>this issue</a> for details if you get the spam of TF based errors... which probably shouldn't even be errors... )</em></p>\\n<p>In another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map hierarchy_publisher\\n</code></pre>\\n<p>This will use the hierarchy available in <code class=\\\"language-none\\\">./experiments/zoo_hierarchy.xml</code> by default. Feel free to make your own if you would like to do different experiments.</p>\\n<p>Start the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map visualiser\\n</code></pre>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map visualiser --dark\\n</code></pre>\\n<p><img src=\\\"/_next/static/images/abstract_map_light_vs_dark-ae93c3e7b8419b56719b5d876dd150f4.png\\\" alt=\\\"Visualise the abstract map with dark or light colours\\\"></p>\\n<p>Finally, start the abstract map with a goal, and watch it attempt to complete the navigation task:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">roslaunch abstract_map abstract_map.launch goal:=Lion\\n</code></pre>\\n<p>If you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in &quot;observe mode&quot;.</p>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"Abstract Map (Python)\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map\",\"src\":\"/content/human_cues/abstract-map.md\",\"id\":\"abstract-map\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"},{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>App for the Human vs Abstract Map Zoo Experiments</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\\\"><source src=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>This repository contains the mobile application used by human participants in the zoo experiments described in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The app, created with Android Studio, includes the following:</p>\\n<ul>\\n<li>opening screen for users to select experiment name &amp; goal location</li>\\n<li>live display of the camera to help users correctly capture a tag</li>\\n<li>instant visual feedback when a tag is detected, with colouring to denote whether symbolic spatial information is not the goal (red), navigation information (orange), or the goal (green)</li>\\n<li>experiment definitions &amp; tag mappings are creatable via the same XML style used in the <a href=https://github.com/btalb/abstract_map>abstract_map</a> package</li>\\n<li>integration with the <a href=https://github.com/AprilRobotics/apriltag>native C AprilTags</a> using the Android NDK</li>\\n</ul>\\n<h2>Developing &amp; producing the app</h2>\\n<p>The project should be directly openable using Android Studio.</p>\\n<p>Please keep in mind that this app was last developed in 2019, and Android Studio often introduces minor breaking changes with new versions. Often you will have to tweak things like Gradle versions / syntax etc. to get a project working with newer versions. Android Studio is very good though with pointing out where it sees errors and offering suggestions for how to resolve them.</p>\\n<p>Once you have the project open, you should be able to compile the app and load it directly onto a device without issues.</p>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"Android App for Human Participants\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_app\",\"src\":\"/content/human_cues/abstract-map-app.md\",\"id\":\"abstract-map-app\",\"image_position\":\"center\",\"image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\",\"_image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\"},{\"content\":\"<p><strong>NOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation (although it can be installed independently through pip if desired). For a working BenchBot system, please install the BenchBot software stack by following the instructions <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>\\n<h1>BenchBot Add-ons Manager</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/8ef442bbcf06f5d9f4a4bd553bd28212.jpg\\\"><source src=\\\"/_next/static/gifs/8ef442bbcf06f5d9f4a4bd553bd28212.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>The BenchBot Add-ons Manager allows you to use BenchBot with a wide array of additional content, and customise your installation to suite your needs. Semantic Scene Understanding not your thing? Install the Semantic Question Answering add-ons instead. Want to create your own content? Write some basic YAML files to make your own add-ons. Need to re-use existing content? Simply include a dependency on that add-on. Add-ons are all about making BenchBot whatever you need it to beâ€”build a BenchBot for your research problems, exactly as you need it.</p>\\n<p>Add-ons come in a variety of types. Anything that you may need to customise for your own experiments or research, should be customisable through an add-on. If not, let us know, and we'll add more add-on enabled functionality to BenchBot!</p>\\n<p>The list of currently supported types of add-ons are:</p>\\n<ul>\\n<li><strong>batches</strong>: a list of environments used for repeatable evaluation scores with the <code class=\\\"language-none\\\">benchbot_batch</code> script.</li>\\n<li><strong>environments</strong>: simulated or real world environments that a task can be performed in, with a robot. Only <a href=\\\"https://developer.nvidia.com/Isaac-sim\\\">Isaac Sim</a> simulation is currently supported, but there is capacity to support other simulators. Please get in contact if you'd like to see another simulator in BenchBot!</li>\\n<li><strong>evaluation_methods</strong>: a method for evaluating a set of formatted results, against a corresponding ground truth, and producing scores describing how well a result performed a given task.</li>\\n<li><strong>formats</strong>: formalisation of a format for results or ground truth data, including helper functions.</li>\\n<li><strong>ground_truths</strong>: ground truth data in a declared format, about a specific environment. Environments can have many different types of ground truths depending on what different tasks require.</li>\\n<li><strong>robots</strong>: a robot definition declaring the communication channels available to the BenchBot ecosystem. Both simulated and real world robots are supported, they just need to run ROS.</li>\\n<li><strong>tasks</strong>: a task is a definition of something we want a robot to do, including what observations and actions it has available, and how results should be reported.</li>\\n</ul>\\n<p>See the sections below for details of how to interact with installed add-ons, how to create your own add-ons, and formalisation of what's required in an add-on.</p>\\n<h2>Installing and using the add-ons manager</h2>\\n<p>In general, you won't use the add-ons manager directly. Instead you interact with the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a>, which uses the add-ons manager to manage and access add-ons.</p>\\n<p>The manager is a Python package if you do find you want to use it directly, and installable with pip. Run the following in the root directory where the repository was cloned:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ pip install .\\n</code></pre>\\n<p>The manager can then be imported and used to manage installation, loading, accessing, processing, and updating of add-ons. Some samples of supported functionality are shown below:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_addons <span class=\\\"token keyword\\\">import</span> manager <span class=\\\"token keyword\\\">as</span> bam\\n\\n<span class=\\\"token comment\\\"># Check if example with 'name' = 'hello_scd' exists</span>\\nbam<span class=\\\"token punctuation\\\">.</span>exists<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'examples'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'name'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'hello_scd'</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Find all installed environments</span>\\nbam<span class=\\\"token punctuation\\\">.</span>find_all<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'environments'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Get a list of the names for all installed tasks</span>\\nbam<span class=\\\"token punctuation\\\">.</span>get_field<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'tasks'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'name'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Get a list of (name, variant) pairs for all installed environments</span>\\nbam<span class=\\\"token punctuation\\\">.</span>get_fields<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'environments'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">[</span><span class=\\\"token string\\\">'name'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'variant'</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Find a robot with 'name' = 'carter'</span>\\nbam<span class=\\\"token punctuation\\\">.</span>get_match<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'robots'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'name'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'carter'</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Get the 'results_format' value for the task called 'scd:passive:ground_truth'</span>\\nbam<span class=\\\"token punctuation\\\">.</span>get_value_by_name<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'tasks'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'scd:passive:ground_truth'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'results_format'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Load YAML data for all installed ground truths</span>\\nbam<span class=\\\"token punctuation\\\">.</span>load_yaml_list<span class=\\\"token punctuation\\\">(</span>bam<span class=\\\"token punctuation\\\">.</span>find_all<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'ground_truths'</span><span class=\\\"token punctuation\\\">,</span> extension<span class=\\\"token operator\\\">=</span><span class=\\\"token string\\\">'json'</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Install a list of comma-separated add-ons</span>\\nbam<span class=\\\"token punctuation\\\">.</span>install_addons<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'benchbot-addons/ssu,benchbot-addons/sqa'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Install a specific add-on (&amp; it's dependencies)</span>\\nbam<span class=\\\"token punctuation\\\">.</span>install_addon<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'tasks_ssu'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Print the list of currently installed add-ons, &amp; officially available add-ons</span>\\nbam<span class=\\\"token punctuation\\\">.</span>print_state<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Uninstall all add-ons</span>\\nbam<span class=\\\"token punctuation\\\">.</span>remove_addons<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Uninstall a string separated list of add-ons</span>\\nbam<span class=\\\"token punctuation\\\">.</span>remove_addon<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'benchbot-addons/ssu,benchbot-addons/sqa'</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<h2>How to add your own add-ons</h2>\\n<p>There are two different types of add-ons: 'official' add-ons and third-party add-ons.</p>\\n<p>'Official' are add-ons that we've verified, and are stored in our <a href=https://github.com/benchbot-addons>benchbot-addons</a> GitHub organisation. You can get a full list of official add-ons through the <code class=\\\"language-none\\\">manager.official_addons()</code> helper function, or <code class=\\\"language-none\\\">benchbot_install --list-addons</code> script in the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a>.</p>\\n<p>Third-party add-ons only differ in that we haven't looked at them, and they can be hosted anywhere on GitHub you please.</p>\\n<p>Creating all add-ons is exactly the same process, the only difference is whether the repository is inside or outside of the <a href=https://github.com/benchbot-addons>benchbot-addons</a> GitHub organisation:</p>\\n<ol>\\n<li>Create a new GitHub repository</li>\\n<li>Add folders corresponding to the type of content your add-ons provide (i.e. an environments add-on has an <code class=\\\"language-none\\\">environments</code> directory at the root).</li>\\n<li>Add YAML / JSON files for your content, and make sure they match the corresponding format specification from the section below</li>\\n<li>Add in any extra content your add-on may require: Python files, simulator binaries, images, etc. (if your add-on gets too big for a Git repository, you can zip the content up, host it somewhere, and use the <code class=\\\"language-none\\\">.remote</code> metadata file described in the next section)</li>\\n<li>Decide if your add-on is dependent on any others, and declare any dependencies in a <code class=\\\"language-none\\\">.dependencies</code> file</li>\\n<li>Push everything up to git on your default branch</li>\\n</ol>\\n<p><em><strong>Note:</strong> it's a good idea to only include one type of add-on per repository as it makes your add-on package more usable for others. It's not a hard rule though, so feel free to add multiple folders to your add-on if you require.</em></p>\\n<p>Feel free to have a look at any of the <a href=https://github.com/benchbot-addons>official add-ons</a> for help and examples of how to work with add-ons.</p>\\n<h2>Add-ons format specification</h2>\\n<p>Here are the technical details of what's expected in add-on content. The BenchBot system will assume these specifications are adhered to, and errors can be expected if you try to use add-ons that don't match the specifications.</p>\\n<p>An add-on package has the following structure (technically none of the files are required, they just determine what functionality your add-on includes):</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Filename</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">.dependencies</code></td>\\n<td>A list of add-on packages that must be installed with this package. Packages are specified by their GitHub identifier (i.e. <code class=\\\"language-none\\\">github_username/repository_name</code>), with one per line</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">.dependencies-python</code></td>\\n<td>A list of Python dependencies for your add-on. Syntax for file is exactly the same as <a href=\\\"https://pip.pypa.io/en/stable/user_guide/#requirements-files\\\"><code class=\\\"language-none\\\">requirements.txt</code></a> files.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">.remote</code></td>\\n<td>Specifies content that should be installed from a remote URL, rather than residing in this repository. A remote resource is specified as a URL and target directory separated by a space. One resource is specified per line. The add-ons manager will fetch the URL specified, and extract the contents to the target directory (e.g. <code class=\\\"language-none\\\">http://myhost/my_content.zip environments</code>)</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">&lt;directory&gt;/</code></td>\\n<td>Each named directory corresponds to an add-on type described below. The directory will be ignored if its name doesn't exactly match any of those below.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Batch add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">batches</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">batches/my_batch.yaml</code>).</p>\\n<p>The following keys are supported for batch add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this batch (must be unique!).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'environments'</code></td>\\n<td>Yes</td>\\n<td>A list of environment strings of the format <code class=\\\"language-none\\\">'name':'variant'</code> (e.g. <code class=\\\"language-none\\\">'miniroom:1'</code>).</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Environment add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">environments</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">environments/my_environment.yaml</code>).</p>\\n<p>The following keys are supported for environment add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this environment's name (the <code class=\\\"language-none\\\">('name', 'variant')</code> pair must be unique!).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'variant'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this environment's variant (the <code class=\\\"language-none\\\">('name', 'variant')</code> pair must be unique!).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'type'</code></td>\\n<td>Yes</td>\\n<td>A string describing the type of this environment (<code class=\\\"language-none\\\">'sim_unreal'</code> &amp; <code class=\\\"language-none\\\">'real'</code> are the only values currently used).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'map_path'</code></td>\\n<td>Yes</td>\\n<td>A path to the map for this environment, which will be used by either the simulator or real world system to load the environment.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'start_pose'</code></td>\\n<td>Yes</td>\\n<td>The start pose of the robot that will be provided to users through the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>. The pose is specified as a list of 7 numbers: quarternion_x, quarternion_y, quarternion_z, quarternion_w, position_x, position_y, position_z. This must be accurate!</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'trajectory_poses'</code></td>\\n<td>No</td>\\n<td>A list of poses for the robot to traverse through in order. Each pose is a list of 7 numbers: quarternion_x, quarternion_y, quarternion_z, quarternion_w, position_x, position_y, position_z. This environment won't be usable for tasks that use the <code class=\\\"language-none\\\">'move_next'</code> action if this parameter isn't provided.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'robots'</code></td>\\n<td>No</td>\\n<td>A list of supported names for robot that are supported in this environment. If this list isn't included, all robots with the same <code class=\\\"language-none\\\">'type'</code> as this environment will be able to run.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'object_labels'</code></td>\\n<td>No</td>\\n<td>A list of labels for the objects that exist in the scene. Can be used with simulated sensors like segmentation sensors.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Evaluation method add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">evaluation_methods</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">evaluation_methods/my_evaluation_method.yaml</code>).</p>\\n<p>The following keys are supported for evaluation method add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this evaluation method (must be unique!)</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'valid_result_formats'</code></td>\\n<td>Yes</td>\\n<td>List of strings denoting results formats supported by the evaluation method. Ideally these format definitions should also be installed.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'valid_ground_truth_formats'</code></td>\\n<td>Yes</td>\\n<td>List of strings denoting ground truth formats supported by the evaluation method. Ideally these format definitions should also be installed.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'functions'</code></td>\\n<td>Yes</td>\\n<td>Dictionary of named functions provided by the evaluation method. The named methods are key value pairs where the key is the function name, and the value is a string describing how the function can be imported with Python. For example, <code class=\\\"language-none\\\">evaluate: &quot;omq.evaluate_method&quot;</code> declares a function called <code class=\\\"language-none\\\">'evaluate'</code> that is imported via <code class=\\\"language-none\\\">from omq import evaluate_method</code>. Likewise <code class=\\\"language-none\\\">&quot;omq.submodule.combine_method&quot;</code> translates to <code class=\\\"language-none\\\">from omq.submodule import combine_method</code>. See below for the list of functions expected for evaluation methods.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'description'</code></td>\\n<td>No</td>\\n<td>A string describing what the evaluation method is and how it works. Should be included if you want users to understand where your method can be used.</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>Evaluation methods expect the following named functions:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Name</th>\\n<th>Signature</th>\\n<th>Usage</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'evaluate'</code></td>\\n<td><code class=\\\"language-none\\\">fn(dict: results, list: ground_truths) -&gt; dict</code></td>\\n<td>Evaluates the performance using a <code class=\\\"language-none\\\">results</code> dictionary, and returns a dictionary of containing the scores. It also takes a list of dictionaries containing each ground truth that will be used in evaluation.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'combine'</code></td>\\n<td><code class=\\\"language-none\\\">fn(list: scores) -&gt; dict</code></td>\\n<td>Takes a list of <code class=\\\"language-none\\\">scores</code> dictionaries, and returns an aggregate score. If this method isn't declared, <a href=https://github.com/qcr/benchbot_eval><code class=\\\"language-none\\\">benchbot_eval</code></a> won't return a summary score.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Format definition add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">formats</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">formats/my_format.yaml</code>).</p>\\n<p>The following keys are supported for format add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this format (must be unique!)</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'functions'</code></td>\\n<td>Yes</td>\\n<td>Dictionary of named functions for use with this format. The named methods are key-value pairs where the key is the function name, and the value is a string describing how the function can be imported with Python. For example, <code class=\\\"language-none\\\">create: &quot;object_map.create_empty&quot;</code> declares a function called <code class=\\\"language-none\\\">'create'</code> that is imported via <code class=\\\"language-none\\\">from object_map import create_empty</code>. Likewise <code class=\\\"language-none\\\">&quot;object_map.submodule.validate&quot;</code> translates to <code class=\\\"language-none\\\">from object_map.submodule import validate</code>. See below for the list of functions expected for format definitions.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'description'</code></td>\\n<td>No</td>\\n<td>A string describing what the format is and how it works. Should be included if you want users to understand what your format is supposed to capture.</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>Format definitions expect the following named functions:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Name</th>\\n<th>Signature</th>\\n<th>Usage</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'create'</code></td>\\n<td><code class=\\\"language-none\\\">fn() -&gt; dict</code></td>\\n<td>Function that returns an empty instance of this format. As much as possible should be filled in to make it easy for users to create valid instances (especially when a format is used for results).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'validate'</code></td>\\n<td><code class=\\\"language-none\\\">fn(dict: instance) -&gt; None</code></td>\\n<td>Takes a proposed <code class=\\\"language-none\\\">instance</code> of this format and validates whether it meets the requirements. Will typically use a series of assert statements to confirm fields are valid.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Ground truth add-ons</h3>\\n<p>A JSON file, that must exist in a folder called <code class=\\\"language-none\\\">ground_truths</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">ground_truths/my_ground_truth.json</code>).</p>\\n<p>The following keys are supported for ground truth add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'environment'</code></td>\\n<td>Yes</td>\\n<td>A dictionary containing the definition data for the ground truth's reference environment. The data in this field should be a direct copy of an environment add-on.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'format'</code></td>\\n<td>Yes</td>\\n<td>A dictionary containing the definition data for the ground truth's format. The data in this field should be a direct copy of a format definition add-on.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'ground_truth'</code></td>\\n<td>Yes</td>\\n<td>A valid instance of the format described by the <code class=\\\"language-none\\\">'format'</code> field. This is where your actual ground truth data should be stored.</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>A lot of these keys should be copied from other valid definitions. Please see the <code class=\\\"language-none\\\">GroundTruthCreator</code> helper class in <a href=https://github.com/qcr/benchbot_eval>BenchBot Evaluation</a> for assistance in creating valid ground truths.</p>\\n<h3>Robot add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">robots</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">robots/my_robot.yaml</code>).</p>\\n<p>The following keys are supported for robot add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this robot (must be unique!).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'type'</code></td>\\n<td>Yes</td>\\n<td>A string describing the type of this robot (<code class=\\\"language-none\\\">'sim_unreal'</code> &amp; <code class=\\\"language-none\\\">'real'</code> are the only values currently used).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'address'</code></td>\\n<td>Yes</td>\\n<td>A string for the address where a running <a href=https://github.com/qcr/benchbot_robot_controller>BenchBot Robot Controller</a> can be accessed (e.g. <code class=\\\"language-none\\\">'localhost:10000'</code>)</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'global_frame'</code></td>\\n<td>Yes</td>\\n<td>The name of the global TF frame. All poses reported by the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a> will be with respect to this frame.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'robot_frame'</code></td>\\n<td>Yes</td>\\n<td>The name of the robot's TF frame.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'poses'</code></td>\\n<td>Yes</td>\\n<td>A list of named poses that this robot provides. This list of poses will be available in observations provided by the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'start_cmds'</code></td>\\n<td>Yes</td>\\n<td>A list of commands describing how to start the robot (this will often include the simulator). The commands will be run in parallel, and executed via <code class=\\\"language-none\\\">bash -c '&lt;your_command_string&gt;'</code></td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'connections'</code></td>\\n<td>Yes</td>\\n<td>A dictionary of connections that your robot makes available to the BenchBot ecosystem. The name of the key-value pair is important, and should follow the recommendations provided on standard channels in the <a href=https://github.com/qcr/benchbot_api>BenchBot API documentation</a>. A description of connection definitions is provided below.</td>\\n</tr>\\n</tbody>\\n</table>\\n<p>Connections are the lifeblood of interaction between BenchBot and robot platforms. They are defined by named entries, with the following fields:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'connection'</code></td>\\n<td>Yes</td>\\n<td>Connection type string, used by the <a href=https://github.com/qcr/benchbot_robot_controller>BenchBot Robot Controller</a>. Supported values are <code class=\\\"language-none\\\">'api_to_ros'</code> (used for actions), <code class=\\\"language-none\\\">'ros_to_api'</code> (used for observations), and <code class=\\\"language-none\\\">'roscache_to_api'</code> (special value used for caching observation values).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'ros_topic'</code></td>\\n<td>Yes</td>\\n<td>Topic name for the ROS side of the connection.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'ros_type'</code></td>\\n<td>Yes</td>\\n<td>Topic type for the ROS side of the connection.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'callback_api'</code></td>\\n<td>No</td>\\n<td>A callback that is run on the HTTP encoded data received / sent on the API end of the connection. It takes in data, and returns transformed data based on the callback's action. Callbacks are specified by a string denoting how the callback can be accessed (e.g. <code class=\\\"language-none\\\">'api_callbacks.convert_to_rgb</code> = <code class=\\\"language-none\\\">from api_callbacks import convert_to_rgb</code>). No data transformation occurs if no callback is provided.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'callback_ros'</code></td>\\n<td>No</td>\\n<td>A callback that is run on the ROS data received / sent on the robot controller end of the connection. It takes in data and a reference to the robot controller. <code class=\\\"language-none\\\">'api_to_ros'</code> connections use this data to act on the robot, whereas <code class=\\\"language-none\\\">'ros_to_api'</code> connections turn this data into a dictionary that can be serialised into HTTP traffic. Callbacks are specified by a string denoting how the callback can be accessed (e.g. <code class=\\\"language-none\\\">'api_callbacks.convert_to_rgb</code> = <code class=\\\"language-none\\\">from api_callbacks import convert_to_rgb</code>). No action occurs at the ROS level if no callback is provided.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Task add-ons</h3>\\n<p>A YAML file, that must exist in a folder called <code class=\\\"language-none\\\">tasks</code> in the root of the add-on package (e.g. <code class=\\\"language-none\\\">tasks/my_task.yaml</code>).</p>\\n<p>The following keys are supported for task add-ons:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Key</th>\\n<th>Required</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'name'</code></td>\\n<td>Yes</td>\\n<td>A string used to refer to this task (must be unique!).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'actions'</code></td>\\n<td>Yes</td>\\n<td>A list of named connections to be provided as actions through the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>. Running this task will fail if the robot doesn't provide these named connections.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'observations'</code></td>\\n<td>Yes</td>\\n<td>A list of named connections to be provided as observations through the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>. Running this task will fail if the robot doesn't provide these named connections.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'results_format'</code></td>\\n<td>Yes</td>\\n<td>A string naming the format for results. The format must be installed, as <a href=https://github.com/qcr/benchbot_api>BenchBot API</a> will use the format's functions to provide the user with empty results.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'description'</code></td>\\n<td>No</td>\\n<td>A string describing what the task is, and how it works. Should be included if you want users to understand what challenges your task is trying to capture.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'type'</code></td>\\n<td>No</td>\\n<td>A string describing what robot / environment types are valid for this task. For example, a task that provides a magic image segmentation sensor would only be made available for <code class=\\\"language-none\\\">'sim_unreal'</code> type robots / environments.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'scene_count'</code></td>\\n<td>No</td>\\n<td>Integer representing the number of scenes (i.e. environment variations required for a task). If omitted, a default value of 1 will be used for the task.</td>\\n</tr>\\n</tbody>\\n</table>\\n\",\"name\":\"BenchBot Add-ons Manager\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_addons\",\"src\":\"/content/benchbot/benchbot-addons.md\",\"id\":\"benchbot-addons\",\"image_position\":\"center\",\"image\":\"/_next/static/gifs/8ef442bbcf06f5d9f4a4bd553bd28212.jpg\",\"_image\":\"/_next/static/gifs/8ef442bbcf06f5d9f4a4bd553bd28212.webm\"},{\"content\":\"<p><strong>NOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation. For a working BenchBot system, please install the BenchBot software stack by following the instructions <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>\\n<h1>BenchBot Supervisor</h1>\\n<p align=\\\"center\\\"><img alt=\\\"benchbot_supervisor\\\" src=\\\"/_next/static/images/benchbot_supervisor-308d8e63428bf85c3824123d688af972.jpg\\\" width=\\\"60%\\\"/></p>\\n<p>The BenchBot Supervisor is a HTTP server facilitating communication between user-facing interfaces like the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>, and the low-level robot components like <a href=https://github.com/qcr/benchbot_simulator>BenchBot Simulator</a> or real robots. Communication is typically routed through a <a href=https://github.com/qcr/benchbot_robot_controller>BenchBot Robot Controller</a>, which provides automated process management for low-level components and wraps all ROS communications.</p>\\n<h2>Installing and running the BenchBot Supervisor</h2>\\n<p>BenchBot Supervisor is a Python package containing a <code class=\\\"language-none\\\">Supervisor</code> class that wraps a HTTP server for both upstream and downstream communication. Install by running the following in the root directory of where this repository was cloned:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ pip install .\\n</code></pre>\\n<p>Once installed, the Python class can be used as follows:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_supervisor <span class=\\\"token keyword\\\">import</span> Supervisor\\n\\ns <span class=\\\"token operator\\\">=</span> Supervisor<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">.</span><span class=\\\"token punctuation\\\">.</span><span class=\\\"token punctuation\\\">.</span>args<span class=\\\"token punctuation\\\">.</span><span class=\\\"token punctuation\\\">.</span><span class=\\\"token punctuation\\\">.</span><span class=\\\"token punctuation\\\">)</span>\\ns<span class=\\\"token punctuation\\\">.</span>run<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>The following parameters are typically required for a useful instantiation of the supervisor:</p>\\n<ul>\\n<li><strong>addons_path</strong>: path to installed <a href=https://github.com/qcr/benchbot_addons>BenchBot add-ons</a> (is the same as the directory where <code class=\\\"language-none\\\">manager.py</code> can be found)</li>\\n<li><strong>task_name</strong>: string matching the <code class=\\\"language-none\\\">'name'</code> field of an installed task</li>\\n<li><strong>robot_name</strong>: string matching the <code class=\\\"language-none\\\">'name'</code> field of an installed robot</li>\\n<li><strong>environment_names</strong>: list of strings, each matching the <code class=\\\"language-none\\\">'name':'variant'</code> field combination of an installed environment (the <code class=\\\"language-none\\\">'name'</code> must be the same for all environments in the list)</li>\\n<li><strong>port</strong>: select a different port than the default (10000)</li>\\n</ul>\\n<p>The module can also be executed directly, which makes the passing of arguments from the command line simple (see <code class=\\\"language-none\\\">python -m benchbot_supervisor --help</code> for argument details):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ python -m benchbot_supervisor ...args...\\n</code></pre>\\n<p>As an example, the below command runs the supervisor for a scene change detection task, where active control is employed with ground truth localisation on a simulated Carter robot, and environments miniroom:1 and miniroom:5 are used:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ python -m benchbot_supervisor \\\\\\n    --task-name scd:active:ground_truth \\\\\\n    --robot-name carter \\\\\\n    --environment-names miniroom:1,miniroom:5\\n</code></pre>\\n<h2>Employing task, robot, and environment configurations</h2>\\n<p>The BenchBot Supervisor requires configuration details for the selected tasks, robots, and environments. It uses these details to manage each of the system components, like API interaction and control of the simulator / real robot. Configuration details are provided by YAML files, which are referenced via their <code class=\\\"language-none\\\">'name'</code> field as shown above.</p>\\n<p>The <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager</a> manages the installation of, and access to, these files. See the documentation there for further details on configuration files. All you need to do to use add-ons with the supervisor is provide the location via the <code class=\\\"language-none\\\">'addons_path'</code> argument.</p>\\n<h2>Interacting with the BenchBot Supervisor</h2>\\n<p>The supervisor includes a RESTful HTTP API for all interaction with a user-facing API. The RESTful API includes the following commands:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Request Route</th>\\n<th>Response Format</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">/</code></td>\\n<td><pre class=\\\"language-none\\\">Hello, I am the BenchBot supervisor</pre></td>\\n<td>Arbitrary response to confirm connection.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/config/</code></td>\\n<td><pre class=\\\"language-none\\\">{<br> ...<br> 'param_name': param_value,<br> ...<br>}</pre></td>\\n<td>Dictionary containing containing parameter values for all of supervisor configuration settings. Keys correspond to parameter name, &amp; values to parameter value.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/config/&lt;config&gt;</code></td>\\n<td><code class=\\\"language-none\\\">config_value</code></td>\\n<td>Directly retrieve the value of a supervisor configuration parameter with name <code class=\\\"language-none\\\">'config'</code>. Returns <code class=\\\"language-none\\\">param_value</code> of <code class=\\\"language-none\\\">'config'</code>.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/connections/&lt;connection&gt;</code></td>\\n<td><code class=\\\"language-none\\\">dict</code></td>\\n<td>Returns the response of the connection (e.g. an <code class=\\\"language-none\\\">image_rgb</code> connection would return the image) as a <code class=\\\"language-none\\\">dict</code>. Format &amp; style of the <code class=\\\"language-none\\\">dict</code> is defined by the methods described above in &quot;Defining environment, robot, &amp; task configurations&quot;.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/results_functions/</code></td>\\n<td><code class=\\\"language-none\\\">list</code></td>\\n<td>Returns a list of the results function names that can be remotely executed via the route below.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/results_functions/&lt;function&gt;</code></td>\\n<td><code class=\\\"language-none\\\">dict</code></td>\\n<td>Calls results function with name <code class=\\\"language-none\\\">'function'</code>, and returns the result of the function call in the response's JSON body.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/robot/</code></td>\\n<td><pre class=\\\"language-none\\\">Hello, I am the BenchBot robot controller</pre></td>\\n<td>Arbitrary response confirming a robot controller is available.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">/robot/&lt;command&gt;</code></td>\\n<td><code class=\\\"language-none\\\">dict</code></td>\\n<td>Passes the command <code class=\\\"language-none\\\">command</code> down to a running robot controller manager. See <a href=https://github.com/qcr/benchbot_robot_controller>BenchBot Robot Controller</a> for documentation of supported commands &amp; expected responses.</td>\\n</tr>\\n</tbody>\\n</table>\\n\",\"name\":\"BenchBot Backend Supervisor\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_supervisor\",\"image_position\":\"center 0%\",\"src\":\"/content/benchbot/benchbot-supervisor.md\",\"id\":\"benchbot-supervisor\",\"image\":\"/_next/static/images/benchbot_supervisor-308d8e63428bf85c3824123d688af972.jpg\"},{\"content\":\"<p><strong>NOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation (although it can be installed independently through pip and run on results files if desired). For a working BenchBot system, please install the BenchBot software stack by following the instructions <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>\\n<h1>BenchBot Evaluation</h1>\\n<p>BenchBot Evaluation is a library of functions used to call evaluation methods. These methods are installed through the <a href=https://github.com/qcr/benchbot-addons>BenchBot Add-ons Manager</a>, and evaluate the performance of a BenchBot system against the metric. The easiest way to use this module is through the helper scripts provided with the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a>.</p>\\n<h2>Installing and performing evaluation with BenchBot Evaluation</h2>\\n<p>BenchBot Evaluation is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ pip install .\\n</code></pre>\\n<p>Although evaluation is best run from within the BenchBot software stack, it can be run in isolation if desired. The following code snippet shows how to perform evaluation with the <code class=\\\"language-none\\\">'omq'</code> method from Python:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_eval<span class=\\\"token punctuation\\\">.</span>evaluator <span class=\\\"token keyword\\\">import</span> Evaluator<span class=\\\"token punctuation\\\">,</span> Validator\\n\\nValidator<span class=\\\"token punctuation\\\">(</span>results_file<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>validate_results_data<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\nEvaluator<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'omq'</span><span class=\\\"token punctuation\\\">,</span> scores_file<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>evaluate<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>This prints the final scores to the screen and saves them to a file using the following inputs:</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">results_file</code>: points to the JSON file with the output from your experiment</li>\\n<li><code class=\\\"language-none\\\">ground_truth_folder</code>: the directory containing the relevant environment ground truth JSON files</li>\\n<li><code class=\\\"language-none\\\">save_file</code>: is where final scores are to be saved</li>\\n</ul>\\n<h2>How add-ons interact with BenchBot Evaluation</h2>\\n<p>Two types of add-ons are used in the BenchBot Evaluation process: format definitions, and evaluation methods. An evaluation method's YAML file defines what results formats and ground truth formats the method supports. This means:</p>\\n<ul>\\n<li>this package requires installation of the <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager</a> for interacting with installed add-ons</li>\\n<li>the <code class=\\\"language-none\\\">results_file</code> must be a valid instance of a supported format</li>\\n<li>there must be a valid ground truth available in a supported format, for the same environment as the results</li>\\n<li>validity is determined by the format-specific validation function described in the format's YAML file</li>\\n</ul>\\n<p>Please see the <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager's documentation</a> for further details on the different types of add-ons.</p>\\n<h2>Creating valid results and ground truth files</h2>\\n<p>The <a href=https://github.com/qcr/benchbot>BenchBot software stack</a> includes tools to assist in creating results and ground truth files:</p>\\n<ul>\\n<li>\\n<p><strong>results:</strong> are best created using the <code class=\\\"language-none\\\">empty_results()</code> and <code class=\\\"language-none\\\">results_functions()</code> helper functions in the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>, which automatically populate metadata for your current task and environment.</p>\\n</li>\\n<li>\\n<p><strong>ground truths:</strong> this package includes a <code class=\\\"language-none\\\">GroundTruthCreator</code> class to aid in creating ground truths of a specific format, for a specific environment. Example use includes:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_eval<span class=\\\"token punctuation\\\">.</span>ground_truth_creator <span class=\\\"token keyword\\\">import</span> GroundTruthCreator\\n\\ngtc <span class=\\\"token operator\\\">=</span> GroundTruthCreator<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'object_map_ground_truth'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'miniroom:1'</span><span class=\\\"token punctuation\\\">)</span>\\ngt <span class=\\\"token operator\\\">=</span> gtc<span class=\\\"token punctuation\\\">.</span>create_empty<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">;</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>gtc<span class=\\\"token punctuation\\\">.</span>functions<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>  <span class=\\\"token comment\\\"># ['create', 'create_object']</span>\\ngt<span class=\\\"token punctuation\\\">[</span><span class=\\\"token string\\\">'ground_truth'</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token string\\\">'objects'</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span> <span class=\\\"token operator\\\">=</span> gtc<span class=\\\"token punctuation\\\">.</span>functions<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'create_object'</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n</li>\\n</ul>\\n\",\"name\":\"BenchBot Evaluation Tools\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_eval\",\"src\":\"/content/benchbot/benchbot-eval.md\",\"id\":\"benchbot-eval\",\"image_position\":\"center\",\"image\":\"/_next/static/images/qcr_logo_light_filled-b2f2ba81b0ef111afdf9fa7264fb4adf.png\"},{\"content\":\"<p><strong>NOTE: this software needs to interface with a running instance of the BenchBot software stack. Unless you are running against a remote stack / robot, please install this software with the BenchBot software stack as described <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>\\n<h1>BenchBot API</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.jpg\\\"><source src=\\\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>The BenchBot API provides a simple interface for controlling a robot or simulator through actions, and receiving data through observations. As shown above, the entire code required for running an agent in a realistic 3D simulator is only a handful of simple Python commands.</p>\\n<p><a href=\\\"https://gym.openai.com\\\">Open AI Gym</a> users will find the breakdown into actions, observations, and steps extremely familiar. BenchBot API allows researchers to develop and test novel algorithms with real robot systems and realistic 3D simulators, without the typical hassles arising when interfacing with complicated multi-component robot systems.</p>\\n<p>Running a robot through an entire environment, with your own custom agent, is as simple as one line of code with the BenchBot API:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_api <span class=\\\"token keyword\\\">import</span> BenchBot\\n<span class=\\\"token keyword\\\">from</span> my_agent <span class=\\\"token keyword\\\">import</span> MyAgent\\n\\nBenchBot<span class=\\\"token punctuation\\\">(</span>agent<span class=\\\"token operator\\\">=</span>MyAgent<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">.</span>run<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>The above assumes you have created your own agent by overloading the abstract <code class=\\\"language-none\\\">Agent</code> class provided with the API. Overloading the abstract class requires implementing 3 basic methods. Below is a basic example to spin on the spot:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_api <span class=\\\"token keyword\\\">import</span> Agent\\n<span class=\\\"token keyword\\\">import</span> json\\n\\n<span class=\\\"token keyword\\\">class</span> <span class=\\\"token class-name\\\">MyAgent</span><span class=\\\"token punctuation\\\">(</span>Agent<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n\\n    <span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">is_done</span><span class=\\\"token punctuation\\\">(</span>self<span class=\\\"token punctuation\\\">,</span> action_result<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        <span class=\\\"token comment\\\"># Go forever</span>\\n        <span class=\\\"token keyword\\\">return</span> <span class=\\\"token boolean\\\">False</span>\\n\\n    <span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">pick_action</span><span class=\\\"token punctuation\\\">(</span>self<span class=\\\"token punctuation\\\">,</span> observations<span class=\\\"token punctuation\\\">,</span> action_list<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        <span class=\\\"token comment\\\"># Rotates on the spot indefinitely, 5 degrees at a time</span>\\n        <span class=\\\"token comment\\\"># (assumes we are running in passive mode)</span>\\n        <span class=\\\"token keyword\\\">return</span> <span class=\\\"token string\\\">'move_angle'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">{</span><span class=\\\"token string\\\">'angle'</span><span class=\\\"token punctuation\\\">:</span> <span class=\\\"token number\\\">5</span><span class=\\\"token punctuation\\\">}</span>\\n\\n    <span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">save_result</span><span class=\\\"token punctuation\\\">(</span>self<span class=\\\"token punctuation\\\">,</span> filename<span class=\\\"token punctuation\\\">,</span> empty_results<span class=\\\"token punctuation\\\">,</span> results_format_fns<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n        <span class=\\\"token comment\\\"># Save some blank results</span>\\n        <span class=\\\"token keyword\\\">with</span> <span class=\\\"token builtin\\\">open</span><span class=\\\"token punctuation\\\">(</span>filename<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'w'</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token keyword\\\">as</span> f<span class=\\\"token punctuation\\\">:</span>\\n            json<span class=\\\"token punctuation\\\">.</span>dump<span class=\\\"token punctuation\\\">(</span>empty_results<span class=\\\"token punctuation\\\">,</span> f<span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>If you prefer to do things manually, a more exhaustive suite of functions are also available as part of the BenchBot API. Instead of using the <code class=\\\"language-none\\\">BenchBot.run()</code> method, a large number of methods are available through the API. Below highlights a handful of the capabilities of BenchBot API:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_api <span class=\\\"token keyword\\\">import</span> BenchBot<span class=\\\"token punctuation\\\">,</span> RESULT_LOCATION\\n<span class=\\\"token keyword\\\">import</span> json\\n<span class=\\\"token keyword\\\">import</span> matplotlib<span class=\\\"token punctuation\\\">.</span>pyplot <span class=\\\"token keyword\\\">as</span> plt\\n\\n<span class=\\\"token comment\\\"># Create a BenchBot instance &amp; reset the simulator / robot to starting state</span>\\nb <span class=\\\"token operator\\\">=</span> BenchBot<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\nobservations<span class=\\\"token punctuation\\\">,</span> action_result <span class=\\\"token operator\\\">=</span> b<span class=\\\"token punctuation\\\">.</span>reset<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Print details of selected task &amp; environment</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>b<span class=\\\"token punctuation\\\">.</span>task_details<span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>b<span class=\\\"token punctuation\\\">.</span>environment_details<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Visualise the current RGB image from the robot</span>\\nplt<span class=\\\"token punctuation\\\">.</span>imshow<span class=\\\"token punctuation\\\">(</span>observations<span class=\\\"token punctuation\\\">[</span><span class=\\\"token string\\\">'image_rgb'</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Move to the next pose if we have a 'move_next' action available</span>\\n<span class=\\\"token keyword\\\">if</span> <span class=\\\"token string\\\">'move_next'</span> <span class=\\\"token keyword\\\">in</span> b<span class=\\\"token punctuation\\\">.</span>actions<span class=\\\"token punctuation\\\">:</span>\\n    observations<span class=\\\"token punctuation\\\">,</span> action_result <span class=\\\"token operator\\\">=</span> b<span class=\\\"token punctuation\\\">.</span>step<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'move_next'</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Save some empty results</span>\\n<span class=\\\"token keyword\\\">with</span> <span class=\\\"token builtin\\\">open</span><span class=\\\"token punctuation\\\">(</span>RESULT_LOCATION<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'w'</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token keyword\\\">as</span> f<span class=\\\"token punctuation\\\">:</span>\\n    json<span class=\\\"token punctuation\\\">.</span>dump<span class=\\\"token punctuation\\\">(</span>b<span class=\\\"token punctuation\\\">.</span>empty_results<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">,</span> f<span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>For sample solutions that use the BenchBot API, see the examples add-ons available (e.g. <a href=https://github.com/benchbot-addons/examples_base><code class=\\\"language-none\\\">benchbot-addons/examples_base</code></a> and <a href=https://github.com/benchbot-addons/examples_ssu><code class=\\\"language-none\\\">benchbot-addons/examples_ssu</code></a>).</p>\\n<h2>Installing BenchBot API</h2>\\n<p>BenchBot API is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ pip install .\\n</code></pre>\\n<h2>Using the API to communicate with a robot</h2>\\n<p>Communication with the robot comes through a series of &quot;channels&quot; which are defined by the robot's definition file (e.g. <a href=https://github.com/benchbot-addons/robots_isaac/blob/master/robots/carter.yaml>carter</a>). A task definition file (e.g. <a href=https://github.com/benchbot-addons/tasks_ssu/blob/master/tasks/sslam_pgt.yaml>semantic_slam:passive:ground_truth</a>) then declares which of these connections are provided to the API as either sensor observations or actions to be executed by a robot actuator.</p>\\n<p>The API talks to the <a href=https://github.com/qcr/benchbot_supervisor>BenchBot Supervisor</a>, which handles loading and managing the different kinds of back-end configuration files. This abstracts all of the underlying communication complexities away from the user, allowing the BenchBot API to remain a simple interface that focuses on getting observations and sending actions.</p>\\n<p>An action is sent to the robot by calling the <code class=\\\"language-none\\\">BenchBot.step()</code> method with a valid action (found by checking the <code class=\\\"language-none\\\">BenchBot.actions</code> property):</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_api <span class=\\\"token keyword\\\">import</span> BenchBot\\n\\nb <span class=\\\"token operator\\\">=</span> BenchBot<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\navailable_actions <span class=\\\"token operator\\\">=</span> b<span class=\\\"token punctuation\\\">.</span>actions\\nb<span class=\\\"token punctuation\\\">.</span>step<span class=\\\"token punctuation\\\">(</span>b<span class=\\\"token punctuation\\\">.</span>actions<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">{</span><span class=\\\"token string\\\">'action_arg:'</span><span class=\\\"token punctuation\\\">,</span> arg_value<span class=\\\"token punctuation\\\">}</span><span class=\\\"token punctuation\\\">)</span>  <span class=\\\"token comment\\\"># Perform the first available action</span>\\n</code></pre>\\n<p>The second parameter is a dictionary of named arguments for the selected action. For example, moving 5m forward with the <code class=\\\"language-none\\\">'move_distance'</code> action is represented by the dictionary <code class=\\\"language-none\\\">{'distance': 5}</code>.</p>\\n<p>Observations lists are received as return values from a <code class=\\\"language-none\\\">BenchBot.step()</code> call (<code class=\\\"language-none\\\">BenchBot.reset()</code> internally calls <code class=\\\"language-none\\\">BenchBot.step(None)</code>, which means don't perform an action):</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> benchbot_api <span class=\\\"token keyword\\\">import</span> BenchBot\\n\\nb <span class=\\\"token operator\\\">=</span> BenchBot<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\nobservations<span class=\\\"token punctuation\\\">,</span> action_result <span class=\\\"token operator\\\">=</span> b<span class=\\\"token punctuation\\\">.</span>reset<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\nobservations<span class=\\\"token punctuation\\\">,</span> action_result <span class=\\\"token operator\\\">=</span> b<span class=\\\"token punctuation\\\">.</span>step<span class=\\\"token punctuation\\\">(</span><span class=\\\"token string\\\">'move_distance'</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">{</span><span class=\\\"token string\\\">'distance'</span><span class=\\\"token punctuation\\\">:</span> <span class=\\\"token number\\\">5</span><span class=\\\"token punctuation\\\">}</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>The returned <code class=\\\"language-none\\\">observations</code> variable holds a dictionary with key-value pairs corresponding to the name-data defined by each observation channel.</p>\\n<p>The <code class=\\\"language-none\\\">action_result</code> is an enumerated value denoting the result of the action (use <code class=\\\"language-none\\\">from benchbot_api import ActionResult</code> to access the <code class=\\\"language-none\\\">Enum</code> class). You should use this result to guide the progression of your algorithm either manually or in the <code class=\\\"language-none\\\">is_done()</code> method of your <code class=\\\"language-none\\\">Agent</code>. Possible values for the returned <code class=\\\"language-none\\\">action_result</code> are:</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">ActionResult.SUCCESS</code>: the action was carried out successfully</li>\\n<li><code class=\\\"language-none\\\">ActionResult.FINISHED</code>: the action was carried out successfully, and the robot is now finished its traversal through the scene (only used in <code class=\\\"language-none\\\">passive</code> actuation mode)</li>\\n<li><code class=\\\"language-none\\\">ActionResult.COLLISION</code>: the action crashed the robot into an obstacle, and as a result it will not respond to any further actuation commands (at this point you should quit)</li>\\n</ul>\\n<h3>Standard Communication Channels</h3>\\n<p>Tasks and robot definition files declare actions and observations, and these files are include through <a href=https://github.com/qcr/benchbot_addons>BenchBot add-ons</a>. The add-on creator is free to add and declare channels as they please, but it is a better experience for all if channel definitions are as consistent as possible across the BenchBot ecosystem.</p>\\n<p>So if you're adding a robot that move between a set of poses, declare a channel called <code class=\\\"language-none\\\">'move_next</code> with no arguments. Likewise, a robot that receives image observations should use a channel named <code class=\\\"language-none\\\">'image_rgb'</code> with the same format as described below. Feel free to implement the channels however you please for your robot, but consistent interfaces should always be preferred.</p>\\n<p>If you encounter a task using non-standard channel configurations, the API has all the functionality you need as a user to handle them (<code class=\\\"language-none\\\">actions</code>, <code class=\\\"language-none\\\">config</code>, &amp; <code class=\\\"language-none\\\">observations</code> properties). On the other hand, maybe the non-standard channel should be a new standard. New standard communication channels are always welcome; please open a pull request with the details!</p>\\n<h4>Standard action channels:</h4>\\n<table>\\n<thead>\\n<tr>\\n<th>Name</th>\\n<th style=\\\"text-align:center\\\">Required Arguments</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'move_next'</code></td>\\n<td style=\\\"text-align:center\\\"><code class=\\\"language-none\\\">None</code></td>\\n<td>Moves the robot to the next pose in its list of pre-defined poses (only available in environments that declare a <code class=\\\"language-none\\\">'trajectory_poses'</code> field).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'move_distance'</code></td>\\n<td style=\\\"text-align:center\\\"><pre class=\\\"language-none\\\">{'distance': float}</pre></td>\\n<td>Moves the robot <code class=\\\"language-none\\\">'distance'</code> metres directly ahead.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'move_angle'</code></td>\\n<td style=\\\"text-align:center\\\"><pre class=\\\"language-none\\\">{'angle': float}</pre></td>\\n<td>Rotate the angle on the spot by <code class=\\\"language-none\\\">'angle'</code> degrees.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h4>Standard observation channels:</h4>\\n<table>\\n<thead>\\n<tr>\\n<th>Name</th>\\n<th style=\\\"text-align:left\\\">Data format</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">'image_depth'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">numpy.ndarray(shape=(H,W),<br> dtype='float32')</pre></td>\\n<td>Depth image from the default image sensor with depths in meters.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'image_depth_info'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">{<br> 'frame_id': string<br> 'height': int<br> 'width': int<br> 'matrix_instrinsics':<br> numpy.ndarray(shape=(3,3),<br> dtype='float64')<br>'matrix_projection':<br> numpy.ndarray(shape=(3,4)<br> dtype='float64')<br>}</pre></td>\\n<td>Sensor information for the depth image. <code class=\\\"language-none\\\">'matrix_instrinsics'</code> is of the format:<br><pre class=\\\"language-none\\\">[fx 0 cx]<br>[0 fy cy]<br>[0 0 1]</pre> for a camera with focal lengths <code class=\\\"language-none\\\">(fx,fy)</code>, &amp; principal point <code class=\\\"language-none\\\">(cx,cy)</code>. Likewise, <code class=\\\"language-none\\\">'matrix_projection'</code> is:<br><pre class=\\\"language-none\\\">[fx 0 cx Tx]<br>[0 fy cy Ty]<br>[0 0 1 0]</pre>where <code class=\\\"language-none\\\">(Tx,Ty)</code> is the translation between stereo sensors. See <a href=\\\"http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html\\\">here</a> for further information on fields.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'image_rgb'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">numpy.ndarray(shape=(H,W,3),<br> dtype='uint8')</pre></td>\\n<td>RGB image from the default image sensor with colour values mapped to the 3 channels, in the 0-255 range.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'image_rgb_info'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">{<br> 'frame_id': string<br> 'height': int<br> 'width': int<br> 'matrix_instrinsics':<br> numpy.ndarray(shape=(3,3),<br> dtype='float64')<br>'matrix_projection':<br> numpy.ndarray(shape=(3,4)<br> dtype='float64')<br>}</pre></td>\\n<td>Sensor information for the RGB image. <code class=\\\"language-none\\\">'matrix_instrinsics'</code> is of the format:<br><pre class=\\\"language-none\\\">[fx 0 cx]<br>[0 fy cy]<br>[0 0 1]</pre> for a camera with focal lengths <code class=\\\"language-none\\\">(fx,fy)</code>, &amp; principal point <code class=\\\"language-none\\\">(cx,cy)</code>. Likewise, <code class=\\\"language-none\\\">'matrix_projection'</code> is:<br><pre class=\\\"language-none\\\">[fx 0 cx Tx]<br>[0 fy cy Ty]<br>[0 0 1 0]</pre>where <code class=\\\"language-none\\\">(Tx,Ty)</code> is the translation between stereo sensors. See <a href=\\\"http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html\\\">here</a> for further information on fields.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'laser'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">{<br> 'range_max': float64,<br> 'range_min': float64,<br> 'scans':<br> numpy.ndarray(shape=(N,2),<br> dtype='float64')<br>}</pre></td>\\n<td>Set of scan values from a laser sensor, between <code class=\\\"language-none\\\">'range_min'</code> &amp; <code class=\\\"language-none\\\">'range_max'</code> (in meters). The <code class=\\\"language-none\\\">'scans'</code> array consists of <code class=\\\"language-none\\\">N</code> scans of format <code class=\\\"language-none\\\">[scan_angle, scan_value]</code>. For example, <code class=\\\"language-none\\\">scans[100,0]</code> would get the angle of the 100th scan &amp; <code class=\\\"language-none\\\">scans[100,1]</code> would get the distance value.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">'poses'</code></td>\\n<td style=\\\"text-align:left\\\"><pre class=\\\"language-none\\\">{<br> ...<br> 'frame_name': {<br> 'parent_frame': string<br> 'rotation_rpy':<br> numpy.ndarray(shape=(3,),<br> dtype='float64')<br> 'rotation_xyzw':<br> numpy.ndarray(shape=(4,),<br> dtype='float64')<br> 'translation_xyz':<br> numpy.ndarray(shape=(3,),<br> dtype='float64')<br> }<br> ...<br>}</pre></td>\\n<td>Dictionary of relative poses for the current system state. The pose of each system component is available at key <code class=\\\"language-none\\\">'frame_name'</code>. Each pose has a <code class=\\\"language-none\\\">'parent_frame'</code> which the pose is relative to (all poses are typically with respect to global <code class=\\\"language-none\\\">'map'</code> frame), &amp; the pose values. <code class=\\\"language-none\\\">'rotation_rpy'</code> is <code class=\\\"language-none\\\">[roll,pitch,yaw]</code>, <code class=\\\"language-none\\\">'rotation_xyzw'</code> is the equivalent quaternion <code class=\\\"language-none\\\">[x,y,z,w]</code>, &amp; <code class=\\\"language-none\\\">'translation_xyz'</code> is the Cartesion <code class=\\\"language-none\\\">[x,y,z]</code> coordinates.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h2>Using the API to communicate with the BenchBot system</h2>\\n<p>A running BenchBot system manages many other elements besides simply getting data to and from a real / simulated robot. BenchBot encapsulates not just the robot, but also the environment it is operating in (whether that be simulator or real) and task that is currently being attempted.</p>\\n<p>The API handles communication for all parts of the BenchBot system, including controlling the currently running environment and obtaining configuration information. Below are details for some of the more useful features of the API (all features are also documented in the <a href=https:/github.com/qcr/benchbot_api/blob/master/benchbot_api/benchbot.py><code class=\\\"language-none\\\">benchbot.py</code></a> source code).</p>\\n<h3>Gathering configuration information</h3>\\n<table>\\n<thead>\\n<tr>\\n<th>API method or property</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">config</code></td>\\n<td>Returns a <code class=\\\"language-none\\\">dict</code> exhaustively describing the current BenchBot configuration. Most of the information returned will not be useful for general BenchBot use.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Interacting with the environment</h3>\\n<table>\\n<thead>\\n<tr>\\n<th>API method or property</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">reset()</code></td>\\n<td>Resets the current environment scene. For the simulator, this means restarting the running simulator instance with the robot back at its initial position. The method returns initial <code class=\\\"language-none\\\">observations</code>, &amp; the <code class=\\\"language-none\\\">action_result</code> (should always be <code class=\\\"language-none\\\">BenchBot.ActionResult.SUCCESS</code>).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">next_scene()</code></td>\\n<td>Starts the next scene in the current environment (only relevant for tasks with multiple scenes). Note there is no going back once you have moved to the next scene. Returns the same as <code class=\\\"language-none\\\">reset()</code>.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Interacting with an agent</h3>\\n<table>\\n<thead>\\n<tr>\\n<th>API method or property</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">actions</code></td>\\n<td>Returns the list of actions currently available to the agent. This will update as actions are performed in the environment (for example if the agent has collided with an obstacle this list will be empty).</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">observations</code></td>\\n<td>Returns the lists of observations available to the agent.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">step(action, **action_args)</code></td>\\n<td>Performs the requested action with the provided named action arguments. See <a href=https:/github.com/qcr/benchbot_api/#using-the-api-to-communicate-with-a-robot>Using the API to communicate with a robot</a> above for further details.</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Creating results</h3>\\n<table>\\n<thead>\\n<tr>\\n<th>API method or property</th>\\n<th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><code class=\\\"language-none\\\">empty_results()</code></td>\\n<td>Generates a <code class=\\\"language-none\\\">dict</code> of with required result metadata &amp; empty results. Metadata (<code class=\\\"language-none\\\">'task_details'</code> &amp; <code class=\\\"language-none\\\">'environment_details'</code>) is pre-filled. To create results, all a user needs to do is fill in the empty <code class=\\\"language-none\\\">'results'</code> field using format's results functions. These functions are available through the <code class=\\\"language-none\\\">'results_functions()</code> method.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">results_functions()</code></td>\\n<td>Returns a <code class=\\\"language-none\\\">dict</code> of functions defined by the task's <code class=\\\"language-none\\\">'results_format'</code>. Example use for calling a <code class=\\\"language-none\\\">create()</code> function is <code class=\\\"language-none\\\">results_functions()['create']()</code>.</td>\\n</tr>\\n<tr>\\n<td><code class=\\\"language-none\\\">RESULT_LOCATION</code> (outside of <code class=\\\"language-none\\\">BenchBot</code> class)</td>\\n<td>A static string denoting where results should be saved (<code class=\\\"language-none\\\">/tmp/results</code>). Using this locations ensures tools in the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a> work as expected.</td>\\n</tr>\\n</tbody>\\n</table>\\n\",\"name\":\"BenchBot Python API\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_api\",\"image_position\":\"center 100%\",\"src\":\"/content/benchbot/benchbot-api.md\",\"id\":\"benchbot-api\",\"image\":\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.jpg\",\"_image\":\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.webm\"},{\"content\":\"<p><strong>NOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation. For a working BenchBot system, please install the BenchBot software stack by following the instructions <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>\\n<h1>BenchBot Simulator</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/ec1857995c55d26bf6712cf7c339dbae.jpg\\\"><source src=\\\"/_next/static/gifs/ec1857995c55d26bf6712cf7c339dbae.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>The BenchBot Simulator is an extension of the <a href=\\\"https://developer.nvidia.com/isaac-sdk\\\">NVIDIA Isaac SDK</a> that establishes ROS communications to a running instance of an Unreal Engine-based <a href=\\\"https://developer.nvidia.com/isaac-sim\\\">NVIDIA Isaac SIM</a>. BenchBot simulator is explicitly linked to version 2019.2 of Isaac, the last version with direct support for the Unreal Engine-based simulator. In the future we intend to move to <a href=\\\"https://developer.nvidia.com/nvidia-omniverse\\\">Omniverse</a>, NVIDIA's new 3D production pipeline with RTX support.</p>\\n<p>BenchBot simulator provides direct access to the following data on the simulated robot. Access is via ROS on the topic provided in brackets:</p>\\n<ul>\\n<li>RGB images from the top camera (<code class=\\\"language-none\\\">/camera/color/image_raw</code>)</li>\\n<li>Camera information for RGB images from the top camera (<code class=\\\"language-none\\\">/camera/color/camera_info</code>)</li>\\n<li>Depth images from the top camera (<code class=\\\"language-none\\\">/camera/depth/image_raw</code>)</li>\\n<li>Camera information for depth images from the top camera (<code class=\\\"language-none\\\">/camera/depth/camera_info</code>)</li>\\n<li>Laserscan data from the LiDAR (<code class=\\\"language-none\\\">/scan_laser</code>)</li>\\n<li>Raw odometry data (<code class=\\\"language-none\\\">/odom</code>)</li>\\n<li>A full transform tree (<code class=\\\"language-none\\\">/tf</code>)</li>\\n</ul>\\n<p>Direct control of the robot is also facilitated via:</p>\\n<ul>\\n<li>3D twist velocities sent to topic <code class=\\\"language-none\\\">/cmd_vel</code></li>\\n</ul>\\n<h2>Installing BenchBot Simulator</h2>\\n<p><strong>Please see the note at the top of the page; installation of BenchBot Simulator in isolation is generally not what you want!</strong></p>\\n<p>If you are sure you need to install the simulator in isolation, the following steps should be sufficient. Note there are a significant number of driver &amp; software requirements for your system:</p>\\n<ol>\\n<li>\\n<p>Download version 2019.2 of the Isaac SDK from the <a href=\\\"https://developer.nvidia.com/isaac/downloads\\\">NVIDIA site</a>. If creating your own environments, also download version 2019.2 of Isaac SIM (<em>not</em> NavSim). You will have to create / sign in to an NVIDIA developer account, and look in the &quot;Archive&quot; drop down for version 2019.2.</p>\\n</li>\\n<li>\\n<p>Either setup your system with Isaac SIM, or download our environments:</p>\\n<p>a) Follow the <a href=\\\"https://docs.nvidia.com/isaac/isaac_sim/setup.html\\\">install instructions</a> for Isaac SIM to get Unreal Engine (through IsaacSimProject) running on your system. You will have to link Epic Games to your Github account to get access.</p>\\n<p>b) Download our latest environments: <a href=https://github.com/benchbot-addons/envs_isaac_develop/blob/master/.remote>Isaac Development Environments</a>, and <a href=https://github.com/benchbot-addons/envs_isaac_challenge/blob/master/.remote>Isaac Challenge Environments</a></p>\\n</li>\\n<li>\\n<p>Install the Isaac SDK by following the instructions <a href=\\\"https://docs.nvidia.com/isaac/archive/2019.2/doc/setup.html\\\">here</a>.</p>\\n</li>\\n<li>\\n<p>Clone the BenchBot simulator, apply our patches to the installed Isaac SDK, &amp; build the simulator using the Bazel wrapper script (ensure the environment variable <code class=\\\"language-none\\\">ISAAC_SDK_PATH</code> is set to where you installed Isaac SDK):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ git clone https://github.com/qcr/benchbot_simulator &amp;&amp; cd benchbot_simulator\\nu@pc:~$ .isaac_patches/apply_patches\\nu@pc:~$ ./bazelros build //apps/benchbot_simulator\\n</code></pre>\\n</li>\\n</ol>\\n<h2>Running BenchBot Simulator</h2>\\n<p>The BenchBot simulator interface is run alongside a running Isaac Unreal Engine Simulator. To get both components running:</p>\\n<ol>\\n<li>\\n<p>Start the Unreal Engine Simulator, either via our precompiled environments or the IsaacSimProject Unreal Editor:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ ./IsaacSimProject &lt;map_name&gt; \\\\\\n    -isaac_sim_config_json='&lt;path_to_isaac&gt;/apps/carter/carter_sim/bridge_config/carter_full.json' \\\\\\n    -windowed -ResX=960 -ResY=540 -vulkan -game\\n</code></pre>\\n</li>\\n<li>\\n<p>Launch BenchBot simulator Isaac application (you first need to hardcode the pose unfortunately...):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ START_POSE=&lt;robot_start_pose&gt; \\\\\\n    sed -i &quot;0,/\\\\&quot;pose\\\\&quot;:/{s/\\\\(\\\\&quot;pose\\\\&quot;: \\\\)\\\\(.*\\\\)/\\\\1$START_POSE}&quot; \\\\\\n    &lt;path_to_isaac&gt;/apps/carter/carter_sim/bridge_config/carter_full_config.json\\nu@pc:~$ ./bazelros run //apps/benchbot_simulator\\n</code></pre>\\n</li>\\n</ol>\\n<p>At this point you will have a running Isaac Unreal Engine Simulator, with sensorimotor data available from the robot in ROS!</p>\\n<h2>Using BenchBot Simulator with the BenchBot Robot Controller</h2>\\n<p>The <a href=https://github.com/qcr/benchbot_robot_controller>BenchBot Robot Controller</a> is a wrapping ROS / HTTP hybrid script that manages running robots and their required subprocesses. See the <code class=\\\"language-none\\\">carter_sim.yaml</code> configuration in the <a href=https://github.com/qcr/benchbot_supervisor>BenchBot Supervisor</a> for an example configuration of how to run BenchBot Simulator through the Robot Controller.</p>\\n\",\"name\":\"BenchBot Simulator (Isaac)\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_simulator\",\"src\":\"/content/benchbot/benchbot-simulator.md\",\"id\":\"benchbot-simulator\",\"image_position\":\"center\",\"image\":\"/_next/static/gifs/ec1857995c55d26bf6712cf7c339dbae.jpg\",\"_image\":\"/_next/static/gifs/ec1857995c55d26bf6712cf7c339dbae.webm\"},{\"content\":\"<p align=center><strong>~ Our <a href=\\\"https://evalai.cloudcv.org/web/challenges/challenge-page/807/overview\\\">Robotic Vision Scene Understanding (RVSU) Challenge is live on EvalAI</a> ~<br>(prizes include $2,500USD provided by <a href=\\\"https://www.roboticvision.org/\\\">ACRV</a> & GPUs provided by sponsors <a href=\\\"https://www.nvidia.com/en-us/research/robotics/\\\">NVIDIA</a>)</strong></p>\\n<p align=center><strong>~ Our <a href=\\\"https://github.com/qcr/benchbot/wiki/Tutorial:-Performing-Semantic-SLAM-with-Votenet\\\">BenchBot tutorial</a> is the best place to get started developing with BenchBot ~</strong></p>\\n<h1>BenchBot Software Stack</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/0f460afe63fb093a8b44efcd5c652cf8.jpg\\\"><source src=\\\"/_next/static/gifs/0f460afe63fb093a8b44efcd5c652cf8.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>The BenchBot software stack is a collection of software packages that allow end users to control robots in real or simulated environments with a simple python API. It leverages the simple &quot;observe, act, repeat&quot; approach to robot problems prevalent in reinforcement learning communities (<a href=\\\"https://gym.openai.com/\\\">OpenAI Gym</a> users will find the BenchBot API interface very similar).</p>\\n<p>BenchBot was created as a tool to assist in the research challenges faced by the semantic scene understanding community; challenges including understanding a scene in simulation, transferring algorithms to real world systems, and meaningfully evaluating algorithm performance. We've since realised, these challenges don't just exist for semantic scene understanding, they're prevalent in a wide range of robotic problems.</p>\\n<p>This led us to create version 2 of BenchBot with a focus on allowing users to define their own functionality for BenchBot through <a href=https://github.com/qcr/benchbot_addons>add-ons</a>. Want to integrate your own environments? Plug-in new robot platforms? Define new tasks? Share examples with others? Add evaluation measures? This all now possible with add-ons, and you don't have to do anything more than add some YAML and Python files defining your new content!</p>\\n<p>The &quot;bench&quot; in &quot;BenchBot&quot; refers to benchmarking, with our goal to provide a system that greatly simplifies the benchmarking of novel algorithms in both realistic 3D simulation and on real robot platforms. If there is something else you would like to use BenchBot for (like integrating different simulators), please let us know. We're very interested in BenchBot being the glue between your novel robotics research and whatever your robot platform may be.</p>\\n<p>This repository contains the software stack needed to develop solutions for BenchBot tasks on your local machine. It installs and configures a significant amount of software for you, wraps software in stable Docker images (~50GB), and provides simple interaction with the stack through 4 basic scripts: <code class=\\\"language-none\\\">benchbot_install</code>, <code class=\\\"language-none\\\">benchbot_run</code>, <code class=\\\"language-none\\\">benchbot_submit</code>, and <code class=\\\"language-none\\\">benchbot_eval</code>.</p>\\n<h2>System recommendations and requirements</h2>\\n<p>The BenchBot software stack is designed to run seamlessly on a wide number of system configurations (currently limited to Ubuntu 18.04+). System hardware requirements are relatively high due to the software run for 3D simulation (Unreal Engine, Nvidia Isaac, Vulkan, etc.):</p>\\n<ul>\\n<li>Nvidia Graphics card (GeForce GTX 1080 minimum, Titan XP+ / GeForce RTX 2070+ recommended)</li>\\n<li>CPU with multiple cores (Intel i7-6800K minimum)</li>\\n<li>32GB+ RAM</li>\\n<li>64GB+ spare storage (an SSD storage device is <strong>strongly</strong> recommended)</li>\\n</ul>\\n<p>Having a system that meets the above hardware requirements is all that is required to begin installing the BenchBot software stack. The install script analyses your system configuration and offers to install any missing software components interactively. The list of 3rd party software components involved includes:</p>\\n<ul>\\n<li>Nvidia Driver (4.18+ required, 4.50+ recommended)</li>\\n<li>CUDA with GPU support (10.0+ required, 10.1+ recommended)</li>\\n<li>Docker Engine - Community Edition (19.03+ required, 19.03.2+ recommended)</li>\\n<li>Nvidia Container Toolkit (1.0+ required, 1.0.5+ recommended)</li>\\n<li>ISAAC 2019.2 SDK (requires an Nvidia developer login)</li>\\n</ul>\\n<h2>Managing your installation</h2>\\n<p>Installation is simple:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ git clone https://github.com/qcr/benchbot &amp;&amp; cd benchbot\\nu@pc:~$ ./install\\n</code></pre>\\n<p>Any missing software components, or configuration issues with your system, should be detected by the install script and resolved interactively. The installation asks if you want to add BenchBot helper scripts to your <code class=\\\"language-none\\\">PATH</code>. Choosing yes will make the following commands available from any directory: <code class=\\\"language-none\\\">benchbot_install</code> (same as <code class=\\\"language-none\\\">./install</code> above), <code class=\\\"language-none\\\">benchbot_run</code>, <code class=\\\"language-none\\\">benchbot_submit</code>, <code class=\\\"language-none\\\">benchbot_eval</code>, and <code class=\\\"language-none\\\">benchbot_batch</code>.</p>\\n<p>BenchBot installs a default set of add-ons (currently <code class=\\\"language-none\\\">'benchbot-addons/ssu'</code>), but this can be changed based on how you want to use BenchBot. For example, the following will also install the <code class=\\\"language-none\\\">'benchbot-addons/sqa'</code> add-ons:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_install --addons benchbot-addons/ssu,benchbot-addons/sqa\\n</code></pre>\\n<p>See the <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager's documentation</a> for more information on using add-ons.</p>\\n<p>The BenchBot software stack will frequently check for updates and can update itself automatically. To update simply run the install script again (add the <code class=\\\"language-none\\\">--force-clean</code> flag if you would like to install from scratch):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_install\\n</code></pre>\\n<p>If you decide to uninstall the BenchBot software stack, run:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_install --uninstall\\n</code></pre>\\n<p>There are a number of other options to customise your BenchBot installation, which are all described by running:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_install --help\\n</code></pre>\\n<h2>Getting started</h2>\\n<p>Getting a solution up and running with BenchBot is as simple as 1,2,3. Here's how to use BenchBot with content from the <a href=https://github.com/benchbot-addons/ssu>semantic scene understanding add-on</a>:</p>\\n<ol>\\n<li>\\n<p>Run a simulator with the BenchBot software stack by selecting an available robot, environment, and task definition:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_run --robot carter --env miniroom:1 --task semantic_slam:active:ground_truth\\n</code></pre>\\n<p>A number of useful flags exist to help you explore what content is available in your installation (see <code class=\\\"language-none\\\">--help</code> for full details). For example, you can list what tasks are available via <code class=\\\"language-none\\\">--list-tasks</code> and view the task specification via <code class=\\\"language-none\\\">--show-task TASK_NAME</code>.</p>\\n</li>\\n<li>\\n<p>Create a solution to a BenchBot task, and run it against the software stack. To run a solution you must select a mode. For example, if you've created a solution in <code class=\\\"language-none\\\">my_solution.py</code> that you would like to run natively:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_submit --native python my_solution.py\\n</code></pre>\\n<p>See <code class=\\\"language-none\\\">--help</code> for other options. You also have access to all of the examples available in your installation. For instance, you can run the <code class=\\\"language-none\\\">hello_active</code> example in containerised mode via:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_submit --containerised --example hello_active\\n</code></pre>\\n<p>See <code class=\\\"language-none\\\">--list-examples</code> and <code class=\\\"language-none\\\">--show-example EXAMPLE_NAME</code> for full details on what's available out of the box.</p>\\n</li>\\n<li>\\n<p>Evaluate the performance of your system using a supported evaluation method (see <code class=\\\"language-none\\\">--list-methods</code>). To use the <code class=\\\"language-none\\\">omq</code> evaluation method on <code class=\\\"language-none\\\">my_results.json</code>:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_eval --method omq my_results.json\\n</code></pre>\\n<p>You can also simply run evaluation automatically after your submission completes:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_submit --evaluate-results omq --native --example hello_eval_semantic_slam\\n</code></pre>\\n</li>\\n</ol>\\n<p>The <a href=https://github.com/qcr/benchbot/wiki/Tutorial:-Performing-Semantic-SLAM-with-Votenet>BenchBot Tutorial</a> is a great place to start working with BenchBot; the tutorial takes you from a blank system to a working Semantic SLAM solution, with many educational steps along the way. Also remember the examples in your installation (<a href=https://github.com/benchbot-addons/examples_base><code class=\\\"language-none\\\">benchbot-addons/examples_base</code></a> is a good starting point) which show how to get up and running with the BenchBot software stack.</p>\\n<h2>Power tools for autonomous algorithm evaluation</h2>\\n<p>Once you are confident your algorithm is a solution to the chosen task, the BenchBot software stack's power tools allow you to comprehensively explore your algorithm's performance. You can autonomously run your algorithm over multiple environments, and evaluate it holistically to produce a single summary statistic of your algorithm's performance. Here are some examples again with content from the <a href=https://github.com/benchbot-addons/ssu>semantic scene understanding add-on</a>:</p>\\n<ul>\\n<li>\\n<p>Use <code class=\\\"language-none\\\">benchbot_batch</code> to run your algorithm in a number of environments and produce a set of results. The script has a number of toggles available to customise the process (see <code class=\\\"language-none\\\">--help</code> for full details). To autonomously run your <code class=\\\"language-none\\\">semantic_slam:active:ground_truth</code> algorithm over 3 environments:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --native python my_solution.py\\n</code></pre>\\n<p>Or you can use one of the pre-defined environment batches installed via add-ons (e.g. <a href=https://github.com/benchbot-addons/batches_isaac><code class=\\\"language-none\\\">benchbot-addons/batches_isaac</code></a>):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs-batch develop_1 --native python my_solution.py\\n</code></pre>\\n<p>Additionally, you can create a results ZIP and request an overall evaluation score at the end of the batch:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --zip --score-results omq --native python my_solution.py\\n</code></pre>\\n<p>Lastly, both native and containerised submissions are supported exactly as in <code class=\\\"language-none\\\">benchbot_submit</code>:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --containerised my_solution_folder/\\n</code></pre>\\n</li>\\n<li>\\n<p>You can also directly call the holistic evaluation performed above by <code class=\\\"language-none\\\">benchbot_batch</code> through the <code class=\\\"language-none\\\">benchbot_eval</code> script. The script supports single result files, multiple results files, or a ZIP of multiple results files. See <code class=\\\"language-none\\\">benchbot_eval --help</code> for full details. Below are examples calling <code class=\\\"language-none\\\">benchbot_eval</code> with a series of results and a ZIP of results respectively:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_eval --method omq -o my_jsons_scores result_1.json result_2.json result_3.json\\n</code></pre>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">u@pc:~$ benchbot_eval --method omq -o my_zip_scores results.zip\\n</code></pre>\\n</li>\\n</ul>\\n<h2>Using BenchBot in your research</h2>\\n<p>BenchBot was made to enable and assist the development of high quality, repeatable research results. We welcome any and all use of the BenchBot software stack in your research.</p>\\n<p>To use our system, we just ask that you cite our paper on the BenchBot system. This will help us follow uses of BenchBot in the research community, and understand how we can improve the system to help support future research results. Citation details are as follows:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@misc{talbot2020benchbot,\\n    title={BenchBot: Evaluating Robotics Research in Photorealistic 3D Simulation and on Real Robots},\\n    author={Ben Talbot and David Hall and Haoyang Zhang and Suman Raj Bista and Rohan Smith and Feras Dayoub and Niko SÃ¼nderhauf},\\n    year={2020},\\n    eprint={2008.00635},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.RO}\\n}\\n</code></pre>\\n<h2>Components of the BenchBot software stack</h2>\\n<p>The BenchBot software stack is split into a number of standalone components, each with their own GitHub repository and documentation. This repository glues them all together for you into a working system. The components of the stack are:</p>\\n<ul>\\n<li><strong><a href=https://github.com/qcr/benchbot_api>benchbot_api</a>:</strong> user-facing Python interface to the BenchBot system, allowing the user to control simulated or real robots in simulated or real world environments through simple commands</li>\\n<li><strong><a href=https://github.com/qcr/benchbot_addons>benchbot_addons</a>:</strong> a Python manager for add-ons to a BenchBot system, with full documentation on how to create and add your own add-ons</li>\\n<li><strong><a href=https://github.com/qcr/benchbot_supervisor>benchbot_supervisor</a>:</strong> a HTTP server facilitating communication between user-facing interfaces and the underlying robot controller</li>\\n<li><strong><a href=https://github.com/qcr/benchbot_robot_controller>benchbot_robot_controller</a>:</strong> a wrapping script which controls the low-level ROS functionality of a simulator or real robot, handles automated subprocess management, and exposes interaction via a HTTP server</li>\\n<li><strong><a href=https://github.com/qcr/benchbot_simulator>benchbot_simulator</a>:</strong> a realistic 3D simulator employing Nvidia's Isaac framework, in combination with Unreal Engine environments</li>\\n<li><strong><a href=https://github.com/qcr/benchbot_eval>benchbot_eval</a>:</strong> Python library for evaluating the performance in a task, based on the results produced by a submission</li>\\n</ul>\\n<h2>Further information</h2>\\n<ul>\\n<li><strong><a href=https://github.com/qcr/benchbot/wiki/FAQs>FAQs</a>:</strong> Wiki page where answers to frequently asked questions and resolutions to common issues will be provided</li>\\n<li><strong><a href=https://github.com/qcr/benchbot/wiki/Tutorial:-Performing-Semantic-SLAM-with-Votenet>Semantic SLAM Tutorial</a>:</strong> a tutorial stepping through creating a semantic SLAM system in BenchBot that utilises the 3D object detector <a href=https://github.com/facebookresearch/votenet>VoteNet</a></li>\\n</ul>\\n<h2>Supporters</h2>\\n<p>Development of the BenchBot software stack was directly supported by:</p>\\n<p><a href=\\\"https://www.roboticvision.org/\\\"><img src=\\\"/_next/static/images/acrv_logo_small-e816f01e0557cf5cee1e9eb709d9a5e5.png\\\" alt=\\\"Australian Centre for Robotic Vision\\\"></a>â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ<a href=\\\"https://research.qut.edu.au/qcr/\\\"><img src=\\\"data:image/png;base64,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\\\" alt=\\\"QUT Centre for Robotics\\\"></a></p>\\n\",\"name\":\"BenchBot Software Stack\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot\",\"image_position\":\"100% center\",\"src\":\"/content/benchbot/benchbot.md\",\"id\":\"benchbot\",\"image\":\"/_next/static/gifs/0f460afe63fb093a8b44efcd5c652cf8.jpg\",\"_image\":\"/_next/static/gifs/0f460afe63fb093a8b44efcd5c652cf8.webm\"},{\"content\":\"<h1>Delta Descriptors</h1>\\n<p>Source code for the paper - &quot;Delta Descriptors: Change-Based Place Representation for Robust Visual Localization&quot;, published in IEEE Robotics and Automation Letters (RA-L) 2020 and to be presented at IROS 2020. [<a href=\\\"https://arxiv.org/abs/2006.05700\\\">arXiv</a>] [<a href=\\\"https://ieeexplore.ieee.org/document/9128035\\\">IEEE Xplore</a>][<a href=\\\"https://www.youtube.com/watch?v=qY4VobAoLPY\\\">YouTube</a>]</p>\\n<p>We propose Delta Descriptor, defined as a high-dimensional signed vector of change measured across the places observed along a route. Using a difference-based description, places can be effectively recognized despite significant appearance variations.\\n<img src=\\\"/_next/static/images/ral-iros-2020-delta-descriptors-schematic-f941eee94161c1b37bce3429b2adadc3.png\\\" alt=\\\"Schematic of the proposed approach\\\" title=\\\"Schematic of the proposed approach - Delta Descriptors\\\">\\nImages on the left are from the <a href=\\\"https://robotcar-dataset.robots.ox.ac.uk/\\\">Oxford Robotcar</a> dataset.</p>\\n<h2>Requirements</h2>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">matplotlib==2.0.2\\nnumpy==1.15.2\\ntqdm==4.29.1\\nscipy==1.1.0\\nscikit_learn==0.23.1\\n</code></pre>\\n<p>See <code class=\\\"language-none\\\">requirements.txt</code>, generated using <code class=\\\"language-none\\\">pipreqs==0.4.10</code> and <code class=\\\"language-none\\\">python3.5.6</code></p>\\n<h2>Usage</h2>\\n<h4>Download this Repository and the Nordland dataset (part)</h4>\\n<p>The dataset used in our paper is available <a href=\\\"https://zenodo.org/record/4016653#.X1WmYM8zZCV\\\">here</a> (or use commands as below). Note that the download only comprises a small part (~1 GB) of the original Nordland videos released <a href=\\\"https://nrkbeta.no/2013/01/15/nordlandsbanen-minute-by-minute-season-by-season/\\\">here</a>. These videos were first used for visual place recognition in <a href=\\\"https://www.tu-chemnitz.de/etit/proaut/publications/openseqslam.pdf\\\">this</a> paper.</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\"><span class=\\\"token function\\\">git</span> clone https://github.com/oravus/DeltaDescriptors.git\\n<span class=\\\"token builtin class-name\\\">cd</span> DeltaDescriptors/\\n<span class=\\\"token function\\\">mkdir</span> data/\\n<span class=\\\"token builtin class-name\\\">cd</span> data/\\n<span class=\\\"token function\\\">wget</span> https://zenodo.org/record/4016653/files/nordland-part-2020.zip\\n<span class=\\\"token function\\\">unzip</span> nordland-part-2020.zip\\n</code></pre>\\n<p>The zip contains two folders: summer and winter, where each one of them comprises 1750 images which were used for experiments conducted in our paper.</p>\\n<h4>Describe and Match</h4>\\n<p>Delta Descriptors are defined on top of global image descriptors, for example, NetVLAD (<a href=https://github.com/oravus/DeltaDescriptors/tree/master/thirdparty>Update 05 Sep 2020: see our python wrapper</a>). Given such descriptors, compute Delta Descriptors and match across two traverses as below:</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --genDesc --genMatch -l <span class=\\\"token number\\\">16</span> -d delta -ip1 <span class=\\\"token operator\\\">&lt;</span>full_path_of_desc.npy<span class=\\\"token operator\\\">></span> -ip2 <span class=\\\"token operator\\\">&lt;</span>full_path_of_query_desc.npy<span class=\\\"token operator\\\">></span>\\n</code></pre>\\n<p>The input descriptor data is assumed to be a 2D tensor of shape <code class=\\\"language-none\\\">[numImages,numDescDims]</code>. The computed descriptors are stored in <code class=\\\"language-none\\\">.npy</code> format and the match results are stored in <code class=\\\"language-none\\\">.npz</code> format comprising a dict of two arrays: <code class=\\\"language-none\\\">matchInds</code> (matched reference index per query image) and <code class=\\\"language-none\\\">matchDists</code> (corresponding distance value). By default, output is stored in the <code class=\\\"language-none\\\">./out</code> folder but can also be specified via <code class=\\\"language-none\\\">--outPath</code> argument. To see all the options, use:</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --help\\n</code></pre>\\n<p>The options <code class=\\\"language-none\\\">--genDesc</code> and <code class=\\\"language-none\\\">--genMatch</code> can be used in isolation or together, see example usage below.</p>\\n<h4>Describe only</h4>\\n<p>In order to compute only the descriptors for a single traverse, use:</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --genDesc -l <span class=\\\"token number\\\">16</span> -d delta -ip1 <span class=\\\"token operator\\\">&lt;</span>full_path_of_desc.npy<span class=\\\"token operator\\\">></span>\\n</code></pre>\\n<h4>Match only</h4>\\n<p>For only computing matches, given the descriptors (Delta or some other), use:</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --genMatch -ip1 <span class=\\\"token operator\\\">&lt;</span>full_path_of_desc.npy<span class=\\\"token operator\\\">></span> -ip2 <span class=\\\"token operator\\\">&lt;</span>full_path_of_query_desc.npy<span class=\\\"token operator\\\">></span>\\n</code></pre>\\n<h4>Evaluate only</h4>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --eval -mop <span class=\\\"token operator\\\">&lt;</span>full_path_of_match_output.npz<span class=\\\"token operator\\\">></span>\\n</code></pre>\\n<p>or evaluate directly with <code class=\\\"language-none\\\">--genMatch</code> (and possibly <code class=\\\"language-none\\\">--genDesc</code>) flag:</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">python src/main.py --eval --genMatch -ip1 <span class=\\\"token operator\\\">&lt;</span>full_path_of_desc.npy<span class=\\\"token operator\\\">></span> -ip2 <span class=\\\"token operator\\\">&lt;</span>full_path_of_query_desc.npy<span class=\\\"token operator\\\">></span>\\n</code></pre>\\n<p>Currently, only Nordland dataset-style (1-to-1 frame correspondence) evaluation is supported, GPS/INS coordinates-based evaluation, for example, for Oxford Robotcar dataset to be added soon. Evalution code can be used to generate PR curves and the code in its current form prints Precision @ 100% Recall for localization radius of 1, 5, 10 and 20 (frames).</p>\\n<h2>Citation</h2>\\n<p>If you find this code or our work useful, cite it as below:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@article{garg2020delta,\\n  title={Delta Descriptors: Change-Based Place Representation for Robust Visual Localization},\\n  author={Garg, Sourav and Harwood, Ben and Anand, Gaurangi and Milford, Michael},\\n  journal={IEEE Robotics and Automation Letters},\\n  year={2020},\\n  publisher={IEEE},\\n  volume={5},\\n  number={4},\\n  pages={5120-5127},  \\n}\\n</code></pre>\\n<h2>License</h2>\\n<p>The code is released under MIT License.</p>\\n<h2>Related Projects</h2>\\n<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>\\n<p><a href=https://github.com/oravus/seq2single>seq2single (2019)</a></p>\\n<p><a href=https://github.com/oravus/lostX>LoST (2018)</a></p>\\n\",\"name\":\"Delta Descriptors\",\"type\":\"code\",\"url\":\"https://github.com/oravus/DeltaDescriptors\",\"src\":\"/content/visual_place_recognition/delta-descriptors.md\",\"id\":\"delta-descriptors\",\"image_position\":\"center\",\"image\":\"/_next/static/images/ral-iros-2020-delta-descriptors-schematic-f941eee94161c1b37bce3429b2adadc3.png\"},{\"content\":\"<h1>PGraph: simple graphs for Python</h1>\\n<p><img src=\\\"https://img.shields.io/pypi/dw/pgraph-python\\\" alt=\\\"pypi downloads\\\">\\n<a href=\\\"https://pypi.python.org/pypi/pgraph-python/\\\"><img src=\\\"https://badge.fury.io/py/pgraph-python.svg\\\" alt=\\\"PyPI version fury.io\\\"></a>\\n<a href=\\\"https://lgtm.com/projects/g/petercorke/pgraph-python/context:python\\\"><img src=\\\"https://img.shields.io/lgtm/grade/python/g/petercorke/pgraph-python.svg?logo=lgtm&amp;logoWidth=18\\\" alt=\\\"Language grade: Python\\\"></a>\\n<a href=\\\"https://pypi.python.org/pypi/pgraph-python/\\\"><img src=\\\"https://img.shields.io/pypi/pyversions/pgraph-python\\\" alt=\\\"PyPI pyversions\\\"></a>\\n<a href=https://github.com/petercorke/pgraph-python/graphs/commit-activity><img src=\\\"https://img.shields.io/badge/Maintained%3F-yes-green.svg\\\" alt=\\\"Maintenance\\\"></a>\\n<a href=https://github.com/petercorke/pgraph-python/blob/master/LICENSE><img src=\\\"https://img.shields.io/github/license/Naereen/StrapDown.js.svg\\\" alt=\\\"GitHub license\\\"></a>\\n<a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<ul>\\n<li>GitHub repository: <a href=https://github.com/petercorke/pgraph-python>https://github.com/petercorke/pgraph-python</a></li>\\n<li>Documentation: <a href=\\\"https://petercorke.github.io/pgraph-python\\\">https://petercorke.github.io/pgraph-python</a></li>\\n<li>Dependencies: <code class=\\\"language-none\\\">numpy</code></li>\\n</ul>\\n<p>This Python package allows the manipulation of directed and non-directed graphs.  Also supports embedded graphs.  It is suitable for graphs with thousands of nodes.</p>\\n<p><img src=https://github.com/petercorke/pgraph-python/raw/master/examples/roads.png alt=\\\"road network\\\"></p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">from pgraph import *\\nimport json\\n\\n# load places and routes\\nwith open('places.json', 'r') as f:\\n    places = json.loads(f.read())\\nwith open('routes.json', 'r') as f:\\n    routes = json.loads(f.read())\\n\\n# build the graph\\ng = UGraph()\\n\\nfor name, info in places.items():\\n    g.add_vertex(name=name, coord=info[&quot;utm&quot;])\\n\\nfor route in routes:\\n    g.add_edge(route[0], route[1], cost=route[2])\\n\\n# plan a path from Hughenden to Brisbane\\np = g.path_Astar('Hughenden', 'Brisbane')\\ng.plot(block=False) # plot it\\ng.highlight_path(p)  # overlay the path\\n</code></pre>\\n<h3>Properties and methods of the graph</h3>\\n<p>Graphs belong to the class <code class=\\\"language-none\\\">UGraph</code> or <code class=\\\"language-none\\\">DGraph</code> for undirected or directed graphs respectively.  The graph is essentially a container for the vertices.</p>\\n<ul>\\n<li>\\n<p><code class=\\\"language-none\\\">g.add_vertex()</code> add a vertex</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.n</code> the number of vertices</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g</code> is an iterator over vertices, can be used as <code class=\\\"language-none\\\">for vertex in g:</code></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g[i]</code> reference a vertex by its index or name</p>\\n<hr>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.add_edge()</code> connect two vertices</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.edges()</code> all edges in the graph</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.plot()</code> plots the vertices and edges</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.nc</code> the number of graph components, 1 if fully connected</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.component(v)</code> the component that vertex <code class=\\\"language-none\\\">v</code> belongs to</p>\\n<hr>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.path_BFS()</code> breadth-first search</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.path_Astar()</code> A* search</p>\\n<hr>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.adjacency()</code> adjacency matrix</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.Laplacian()</code> Laplacian matrix</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">g.incidence()</code> incidence matrix</p>\\n</li>\\n</ul>\\n<h3>Properties and methods of a vertex</h3>\\n<p>Vertices belong to the class <code class=\\\"language-none\\\">UVertex</code> (for undirected graphs) or <code class=\\\"language-none\\\">DVertex</code> (for directed graphs), which are each subclasses of <code class=\\\"language-none\\\">Vertex</code>.</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">v.coord</code> the coordinate vector for embedded graph (optional)</li>\\n<li><code class=\\\"language-none\\\">v.name</code> the name of the vertex (optional)</li>\\n<li><code class=\\\"language-none\\\">v.neighbours()</code> is a list of the neighbouring vertices</li>\\n<li><code class=\\\"language-none\\\">v1.samecomponent(v2)</code> predicate for vertices belonging to the same component</li>\\n</ul>\\n<p>Vertices can be named and referenced by name.</p>\\n<h3>Properties and methods of an edge</h3>\\n<p>Edges are instances of the class <code class=\\\"language-none\\\">Edge</code>.\\nEdges are not referenced by the graph object, each edge references a pair of vertices, and the vertices reference the edges.  For a directed graph only the start vertex of an edge references the edge object, whereas for an undirected graph both vertices reference the edge object.</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">e.cost</code> cost of edge for planning methods</li>\\n<li><code class=\\\"language-none\\\">e.next(v)</code> vertex on edge <code class=\\\"language-none\\\">e</code> that is not <code class=\\\"language-none\\\">v</code></li>\\n<li><code class=\\\"language-none\\\">e.v1</code>, <code class=\\\"language-none\\\">e.v2</code> the two vertices that define the edge <code class=\\\"language-none\\\">e</code></li>\\n</ul>\\n<h2>Modifying a graph</h2>\\n<ul>\\n<li><code class=\\\"language-none\\\">g.remove(v)</code> remove vertex <code class=\\\"language-none\\\">v</code></li>\\n<li><code class=\\\"language-none\\\">e.remove()</code> remove edge <code class=\\\"language-none\\\">e</code></li>\\n</ul>\\n<h2>Subclasing pgraph classes</h2>\\n<p>Consider a user class <code class=\\\"language-none\\\">Foo</code> that we would like to connect using a graph <em>overlay</em>, ie.\\ninstances of <code class=\\\"language-none\\\">Foo</code> becomes vertices in a graph.</p>\\n<ul>\\n<li>Have it subclass either <code class=\\\"language-none\\\">DVertex</code> or <code class=\\\"language-none\\\">UVertex</code> depending on graph type</li>\\n<li>Then place instances of <code class=\\\"language-none\\\">Foo</code> into the graph using <code class=\\\"language-none\\\">add_vertex</code> and create edges as required</li>\\n</ul>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">class Foo(UVertex):\\n  # foo stuff goes here\\n  \\nf1 = Foo(...)\\nf2 = Foo(...)\\n\\ng = UGraph() # create a new undirected graph\\ng.add_vertex(f1)\\ng.add_vertex(f2)\\n\\nf1.connect(f2, cost=3)\\nfor f in f1.neighbours():\\n    # say hi to the neighbours\\n</code></pre>\\n<h2>Under the hood</h2>\\n<p>The key objects and their interactions are shown below.</p>\\n<p><img src=https://github.com/petercorke/pgraph-python/raw/master/docs/source/datastructures.png alt=\\\"data structures\\\"></p>\\n<h2>MATLAB version</h2>\\n<p>This is a re-engineered version of <a href=https://github.com/petercorke/spatialmath-matlab/blob/master/PGraph.m>PGraph.m</a> which ships as part of the <a href=https://github.com/petercorke/spatialmath-matlab>Spatial Math Toolbox for MATLAB</a>.  This class is used to support bundle adjustment, pose-graph SLAM and various planners such as PRM, RRT and Lattice.</p>\\n<p>The Python version was designed from the start to work with directed and undirected graphs, whereas directed graphs were a late addition to the MATLAB version.  Semantics are similar but not identical.  In particular the use of subclassing rather than references to\\n<em>user data</em> is encouraged.</p>\\n\",\"name\":\"Graph classes (Python)\",\"type\":\"code\",\"url\":\"https://github.com/petercorke/pgraph-python\",\"image\":\"https://github.com/petercorke/pgraph-python/raw/master/examples/roads.png\",\"src\":\"/content/pgraph-python.md\",\"id\":\"pgraph-python\",\"image_position\":\"center\"},{\"content\":\"<h1>LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics</h1>\\n<p>This is the source code for the paper titled - &quot;LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics&quot;, pre-print available <a href=\\\"https://arxiv.org/abs/1804.05526\\\">here</a>.</p>\\n<p>An example output image showing Keypoint Correspondences:</p>\\n<p><img src=\\\"/_next/static/images/day-night-keypoint-correspondence-place-recognition-591f10ce5848add43b1423bfc16eafe6.jpg\\\" alt=\\\"An example output image showing Keypoint Correspondences\\\" title=\\\"Keypoint Correspondences using LoST-X\\\"></p>\\n<p>Flowchart of the proposed approach:</p>\\n<p><img src=\\\"/_next/static/images/LoST-Flowchart-Visual_Place_Recognition-5bb346cb2e424ba34d9ac4118afe3800.jpg\\\" alt=\\\"Flowchart of the proposed approach\\\" title=\\\"Flowchart for the proposed approach - LoST-X\\\"></p>\\n<p>If you find this work useful, please cite it as:<br>\\nSourav Garg, Niko Sunderhauf, and Michael Milford. LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics. Proceedings of Robotics: Science and Systems XIV, 2018.<br>\\nbibtex:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@article{garg2018lost,\\ntitle={LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics},\\nauthor={Garg, Sourav and Suenderhauf, Niko and Milford, Michael},\\njournal={Proceedings of Robotics: Science and Systems XIV},\\nyear={2018}\\n}\\n</code></pre>\\n<p>RefineNet's citation as mentioned on their <a href=https://github.com/guosheng/refinenet>Github page</a>.</p>\\n<h2>Setup and Run</h2>\\n<h4>Dependencies</h4>\\n<ul>\\n<li>Ubuntu        (Tested on <em>14.04</em>)</li>\\n<li><a href=\\\"https://arxiv.org/abs/1611.06612\\\">RefineNet</a>\\n<ul>\\n<li>Required primarily for visual semantic information. Convolutional feature maps based dense descriptors are also extracted from the same.</li>\\n<li>A <a href=https://github.com/oravus/refinenet>modified fork</a> of RefineNet's <a href=https://github.com/guosheng/refinenet>code</a> is used in this work to simultaneously store convolutional dense descriptors.</li>\\n<li>Requires Matlab      (Tested on <em>2017a</em>)</li>\\n</ul>\\n</li>\\n<li>Python        (Tested on <em>2.7</em>)\\n<ul>\\n<li>numpy       (Tested on <em>1.11.1</em>, <em>1.14.2</em>)</li>\\n<li>scipy       (Tested on <em>0.13.3</em>, <em>0.17.1</em>)</li>\\n<li>skimage     (Minimum Required <em>0.13.1</em>)</li>\\n<li>sklearn     (Tested on <em>0.14.1</em>, <em>0.19.1</em>)</li>\\n<li>h5py        (Tested on <em>2.7.1</em>)</li>\\n</ul>\\n</li>\\n<li>Docker (optional, recommended, tested on <em>17.12.0-ce</em>)\\n<ul>\\n<li><a href=\\\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\\\">Official page for install instructions</a></li>\\n</ul>\\n</li>\\n</ul>\\n<h4>Download</h4>\\n<ol>\\n<li>In your workspace, clone the repositories:<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/oravus/lostX.git\\ncd lostX\\ngit clone https://github.com/oravus/refinenet.git\\n</code></pre>\\nNOTE: If you download this repository as a zip, the refineNet's fork will not get downloaded automatically, being a git submodule.</li>\\n<li>Download the Resnet-101 model pre-trained on Cityscapes dataset from <a href=\\\"https://drive.google.com/drive/folders/1U2c1N6QJdzB_8HBgXb7mJ6Qk66JDBHI9\\\">here</a> or <a href=\\\"https://pan.baidu.com/s/1nxf2muP#list/path=%2Frefinenet_public_new%2Frefinenet_released%2Frefinenet_res101&amp;parentPath=%2Frefinenet_public_new%2Frefinenet_released\\\">here</a>. More details on RefineNet's <a href=https://github.com/guosheng/refinenet>Github page</a>.\\n<ul>\\n<li>Place the downloaded model's <code class=\\\"language-none\\\">.mat</code> file in the <code class=\\\"language-none\\\">refinenet/model_trained/</code> directory.</li>\\n</ul>\\n</li>\\n<li>If you are using docker, download the docker image:<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">docker pull souravgarg/vpr-lost-kc:v1\\n</code></pre>\\n</li>\\n</ol>\\n<h4>Run</h4>\\n<ol>\\n<li>\\n<p>Generate and store semantic labels and dense convolutional descriptors from RefineNet's <em>conv5</em> layer\\nIn the MATLAB workspace, from the <code class=\\\"language-none\\\">refinenet/main/</code> directory, run:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">demo_predict_mscale_cityscapes\\n</code></pre>\\n<p>The above will use the sample dataset from <code class=\\\"language-none\\\">refinenet/datasets/</code> directory. You can set path to your data in <code class=\\\"language-none\\\">demo_predict_mscale_cityscapes.m</code> through variable <code class=\\\"language-none\\\">datasetName</code> and <code class=\\\"language-none\\\">img_data_dir</code>.<br>\\nYou might have to run <code class=\\\"language-none\\\">vl_compilenn</code> before running the demo, please refer to the instructions for running refinenet in their official <a href=https://github.com/guosheng/refinenet>Readme.md</a></p>\\n</li>\\n<li>\\n<p>[For Docker users]<br>\\nIf you have an environment with python and other dependencies installed, skip this step, otherwise run a docker container:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">docker run -it -v PATH_TO_YOUR_HOME_DIRECTORY/:/workspace/ souravgarg/vpr-lost-kc:v1 /bin/bash\\n</code></pre>\\n<p>From within the docker container, navigate to <code class=\\\"language-none\\\">lostX/lost_kc/</code> repository.<br>\\n<code class=\\\"language-none\\\">-v</code> option mounts the <em>PATH_TO_YOUR_HOME_DIRECTORY</em> to <em>/workspace</em> directory within the docker container.</p>\\n</li>\\n<li>\\n<p>Reformat and pre-process RefineNet's output from <code class=\\\"language-none\\\">lostX/lost_kc/</code> directory:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">python reformat_data.py -p $PATH_TO_REFINENET_OUTPUT\\n</code></pre>\\n<p>$PATH_TO_REFINENET_OUTPUT is set to be the parent directory of <code class=\\\"language-none\\\">predict_result_full</code>, for example, <em>../refinenet/cache_data/test_examples_cityscapes/1-s_result_20180427152622_predict_custom_data/predict_result_1/</em></p>\\n</li>\\n<li>\\n<p>Compute LoST descriptor:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">python LoST.py -p $PATH_TO_REFINENET_OUTPUT \\n</code></pre>\\n</li>\\n<li>\\n<p>Repeat step 1, 3, and 4 to generate output for the other dataset by setting the variable <code class=\\\"language-none\\\">datasetName</code> to <code class=\\\"language-none\\\">2-s</code>.</p>\\n</li>\\n<li>\\n<p>Perform place matching using LoST descriptors based difference matrix and Keypoint Correspondences:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">python match_lost_kc.py -n 10 -f 0 -p1 $PATH_TO_REFINENET_OUTPUT_1  -p2 $PATH_TO_REFINENET_OUTPUT_2\\n</code></pre>\\n</li>\\n</ol>\\n<p>Note: Run <code class=\\\"language-none\\\">python FILENAME -h</code> for any of the python source files in Step 3, 4, and 6 for description of arguments passed to those files.</p>\\n<h2>License</h2>\\n<p>The code is released under MIT License.</p>\\n<h2>Related Projects</h2>\\n<p><a href=https://github.com/oravus/DeltaDescriptors>Delta Descriptors (2020)</a></p>\\n<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>\\n<p><a href=https://github.com/oravus/seq2single>seq2single (2019)</a></p>\\n\",\"name\":\"LoST-X\",\"type\":\"code\",\"url\":\"https://github.com/oravus/lostX\",\"src\":\"/content/visual_place_recognition/lost.md\",\"id\":\"lost\",\"image_position\":\"center\",\"image\":\"/_next/static/images/day-night-keypoint-correspondence-place-recognition-591f10ce5848add43b1423bfc16eafe6.jpg\"},{\"content\":\"<h1>OpenSeqSLAM2.0 Toolbox</h1>\\n<p><img src=\\\"/_next/static/images/openseqslam2-e3f1e85549e34343dc852ed782b11093.png\\\" alt=\\\"The various interactive screens in OpenSeqSLAM2\\\"></p>\\n<p>OpenSeqSLAM2.0 is a MATLAB toolbox that allows users to thoroughly explore the SeqSLAM method in addressing the visual place recognition problem. The visual place recognition problem is centred around recognising a previously traversed route, regardless of whether it is seen during the day or night, in clear or inclement conditions, or in summer or winter. Recognising previously traversed routes is a crucial capability of navigating robots. Through the graphical interfaces packaged in OpenSeqSLAM2 users are able to:</p>\\n<ul>\\n<li>explore a number of previously published variations to the SeqSLAM method (including search and match selection methods);</li>\\n<li>visually track progress;</li>\\n<li>interactively tune parameters;</li>\\n<li>dynamically reconfigure matching parameters while viewing results;</li>\\n<li>explore precision-recall statistics;</li>\\n<li>visualise difference matrices, match sequence images, and image pre-processing steps;</li>\\n<li>view and export matching videos;</li>\\n<li>automatically optimise selection thresholds against a ground truth;</li>\\n<li>sweep any numeric parameter value through a batch operation mode; and</li>\\n<li>operate in headless mode with parallelisation available.</li>\\n</ul>\\n<p>The toolbox is open-source and downloadable from the <a href=https://github.com/qcr/openseqslam2/releases>releases tab</a>. All we ask is that if you use OpenSeqSLAM2 in any academic work, that you include a reference to corresponding publication (bibtex is available at the bottom of the page).</p>\\n<h2>How to use the toolbox</h2>\\n<p>The toolbox is designed to be simple to use (it runs out of the box without any initial configuration required). To run the toolbox, simple run the command below (with the toolbox root directory in your MATLAB path):</p>\\n<pre class=\\\"language-matlab\\\"><code class=\\\"language-matlab\\\"><span class=\\\"token function\\\">OpenSeqSLAM2</span><span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">;</span>\\n</code></pre>\\n<p>There are a number of default configuration files included in the <code class=\\\"language-none\\\">.config</code> directory which showcase the capabilities of the toolbox. To use a configuration file, open the toolbox as described above, then use the <code class=\\\"language-none\\\">Import config</code> button. A summary of the features showcased in each of the configuration files is included below:</p>\\n<ul>\\n<li><strong><code class=\\\"language-none\\\">'images_same'</code></strong>: The trimmed Nordland dataset images, with the same dataset used as both reference and query. Trajectory based search is used, and a velocity-based ground truth is included, but not used for auto-optimisation of match threshold.</li>\\n<li><strong><code class=\\\"language-none\\\">'images_diff'</code></strong>: The trimmed Nordland dataset images, with the summer traversal used as the reference dataset and the winter traversal as the query. Trajectory based search is used, and a *.csv based ground truth is used for auto-optimising the match threshold selection.</li>\\n<li><strong><code class=\\\"language-none\\\">'videos_same'</code></strong>: The day night video dataset, with the same video used as both the reference and query dataset. Trajectory based search is used, with no ground truth provided.</li>\\n<li><strong><code class=\\\"language-none\\\">'videos_diff'</code></strong>: The day night video dataset, with the day traversal used as the reference dataset and the night traversal as the query. Trajectory based search is used, with no ground truth provided.</li>\\n<li><strong><code class=\\\"language-none\\\">'hybrid_search'</code></strong>: Same as <code class=\\\"language-none\\\">'videos_diff'</code>, but the hybrid search is used instead of trajectory search.</li>\\n<li><strong><code class=\\\"language-none\\\">'no_gui'</code></strong>: Same as <code class=\\\"language-none\\\">'videos_diff'</code>, but the progress is presented in the console rather than GUI and no results GUI is shown (tip: run OpenSeqSLAM2(â€˜<configpath>/no_gui.xmlâ€™) to see how the toolbox can run entirely headless)</li>\\n<li><strong><code class=\\\"language-none\\\">'batch_with_gui'</code></strong>: Same as <code class=\\\"language-none\\\">'images_diff'</code>, but a batch parameter sweep of the sequence length parameter is performed. The progress GUI shows the progress of the individual iteration and overall in separate windows.</li>\\n<li><strong><code class=\\\"language-none\\\">'parrallelised_batch'</code></strong>: Same as <code class=\\\"language-none\\\">'batch_with_gui'</code>, but the parameter sweep is done in parallel mode (which cannot be performed with the Progress GUI). The parallel mode will use a worker for each core available in the host CPU.</li>\\n<li><strong><code class=\\\"language-none\\\">'default'</code></strong>: is set to <code class=\\\"language-none\\\">'images_diff'</code></li>\\n</ul>\\n<p><em><em>Note:</em> the programs in the <code class=\\\"language-none\\\">./bin</code> directory can be run standalone by providing the appropriate results / config structs as arguments if you would like to use only a specific part of the pipeline (i.e. only configuration, or progress wrapped execution, or viewing results).</em></p>\\n<h2>Citation details</h2>\\n<p>If using the toolbox in any academic work, please include the following citation:</p>\\n<pre class=\\\"language-bibtex\\\"><code class=\\\"language-bibtex\\\">@ARTICLE{2018openseqslam2,\\n   author = {{Talbot}, B. and {Garg}, S. and {Milford}, M.},\\n    title = &quot;{OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions}&quot;,\\n  journal = {ArXiv e-prints},\\narchivePrefix = &quot;arXiv&quot;,\\n   eprint = {1804.02156},\\n primaryClass = &quot;cs.RO&quot;,\\n keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},\\n     year = 2018,\\n    month = apr,\\n   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402156T},\\n  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\\n}\\n</code></pre>\\n\",\"name\":\"OpenSeqSLAM2\",\"type\":\"code\",\"url\":\"https://github.com/qcr/openseqslam2\",\"src\":\"/content/visual_place_recognition/openseqslam2.md\",\"id\":\"openseqslam2\",\"image_position\":\"center\",\"image\":\"/_next/static/images/openseqslam2-e3f1e85549e34343dc852ed782b11093.png\"},{\"content\":\"<p><a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<h1>Probability-based Detection Quality (PDQ)</h1>\\n<p>This repository contains the implementation of the probability-based detection quality (PDQ) evaluation measure.\\nThis enables <strong>quantitative</strong> analysis of the <strong>spatial and semantic uncertainties</strong> output by a probabilistic object detecttion (PrOD) system.\\nThis repository provides tools for analysing PrOD detections and classical detections using mAP, moLRP, and PDQ (note that PDQ results will be low for a classical detector and mAP and moLRP scores will likely be low for PrOD detections).\\nEvaluation can be performed both on COCO formatted data and on RVC1 (PrOD challenge) formatted data.\\nThe repository also provides visualization tools to enable fine-grained analysis of PDQ results as shown below.</p>\\n<p><img src=\\\"/_next/static/images/PDQ_Examples-360b2ea2f2877553dc4e8ef94bb13bde.jpg\\\" alt=\\\"PrOD evaluation visualization image examples\\\"></p>\\n<p>The code here, particularly for evaluating RVC1 data is based heavily on the PrOD challenge code which can be found\\nhere: https://github.com/jskinn/rvchallenge-evaluation</p>\\n<p>Note that some extra funcitonality for PDQ outside of what is reported in the original paper and challenge is also provided such as evaluating results using the bounding boxes of the ground-truth segmentation masks, probabilistic segmentation evaluation, a greedy alternative to PDQ.</p>\\n<p>For further details on the robotic vision challenges please see the following links for more details:</p>\\n<ul>\\n<li>Robotic Vision Challenges Homepage: http://roboticvisionchallenge.org/</li>\\n<li>PrOD Main Challenge Page: https://competitions.codalab.org/competitions/20597</li>\\n<li>PrOD Continuous Challenge Page: https://competitions.codalab.org/competitions/20595</li>\\n</ul>\\n<h1>Citing PDQ</h1>\\n<p>If you are using PDQ in your research, please cite the paper below:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@inproceedings{hall2020probabilistic,\\n  title={Probabilistic object detection: Definition and evaluation},\\n  author={Hall, David and Dayoub, Feras and Skinner, John and Zhang, Haoyang and Miller, Dimity and Corke, Peter and Carneiro, Gustavo and Angelova, Anelia and S{\\\\&quot;u}nderhauf, Niko},\\n  booktitle={The IEEE Winter Conference on Applications of Computer Vision},\\n  pages={1031--1040},\\n  year={2020}\\n}\\n</code></pre>\\n<h1>Setup</h1>\\n<h2>Install all python requirements</h2>\\n<p>This code comes with a requirements.txt file.\\nMake sure you have installed all libraries as part of your working environment.</p>\\n<h2>Install COCO mAP API</h2>\\n<p>After installing all requirements, you will need to have a fully installed implementation of the COCO API located\\nsomewhere on your machine.\\nYou can download this API here https://github.com/cocodataset/cocoapi.</p>\\n<p>Once this is downloaded and installed, you need to adjust the system path on line 11 of coco_mAP.py and line 16 of\\nread_files.py to match the PythonAPI folder of your COCO API installation.</p>\\n<h2>Add LRP Evaluation Code</h2>\\n<p>You will also require code for using LRP evaluation measures.\\nTo do this you need to simply copy the cocoevalLRP.py file from the LRP github repository to the pycocotools folder within the PythonAPI.\\nYou can download the specific file here https://github.com/cancam/LRP/blob/master/cocoLRPapi-master/PythonAPI/pycocotools/cocoevalLRP.py\\nYou can clone the original repository here https://github.com/cancam/LRP.</p>\\n<p>After cocoevalLRP.py is located in your pycocotools folder, simply adjust the system path on line 11 of coco_LRP.py to match your PythonAPI folder.</p>\\n<h1>Usage</h1>\\n<p>All evaluation code is run on detections saved in .json files formatted as required by the RVC outlined later on.\\nA variation to this is also available for probabilistic segmentation format also described later.\\nIf you are evaluating on COCO data and have saved detections in COCO format, you can convert to RVC1 format using\\n<em>file_convert-coco_to_rvc1.py</em>\\nWhen you have the appropriate files, you can evaluate on mAP, moLRP, and PDQ with <em>evaluate.py</em>.\\nAfter evaluation is complete, you can visualise your detections for a sequence of images w.r.t. PDQ using\\n<em>visualise_pdq_analysis.py</em></p>\\n<p>Evaluation is currently organised so that you can evaluate either on COCO data, or on RVC1 data. Note that RVC1 data\\nexpects multiple sequences rather than a single folder of data.</p>\\n<h2>RVC1 Detection Format</h2>\\n<p>RVC1 detections are saved in a single .json file per sequence being evaluated. Each .json file is formatted as follows:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">{\\n  &quot;classes&quot;: [&lt;an ordered list of class names&gt;],\\n  &quot;detections&quot;: [\\n    [\\n      {\\n        &quot;bbox&quot;: [x1, y1, x2, y2],\\n        &quot;covars&quot;: [\\n          [[xx1, xy1],[xy1, yy1]],\\n          [[xx2, xy2],[xy2, yy2]]\\n        ],\\n        &quot;label_probs&quot;: [&lt;an ordered list of probabilities for each class&gt;]\\n      },\\n      {\\n      }\\n    ],\\n    [],\\n    []\\n    ...\\n  ]\\n}\\n</code></pre>\\n<h3>Important Notes</h3>\\n<p>The two covariance matrices in <code class=\\\"language-none\\\">covars</code> need to be positive semi-definite in order for the code to work. A covariance matrix <code class=\\\"language-none\\\">C</code> is positive semi-definite when its eigenvalues are not negative. You can easily check this condition in python with the following function:</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">def</span> <span class=\\\"token function\\\">is_pos_semidefinite</span><span class=\\\"token punctuation\\\">(</span>C<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">:</span>\\n    <span class=\\\"token keyword\\\">return</span> np<span class=\\\"token punctuation\\\">.</span><span class=\\\"token builtin\\\">all</span><span class=\\\"token punctuation\\\">(</span>np<span class=\\\"token punctuation\\\">.</span>linalg<span class=\\\"token punctuation\\\">.</span>eigvals<span class=\\\"token punctuation\\\">(</span>C<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">>=</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<h3>Probabilistic Segmentation Detections</h3>\\n<p>We now accommodate a way to submit probabilistic segmentation detections.\\nFor this format, a .npy file for each image stores all detection probabilistic segmentation heatmaps for that image.\\nThis 3D array's shape is m x h x w where m is the number of segmentation masks, h is the image height, and w is the\\nimage width.\\nEach detection dictionary now contains the location the .npy file associated with the detection and the mask id for the\\nspecific detection.\\nYou may also define a bounding box to replace the probabilistic segmentation for bounding-box detections and define a\\nchosen class to use for mAP and moLRP evaluation (rather than always using max class of label_probs).</p>\\n<p>Expected format for probabilistic segmentation detection files is as follows:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">{\\n  &quot;classes&quot;: [&lt;an ordered list of class names&gt;],\\n  &quot;detections&quot;: [\\n    [\\n      {\\n        &quot;label_probs&quot;: [&lt;an ordered list of probabilities for each class&gt;],\\n        &quot;masks_file&quot;: &quot;&lt;location of .npy file holding probabilistic segmentation mask&gt;&quot;,\\n        &quot;mask_id&quot;: &lt;index of this detection's mask in mask_file's numpy array&gt;,\\n        &quot;label&quot;: &lt;chosen label within label_probs&gt; (optional),\\n        &quot;bbox&quot;: [x1, y1, x2, y2] (optional for use in mAP and moLRP),\\n      },\\n      {\\n      }\\n    ],\\n    [],\\n    []\\n    ...\\n  ]\\n}\\n</code></pre>\\n<h2>file_convert_coco_to_rvc1.py</h2>\\n<p>To convert coco detections to rvc format simply run:</p>\\n<p><code class=\\\"language-none\\\">python file_convert_coco_to_rvc1.py --coco_gt &lt;gt_json_file&gt; --coco_det &lt;det_json_file&gt; --rvc1_det &lt;output_json_file&gt;</code></p>\\n<p>where <code class=\\\"language-none\\\">&lt;gt_json_file&gt;</code> is the coco format ground-truth json filename, <code class=\\\"language-none\\\">det_json_file</code> is the coco format detection\\njson filename, and <code class=\\\"language-none\\\">output_file</code> is the json filename you will save your rvc1 formatted detections json file.</p>\\n<h3>Important Notes</h3>\\n<p>By default, coco json format does not come with the predicted scores for all the classes available, in which case the conversion script will just\\nextract the score of the chosen class and distribute remaining probability across all others classes. However, this will produce\\nincorrect measures of label quality because it is the probability estimated by the detector for the object's ground-truth class, which might not\\ncorrespond to the chosen class. To facilitate correct measurements, if a detection element in the coco json file (<code class=\\\"language-none\\\">det_json_file</code>) comes with a\\nkey <code class=\\\"language-none\\\">all_scores</code>, the conversion script will consider it as an array of all the scores, and use it instead of the default behaviour.</p>\\n<p>Also, by default, coco json format does not consider the existence of a covariance matrix which is needed for PDQ calculations. The conversion\\nscript assigns by default a zero'ed covariance matrix, but if a detection element in the coco json file (<code class=\\\"language-none\\\">det_json_file</code>) comes with a\\nkey <code class=\\\"language-none\\\">covars</code>, the conversion script will use that covariance matrix instead of the default one with zeros. Please refer to the previous section <code class=\\\"language-none\\\">RVC1 Detection Format</code> for further information on how <code class=\\\"language-none\\\">covars</code> should be formatted in the json file.</p>\\n<h2>evaluate.py</h2>\\n<p>To perform full evaluation simply run:</p>\\n<p><code class=\\\"language-none\\\">python evaluate.py --test_set &lt;test_type&gt; --gt_loc &lt;gt_location&gt; --det_loc &lt;det_location&gt; --save_folder &lt;save_folder&gt; --set_cov &lt;cov&gt; --num_workers &lt;num_workers&gt;</code></p>\\n<p>Optional flags for new functionality include <code class=\\\"language-none\\\">--bbox_gt</code>, <code class=\\\"language-none\\\">--segment_mode</code>, <code class=\\\"language-none\\\">--greedy_mode</code>, and <code class=\\\"language-none\\\">--prob_seg</code>.\\nThere is also an <code class=\\\"language-none\\\">--mAP_heatmap</code> flag but that should not generally be used.</p>\\n<ul>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code class=\\\"language-none\\\">&lt;test_type&gt;</code></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;det_loc&gt;</code> is a string defining either the location of a detection .json file (coco data) or a folder of .json files for\\nmultiple sequences (rvc1 data). Which one it is interpreted as is defined by <code class=\\\"language-none\\\">&lt;test_type&gt;</code>.\\nNote that these detection files must be in rvc1 format.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;save_folder&gt;</code> is a string defining the folder where analysis will be stored in form of scores.txt, and files for visualisations</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of detections.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--bbox_gt</code> flag states that all ground-truth should be teated as bounding boxes for PDQ analysis.\\nAll pixels within the bounding box will be used for analysis and there will be no &quot;ignored&quot; pixels. This enables\\nuse of datasets with no segmentation information provided they are stored in COCO ground-truth format.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--segment_mode</code> flag states that evaluation is performed per-pixel on the ground-truth segments with no &quot;ignored&quot;\\npixels to accommodate box-shaped detections. This should only be used if evaluating a probabilistic segmentation\\ndetection system.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--greedy_mode</code> flag states that assignment of detections to ground-truth objects based upon pPDQ scores is done\\ngreedily rather than optimal assignment. Greedy mode can be faster for some applications but does not match &quot;official&quot;\\nPDQ process and there may be some minuscule difference in score/behaviour.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--prob_seg</code> flag states that detection.json file is formatted for probabilistic segmentation detections as outlined\\nabove.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--mAP_heatmap</code> flag should not generally be used but enables mAP/moLRP evaluation to be based not upon corners\\ndefined by PBox/BBox detections, but that encompass all pixels of the detection above given threshold of probability\\n(0.0027).</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--num_workers</code> number of parallel worker processes to use in the CPU when making the calculations for the PDQ score. By default, this value is 6.</p>\\n</li>\\n</ul>\\n<p>For further details, please consult the code.</p>\\n<h3>Important Notes</h3>\\n<p>For consistency reasons, unlike the original rvc1 evaluation code, we do not multiply PDQ by 100 to provide it as a percentage.\\nPDQ is also labelled as &quot;PDQ&quot; in scores.txt rather than simply &quot;score&quot;.</p>\\n<p>For anyone unfamiliar with moLRP based measures, these values are losses and not qualities like all other provided measures.\\nTo transform these results from losses to qualities simply take 1 - moLRP.</p>\\n<p>Newly implemented modes <code class=\\\"language-none\\\">--segment_mode</code>, <code class=\\\"language-none\\\">--bbox_gt</code>, <code class=\\\"language-none\\\">greedy_mode</code> are not used for the RVC1 challenge but can be\\nuseful for developing research in probabilistic segmentation, when your dataset does not have a segmentation mask, or\\nwhen time is critical, respectively.</p>\\n<h2>visualise_pdq_analysis.py</h2>\\n<p>To create visualisations for probabilistic detections and PDQ analysis on a single sequence of images run:</p>\\n<p><code class=\\\"language-none\\\">python visualise_pdq_analysis.py --data_type &lt;test_type&gt; --ground_truth &lt;gt_location&gt; --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --gt_analysis &lt;gt_analysis_file&gt; --det_analysis &lt;det_analysis_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --colour_mode &lt;colour_mode&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt; --full_info</code></p>\\n<p>where:</p>\\n<ul>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code class=\\\"language-none\\\">&lt;test_type&gt;</code></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;gt_analysis&gt;</code> a string defining the ground-truth analysis .json file matching the sequence to be visualised.\\nMust also correspond to the detection .json file being visualised.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;det_analysis&gt;</code> a string defining the detection analysis .json file matching the sequence to be visualised.\\nMust also correspond to the detection .json file being visualised.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code class=\\\"language-none\\\">&lt;gt_imgs_location&gt;</code></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections. <strong>This must match the set covariance used in evaluate.py</strong></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;colour_mode&gt;</code> is a string defining whether correct and incorrect results are coloured green and red ('gr') or blue and orange ('bo') respectively.\\nDefault option is blue and orange.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\\nArrows show 2 x standard deviation along the major axes of the Gaussian.\\nDefault option is 'ellipse'</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\\nFor example <code class=\\\"language-none\\\">--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for &quot;cat.jpg&quot;, &quot;dog.jpg&quot;, and &quot;whale.jpg&quot;.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">--full_info</code> is an optional flag defining whether full pairwise quality analysis should be written for TP detections. <strong>Recommended setting for in-depth analysis</strong></p>\\n</li>\\n</ul>\\n<p>For further details, please consult the code.</p>\\n<h3>Important Notes</h3>\\n<p>Consistency must be kept between ground-truth analysis, detection analysis, and detection .json files in order to provide meaningful visualisation.</p>\\n<p>If the evaluation which produced the ground-truth analysis and detection analysis used a set covariance input, you must\\nprovide that same set covariance when generating visualisations.</p>\\n<p>New modes such as using probabilistic segmentation detections (<code class=\\\"language-none\\\">--prob_seg</code>) in segment mode (<code class=\\\"language-none\\\">--segment_mode</code>)\\nor using bounding_box ground-truth (<code class=\\\"language-none\\\">--bbox_gt</code>) in the evaluation code are <strong>NOT</strong> yet supported.</p>\\n<h2>visualise_prob_detections.py</h2>\\n<p>To create visualisations for probabilistic detections on a single sequence of images run:</p>\\n<p><code class=\\\"language-none\\\">python visualise_prob_detections.py --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt;</code></p>\\n<p>where:</p>\\n<ul>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code class=\\\"language-none\\\">&lt;gt_imgs_location&gt;</code></p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\\nArrows show 2 x standard deviation along the major axes of the Gaussian.\\nDefault option is 'ellipse'</p>\\n</li>\\n<li>\\n<p><code class=\\\"language-none\\\">&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\\nFor example <code class=\\\"language-none\\\">--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for &quot;cat.jpg&quot;, &quot;dog.jpg&quot;, and &quot;whale.jpg&quot;.</p>\\n</li>\\n</ul>\\n<p>For further details, please consult the code.</p>\\n<h3>Important Notes</h3>\\n<p>Order of detections in detections.json file must match the order of the images as stored in the ground-truth images\\nfolder.</p>\\n<p>New modes such as using probabilistic segmentation detections (<code class=\\\"language-none\\\">--prob_seg</code>) in the evaluation code are\\n<strong>NOT</strong> yet supported.</p>\\n<h1>Acknowledgements</h1>\\n<p>Development of the probability-based detection quality evaluation measure was directly supported by:</p>\\n<p><img src=\\\"/_next/static/images/acrv_logo_small-e816f01e0557cf5cee1e9eb709d9a5e5.png\\\" alt=\\\"Australian Centre for Robotic Vision\\\"></p>\\n\",\"name\":\"Probability-based Detection Quality (PDQ)\",\"type\":\"code\",\"url\":\"https://github.com/david2611/pdq_evaluation\",\"image\":\"/_next/static/images/qcr_web_img-1fefd82c6dfb30c8aeeb9c4fbfbe7253.jpg\",\"image_fit\":\"contain\",\"src\":\"/content/pdq.md\",\"id\":\"pdq\",\"image_position\":\"center\"},{\"content\":\"<h1>RT-GENE &amp; RT-BENE: Real-Time Eye Gaze and Blink Estimation in Natural Environments</h1>\\n<p><a href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\"><img src=\\\"https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg?style=flat-square\\\" alt=\\\"License: CC BY-NC-SA 4.0\\\"></a>\\n<a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md><img src=\\\"http://hits.dwyl.io/Tobias-Fischer/rt_gene.svg\\\" alt=\\\"HitCount\\\"></a>\\n<a href=https://github.com/Tobias-Fischer/rt_gene/stargazers><img src=\\\"https://img.shields.io/github/stars/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"stars\\\"></a>\\n<a href=https://github.com/Tobias-Fischer/rt_gene/issues><img src=\\\"https://img.shields.io/github/issues/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"GitHub issues\\\"></a>\\n<a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md><img src=\\\"https://img.shields.io/github/repo-size/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"GitHub repo size\\\"></a></p>\\n<p><a href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-mpii-gaze?p=rt-gene-real-time-eye-gaze-estimation-in?style=square\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-mpii-gaze&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a>\\n<a href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-rt-gene?p=rt-gene-real-time-eye-gaze-estimation-in\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-rt-gene&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a>\\n<a href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-ut-multi-view?p=rt-gene-real-time-eye-gaze-estimation-in\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-ut-multi-view&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a></p>\\n<p><a href=\\\"https://paperswithcode.com/sota/blink-estimation-on-eyeblink8?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-eyeblink8&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a>\\n<a href=\\\"https://paperswithcode.com/sota/blink-estimation-on-researcher-s-night?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-researcher-s-night&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a>\\n<a href=\\\"https://paperswithcode.com/sota/blink-estimation-on-rt-bene?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"><img src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-rt-bene&amp;style=flat-square\\\" alt=\\\"PWC\\\"></a></p>\\n<p>This repository contains code and dataset references for two papers: RT-GENE (Gaze Estimation; ECCV2018) and RT-BENE (Blink Estimation; ICCV2019 Workshops).</p>\\n<h2>RT-GENE (Gaze Estimation)</h2>\\n<h3>License + Attribution</h3>\\n<p>The RT-GENE code is licensed under <a href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\">CC BY-NC-SA 4.0</a>. Commercial usage is not permitted. If you use this dataset or the code in a scientific publication, please cite the following <a href=\\\"http://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html\\\">paper</a>:</p>\\n<p><img src=\\\"/_next/static/images/paper_abstract-f5758078c2e378a46ec9b98b4176e657.jpg\\\" alt=\\\"Paper abstract\\\"></p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@inproceedings{FischerECCV2018,\\nauthor = {Tobias Fischer and Hyung Jin Chang and Yiannis Demiris},\\ntitle = {{RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments}},\\nbooktitle = {European Conference on Computer Vision},\\nyear = {2018},\\nmonth = {September},\\npages = {339--357}\\n}\\n</code></pre>\\n<p>This work was supported in part by the Samsung Global Research Outreach program, and in part by the EU Horizon 2020 Project PAL (643783-RIA).</p>\\n<h3>Overview + Accompanying Dataset</h3>\\n<p>The code is split into four parts, each having its own README contained. There is also an accompanying <a href=\\\"https://zenodo.org/record/2529036\\\">dataset</a> <a href=\\\"https://goo.gl/tfUaDm\\\">(alternative link)</a> to the code. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: <a href=\\\"https://www.imperial.ac.uk/personal-robotics/software/\\\">https://www.imperial.ac.uk/personal-robotics/software/</a>.</p>\\n<h4>RT-GENE ROS package</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene>rt_gene</a> directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time.</p>\\n<p align=\\\"center\\\">\\n  <img src=\\\"/_next/static/gifs/5b7fb335144a426aa0bf9f3a74f130a6.gif\\\" alt=\\\"RT-GENE inference example\\\"/>\\n</p>\\n<h4>RT-GENE Standalone Version</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_standalone>rt_gene_standalone</a> directory contains instructions for eye gaze estimation given a set of images. It shares code with the <code class=\\\"language-none\\\">rt_gene</code> package (above), in particular the code in <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_gene>rt_gene/src/rt_gene</a>.</p>\\n<h4>RT-GENE Inpainting</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_inpainting>rt_gene_inpainting</a> directory contains code to inpaint the region covered by the eyetracking glasses.</p>\\n<p><img src=\\\"/_next/static/images/inpaint_example-e34160d4c6d03d7e4eacf7795a7d3e02.jpg\\\" alt=\\\"Inpaining example\\\"></p>\\n<h4>RT-GENE Model Training</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_model_training>rt_gene_model_training</a> directory allows using the inpainted images to train a deep neural network for eye gaze estimation.</p>\\n<p align=\\\"center\\\">\\n  <img src=\\\"/_next/static/images/accuracy_prl-10df02076ab749718e69f8cd75d3aec9.jpg\\\" alt=\\\"Accuracy on RT-GENE dataset\\\"/>\\n</p>\\n<h2>RT-BENE (Blink Estimation)</h2>\\n<h3>License + Attribution</h3>\\n<p>The RT-BENE code is licensed under <a href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\">CC BY-NC-SA 4.0</a>. Commercial usage is not permitted. If you use our blink estimation code or dataset, please cite the relevant <a href=\\\"http://openaccess.thecvf.com/content_ICCVW_2019/html/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.html\\\">paper</a>:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@inproceedings{CortaceroICCV2019W,\\nauthor={Kevin Cortacero and Tobias Fischer and Yiannis Demiris},\\nbooktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshops},\\ntitle = {RT-BENE: A Dataset and Baselines for Real-Time Blink Estimation in Natural Environments},\\nyear = {2019},\\n}\\n</code></pre>\\n<p>RT-BENE was supported by the EU Horizon 2020 Project PAL (643783-RIA) and a Royal Academy of Engineering Chair in Emerging Technologies to Yiannis Demiris.</p>\\n<h3>Overview + Accompanying Dataset</h3>\\n<p>The code is split into several parts, each having its own README. There is also an associated <a href=\\\"https://zenodo.org/record/3685316\\\">RT-BENE dataset</a>. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: <a href=\\\"https://www.imperial.ac.uk/personal-robotics/software/\\\">https://www.imperial.ac.uk/personal-robotics/software/</a>. Please note that a lot of the code is shared with RT-GENE (see above), hence there are many references to RT-GENE below.</p>\\n<p><img src=\\\"/_next/static/images/rt_bene_overview-3f326aa400ba8af7178e4bd4f1b92698.png\\\" alt=\\\"Paper overview\\\"></p>\\n<h4>RT-BENE ROS package</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene>rt_gene</a> directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time. For blink estimation, please refer to the <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/scripts/estimate_blink.py>estimate_blink.py</a> file.</p>\\n<p align=\\\"center\\\">\\n  <img src=\\\"/_next/static/gifs/364f43474c2cb421d206d8d484b293c0.gif\\\" alt=\\\"RT-BENE inference example\\\"/>\\n</p>\\n<h4>RT-BENE Standalone Version</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_standalone>rt_bene_standalone</a> directory contains instructions for blink estimation given a set of images. It makes use of the code in <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_bene>rt_gene/src/rt_bene</a>.</p>\\n<h4>RT-BENE Model Training</h4>\\n<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_model_training>rt_bene_model_training</a> directory contains the code required to train models with the labels contained in the RT-BENE dataset (see below). We will soon at evaluation code in this directory, too.</p>\\n<h4>RT-BENE Dataset</h4>\\n<p><img src=\\\"/_next/static/images/rt_bene_labels-efb80b606e00709e04146a1ed7a0938c.png\\\" alt=\\\"RT-BENE labels\\\"></p>\\n<p>We manually annotated images contained in the &quot;noglasses&quot; part of the RT-GENE dataset. The <a href=\\\"https://zenodo.org/record/3685316\\\">RT-BENE dataset on Zenodo</a> contains the eye image patches and associated annotations to train the blink models.</p>\\n\",\"name\":\"Real-Time Gaze (RT-GENE) and Blink Estimation (RT-BENE)\",\"type\":\"code\",\"url\":\"https://github.com/Tobias-Fischer/rt_gene\",\"src\":\"/content/rt-gene.md\",\"id\":\"rt-gene\",\"image_position\":\"center\",\"image\":\"/_next/static/images/paper_abstract-f5758078c2e378a46ec9b98b4176e657.jpg\"},{\"content\":\"<h1>Robotics Toolbox for Python</h1>\\n<p><a href=\\\"https://badge.fury.io/py/roboticstoolbox-python\\\"><img src=\\\"https://badge.fury.io/py/roboticstoolbox-python.svg\\\" alt=\\\"PyPI version\\\"></a>\\n<img src=\\\"https://img.shields.io/pypi/pyversions/roboticstoolbox-python.svg\\\" alt=\\\"PyPI - Python Version\\\">\\n<a href=\\\"https://opensource.org/licenses/MIT\\\"><img src=\\\"https://img.shields.io/badge/License-MIT-yellow.svg\\\" alt=\\\"License: MIT\\\"></a>\\n<a href=\\\"https://mybinder.org/v2/gh/petercorke/robotics-toolbox-python/master?filepath=notebooks\\\"><img src=\\\"https://mybinder.org/badge_logo.svg\\\" alt=\\\"Binder\\\"></a>\\n<a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<p><a href=https://github.com/petercorke/robotics-toolbox-python/actions?query=workflow%3Abuild><img src=https://github.com/petercorke/robotics-toolbox-python/workflows/build/badge.svg?branch=master alt=\\\"Build Status\\\"></a>\\n<a href=\\\"https://codecov.io/gh/petercorke/robotics-toolbox-python\\\"><img src=\\\"https://codecov.io/gh/petercorke/robotics-toolbox-python/branch/master/graph/badge.svg\\\" alt=\\\"Coverage\\\"></a>\\n<a href=\\\"https://lgtm.com/projects/g/petercorke/robotics-toolbox-python/context:python\\\"><img src=\\\"https://img.shields.io/lgtm/grade/python/g/petercorke/robotics-toolbox-python.svg?logo=lgtm&amp;logoWidth=18\\\" alt=\\\"Language grade: Python\\\"></a>\\n<a href=\\\"https://pypistats.org/packages/roboticstoolbox-python\\\"><img src=\\\"https://img.shields.io/pypi/dw/roboticstoolbox-python\\\" alt=\\\"PyPI - Downloads\\\"></a></p>\\n<table style=\\\"border:0px\\\">\\n<tr style=\\\"border:0px\\\">\\n<td style=\\\"border:0px\\\">\\n<img src=https://github.com/petercorke/robotics-toolbox-python/raw/master/docs/figs/RobToolBox_RoundLogoB.png width=\\\"200\\\"></td>\\n<td style=\\\"border:0px\\\">\\nA Python implementation of the <a href=\\\"https://github.com/petercorke/robotics-toolbox-matlab\\\">Robotics Toolbox for MATLAB<sup>&reg;</sup></a>\\n<ul>\\n<li><a href=\\\"https://github.com/petercorke/robotics-toolbox-python\\\">GitHub repository </a></li>\\n<li><a href=\\\"https://petercorke.github.io/robotics-toolbox-python\\\">Documentation</a></li>\\n<li><a href=\\\"https://github.com/petercorke/robotics-toolbox-python/wiki\\\">Examples and details</a></li>\\n</ul>\\n</td>\\n</tr>\\n</table>\\n<h2>Synopsis</h2>\\n<p>This toolbox brings robotics-specific functionality to Python, and leverages\\nPython's advantages of portability, ubiquity and support, and the capability of\\nthe open-source ecosystem for linear algebra (numpy, scipy),  graphics\\n(matplotlib, three.js, WebGL), interactive development (jupyter, jupyterlab,\\nmybinder.org), and documentation (sphinx).</p>\\n<p>The Toolbox provides tools for representing the kinematics and dynamics of\\nserial-link manipulators  - you can easily create your own in Denavit-Hartenberg\\nform, import a URDF file, or use over 30 supplied models for well-known\\ncontemporary robots from Franka-Emika, Kinova, Universal Robotics, Rethink as\\nwell as classical robots such as the Puma 560 and the Stanford arm.</p>\\n<p>The toolbox will also support mobile robots with functions for robot motion models\\n(unicycle, bicycle), path planning algorithms (bug, distance transform, D*,\\nPRM), kinodynamic planning (lattice, RRT), localization (EKF, particle filter),\\nmap building (EKF) and simultaneous localization and mapping (EKF).</p>\\n<p>The Toolbox provides:</p>\\n<ul>\\n<li>code that is mature and provides a point of comparison for other\\nimplementations of the same algorithms;</li>\\n<li>routines which are generally written in a straightforward manner which\\nallows for easy understanding, perhaps at the expense of computational\\nefficiency;</li>\\n<li>source code which can be read for learning and teaching;</li>\\n<li>backward compatability with the Robotics Toolbox for MATLAB</li>\\n</ul>\\n<p>The Toolbox leverages the <a href=https://github.com/petercorke/spatialmath-python>Spatial Maths Toolbox for Python</a> to\\nprovide support for data types such as SO(n) and SE(n) matrices, quaternions, twists and spatial vectors.</p>\\n<h2>Code Example</h2>\\n<p>We will load a model of the Franka-Emika Panda robot defined classically using\\nmodified (Craig's convention) Denavit-Hartenberg notation</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">import</span> roboticstoolbox <span class=\\\"token keyword\\\">as</span> rtb\\nrobot <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>models<span class=\\\"token punctuation\\\">.</span>DH<span class=\\\"token punctuation\\\">.</span>Panda<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">)</span>\\n\\n\\tâ”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“\\n\\tâ”ƒ aâ±¼â‚‹â‚   â”ƒ  âºâ±¼â‚‹â‚  â”ƒ Î¸â±¼  â”ƒ  dâ±¼   â”ƒ   qâ»    â”ƒ   qâº   â”ƒ\\n\\tâ”£â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â•‹â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”â•‹â”â”â”â”â”â”â”â”â”«\\n\\tâ”ƒ    <span class=\\\"token number\\\">0.0</span> â”ƒ   <span class=\\\"token number\\\">0.0</span>Â° â”ƒ  q1 â”ƒ <span class=\\\"token number\\\">0.333</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">166.0</span>Â° â”ƒ <span class=\\\"token number\\\">166.0</span>Â° â”ƒ\\n\\tâ”ƒ    <span class=\\\"token number\\\">0.0</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q2 â”ƒ   <span class=\\\"token number\\\">0.0</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">101.0</span>Â° â”ƒ <span class=\\\"token number\\\">101.0</span>Â° â”ƒ\\n\\tâ”ƒ    <span class=\\\"token number\\\">0.0</span> â”ƒ  <span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q3 â”ƒ <span class=\\\"token number\\\">0.316</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">166.0</span>Â° â”ƒ <span class=\\\"token number\\\">166.0</span>Â° â”ƒ\\n\\tâ”ƒ <span class=\\\"token number\\\">0.0825</span> â”ƒ  <span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q4 â”ƒ   <span class=\\\"token number\\\">0.0</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">176.0</span>Â° â”ƒ  <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">4.0</span>Â° â”ƒ\\n\\tâ”ƒ<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.0825</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q5 â”ƒ <span class=\\\"token number\\\">0.384</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">166.0</span>Â° â”ƒ <span class=\\\"token number\\\">166.0</span>Â° â”ƒ\\n\\tâ”ƒ    <span class=\\\"token number\\\">0.0</span> â”ƒ  <span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q6 â”ƒ   <span class=\\\"token number\\\">0.0</span> â”ƒ   <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1.0</span>Â° â”ƒ <span class=\\\"token number\\\">215.0</span>Â° â”ƒ\\n\\tâ”ƒ  <span class=\\\"token number\\\">0.088</span> â”ƒ  <span class=\\\"token number\\\">90.0</span>Â° â”ƒ  q7 â”ƒ <span class=\\\"token number\\\">0.107</span> â”ƒ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">166.0</span>Â° â”ƒ <span class=\\\"token number\\\">166.0</span>Â° â”ƒ\\n\\tâ”—â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”»â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”â”»â”â”â”â”â”â”â”â”â”›\\n\\t\\n\\tâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\n\\tâ”‚tool â”‚ t <span class=\\\"token operator\\\">=</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0.1</span><span class=\\\"token punctuation\\\">;</span> rpy<span class=\\\"token operator\\\">/</span>xyz <span class=\\\"token operator\\\">=</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">45</span>Â°<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0</span>Â°<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0</span>Â° â”‚\\n\\tâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n\\t\\n\\tâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\\n\\tâ”‚name â”‚ q0  â”‚ q1     â”‚ q2  â”‚ q3    â”‚ q4  â”‚ q5    â”‚ q6   â”‚\\n\\tâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤\\n\\tâ”‚  qz â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚  <span class=\\\"token number\\\">0</span>Â°    â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚  <span class=\\\"token number\\\">0</span>Â°   â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚  <span class=\\\"token number\\\">0</span>Â°   â”‚  <span class=\\\"token number\\\">0</span>Â°  â”‚\\n\\tâ”‚  qr â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">17.2</span>Â° â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚ <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">126</span>Â° â”‚  <span class=\\\"token number\\\">0</span>Â° â”‚  <span class=\\\"token number\\\">115</span>Â° â”‚  <span class=\\\"token number\\\">45</span>Â° â”‚\\n\\tâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\\n\\nT <span class=\\\"token operator\\\">=</span> robot<span class=\\\"token punctuation\\\">.</span>fkine<span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">.</span>qz<span class=\\\"token punctuation\\\">)</span>  <span class=\\\"token comment\\\"># forward kinematics</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>T<span class=\\\"token punctuation\\\">)</span>\\n\\n\\t   <span class=\\\"token number\\\">0.707107</span>    <span class=\\\"token number\\\">0.707107</span>    <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">0.088</span>        \\n\\t   <span class=\\\"token number\\\">0.707107</span>   <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.707107</span>    <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">0</span>            \\n\\t   <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">0</span>          <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span>           <span class=\\\"token number\\\">0.823</span>        \\n\\t   <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">0</span>           <span class=\\\"token number\\\">1</span>          \\n</code></pre>\\n<p>(Python prompts are not shown to make it easy to copy+paste the code, console output is indented)</p>\\n<p>We can solve inverse kinematics very easily.  We first choose an SE(3) pose\\ndefined in terms of position and orientation (end-effector z-axis down (A=-Z) and finger\\norientation parallel to y-axis (O=+Y)).</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">from</span> spatialmath <span class=\\\"token keyword\\\">import</span> SE3\\n\\nT <span class=\\\"token operator\\\">=</span> SE3<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.8</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0.2</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0.1</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> SE3<span class=\\\"token punctuation\\\">.</span>OA<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\nsol <span class=\\\"token operator\\\">=</span> robot<span class=\\\"token punctuation\\\">.</span>ikine_min<span class=\\\"token punctuation\\\">(</span>T<span class=\\\"token punctuation\\\">)</span>         <span class=\\\"token comment\\\"># solve IK</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>sol<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">)</span>                     <span class=\\\"token comment\\\"># display joint angles</span>\\n\\n\\t<span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.01044</span>    <span class=\\\"token number\\\">7.876</span>    <span class=\\\"token number\\\">1.557</span>    <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">6.81</span>    <span class=\\\"token number\\\">1.571</span>    <span class=\\\"token number\\\">4.686</span>   <span class=\\\"token number\\\">0.5169</span><span class=\\\"token punctuation\\\">]</span>\\n\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">.</span>fkine<span class=\\\"token punctuation\\\">(</span>sol<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>    <span class=\\\"token comment\\\"># FK shows that desired end-effector pose was achieved</span>\\n\\n\\tOut<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">35</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">:</span> \\n\\tSE3<span class=\\\"token punctuation\\\">:</span>â”                                           â”“\\n\\t\\tâ”ƒ<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span>         <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">4e</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">08</span>      <span class=\\\"token number\\\">0.000521</span>   <span class=\\\"token number\\\">0.615</span>    â”ƒ\\n\\t\\tâ”ƒ <span class=\\\"token number\\\">2.79e-08</span>   <span class=\\\"token number\\\">1</span>          <span class=\\\"token number\\\">0.00013</span>    <span class=\\\"token number\\\">0.154</span>    â”ƒ\\n\\t\\tâ”ƒ<span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.000521</span>   <span class=\\\"token number\\\">0.00013</span>   <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span>          <span class=\\\"token number\\\">0.105</span>    â”ƒ\\n\\t\\tâ”ƒ <span class=\\\"token number\\\">0</span>          <span class=\\\"token number\\\">0</span>          <span class=\\\"token number\\\">0</span>          <span class=\\\"token number\\\">1</span>        â”ƒ\\n\\t\\tâ”—                                           â”›\\n</code></pre>\\n<p>Note that because this robot is redundant we don't have any control over the arm configuration apart from end-effector pose, ie. we can't control the elbow height.</p>\\n<p>We can animate a path from the upright <code class=\\\"language-none\\\">qz</code> configuration to this pickup configuration</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\">qt <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>trajectory<span class=\\\"token punctuation\\\">.</span>jtraj<span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">.</span>qz<span class=\\\"token punctuation\\\">,</span> q_pickup<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">50</span><span class=\\\"token punctuation\\\">)</span>\\nrobot<span class=\\\"token punctuation\\\">.</span>plot<span class=\\\"token punctuation\\\">(</span>qt<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">,</span> movie<span class=\\\"token operator\\\">=</span><span class=\\\"token string\\\">'panda1.gif'</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/826a58e84a13c9100ba2527f636dc267.jpg\\\"><source src=\\\"/_next/static/gifs/826a58e84a13c9100ba2527f636dc267.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>which uses the default matplotlib backend.  Grey arrows show the joint axes and the colored frame shows the end-effector pose.</p>\\n<p>Let's now load a URDF model of the same robot. The kinematic representation is no longer\\nbased on Denavit-Hartenberg parameters, it is now a rigid-body tree.</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\">robot <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>models<span class=\\\"token punctuation\\\">.</span>URDF<span class=\\\"token punctuation\\\">.</span>Panda<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>  <span class=\\\"token comment\\\"># load URDF version of the Panda</span>\\n<span class=\\\"token keyword\\\">print</span><span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">)</span>    <span class=\\\"token comment\\\"># display the model</span>\\n\\n\\tâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\\n\\tâ”‚<span class=\\\"token builtin\\\">id</span> â”‚     link     â”‚   parent    â”‚    joint     â”‚                     ETS                     â”‚\\n\\tâ”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\\n\\tâ”‚ <span class=\\\"token number\\\">0</span> â”‚  panda_link0 â”‚           <span class=\\\"token operator\\\">-</span> â”‚              â”‚                                             â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">1</span> â”‚  panda_link1 â”‚ panda_link0 â”‚ panda_joint1 â”‚                          tz<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.333</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q0<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">2</span> â”‚  panda_link2 â”‚ panda_link1 â”‚ panda_joint2 â”‚                           Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q1<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">3</span> â”‚  panda_link3 â”‚ panda_link2 â”‚ panda_joint3 â”‚               ty<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.316</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q2<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">4</span> â”‚  panda_link4 â”‚ panda_link3 â”‚ panda_joint4 â”‚               tx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.0825</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q3<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">5</span> â”‚  panda_link5 â”‚ panda_link4 â”‚ panda_joint5 â”‚ tx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.0825</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> ty<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.384</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q4<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">6</span> â”‚  panda_link6 â”‚ panda_link5 â”‚ panda_joint6 â”‚                            Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q5<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">7</span> â”‚  panda_link7 â”‚ panda_link6 â”‚ panda_joint7 â”‚                tx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.088</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">90</span>Â°<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> Rz<span class=\\\"token punctuation\\\">(</span>q6<span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ”‚ <span class=\\\"token number\\\">8</span> â”‚ @panda_link8 â”‚ panda_link7 â”‚ panda_joint8 â”‚                                   tz<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.107</span><span class=\\\"token punctuation\\\">)</span> â”‚\\n\\tâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n</code></pre>\\n<p>The symbol <code class=\\\"language-none\\\">@</code> indicates the link as an end-effector, a leaf node in the rigid-body\\ntree.</p>\\n<p>We can instantiate our robot inside a browser-based 3d-simulation environment.</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\">env <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>backends<span class=\\\"token punctuation\\\">.</span>Swift<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>  <span class=\\\"token comment\\\"># instantiate 3D browser-based visualizer</span>\\nenv<span class=\\\"token punctuation\\\">.</span>launch<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>                <span class=\\\"token comment\\\"># activate it</span>\\nenv<span class=\\\"token punctuation\\\">.</span>add<span class=\\\"token punctuation\\\">(</span>robot<span class=\\\"token punctuation\\\">)</span>              <span class=\\\"token comment\\\"># add robot to the 3D scene</span>\\n<span class=\\\"token keyword\\\">for</span> qk <span class=\\\"token keyword\\\">in</span> qt<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">:</span>             <span class=\\\"token comment\\\"># for each joint configuration on trajectory</span>\\n      robot<span class=\\\"token punctuation\\\">.</span>q <span class=\\\"token operator\\\">=</span> qk          <span class=\\\"token comment\\\"># update the robot state</span>\\n      env<span class=\\\"token punctuation\\\">.</span>step<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>            <span class=\\\"token comment\\\"># update visualization</span>\\n</code></pre>\\n<p align=\\\"center\\\">\\n <img src=\\\"/_next/static/gifs/d064326b92f5b2c4e0c25c057731e6a7.gif\\\">\\n</p>\\n<h1>Getting going</h1>\\n<h2>Installing</h2>\\n<p>You will need Python &gt;= 3.6</p>\\n<h3>Using pip</h3>\\n<p>Install a snapshot from PyPI</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">pip3 <span class=\\\"token function\\\">install</span> roboticstoolbox-python\\n</code></pre>\\n<p>Available options are:</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">vpython</code> install <a href=\\\"https://vpython.org\\\">VPython</a> backend</li>\\n<li><code class=\\\"language-none\\\">collision</code> install collision checking with <a href=\\\"https://pybullet.org\\\">pybullet</a></li>\\n</ul>\\n<p>Put the options in a comma separated list like</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">pip3 <span class=\\\"token function\\\">install</span> roboticstoolbox-python<span class=\\\"token punctuation\\\">[</span>optionlist<span class=\\\"token punctuation\\\">]</span>\\n</code></pre>\\n<p><a href=https://github.com/jhavl/swift>Swift</a>, a web-based visualizer, is\\ninstalled as part of Robotics Toolbox.</p>\\n<h3>From GitHub</h3>\\n<p>To install the bleeding-edge version from GitHub</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\"><span class=\\\"token function\\\">git</span> clone https://github.com/petercorke/robotics-toolbox-python.git\\n<span class=\\\"token builtin class-name\\\">cd</span> robotics-toolbox-python\\npip3 <span class=\\\"token function\\\">install</span> -e <span class=\\\"token builtin class-name\\\">.</span>\\n</code></pre>\\n<h2>Run some examples</h2>\\n<p>The <a href=https://github.com/petercorke/robotics-toolbox-python/tree/master/notebooks><code class=\\\"language-none\\\">notebooks</code></a> folder contains some tutorial Jupyter notebooks which you can browse on GitHub.</p>\\n<p>Or you can run them, and experiment with them, at <a href=\\\"https://mybinder.org/v2/gh/petercorke/robotics-toolbox-python/master?filepath=notebooks\\\">mybinder.org</a>.</p>\\n<h2>Toolbox Research Applications</h2>\\n<p>The toolbox is incredibly useful for developing and prototyping algorithms for research, thanks to the exhaustive set of well documented and mature robotic functions exposed through clean and painless APIs. Additionally, the ease at which a user can visualize their algorithm supports a rapid prototyping paradigm.</p>\\n<h3>Publication List</h3>\\n<p>J. Haviland and P. Corke, &quot;<strong>NEO: A Novel Expeditious Optimisation Algorithm for Reactive Motion Control of Manipulators</strong>,&quot; in <em>IEEE Robotics and Automation Letters</em>, doi: 10.1109/LRA.2021.3056060. In the video, the robot is controlled using the Robotics toolbox for Python and features a recording from the <a href=https://github.com/jhavl/swift>Swift</a> Simulator.</p>\\n<p>[<a href=\\\"https://arxiv.org/abs/2010.08686\\\">Arxiv Paper</a>] [<a href=\\\"https://ieeexplore.ieee.org/document/9343718\\\">IEEE Xplore</a>] [<a href=\\\"https://jhavl.github.io/neo/\\\">Project Website</a>] [<a href=\\\"https://youtu.be/jSLPJBr8QTY\\\">Video</a>] [<a href=https://github.com/petercorke/robotics-toolbox-python/blob/master/examples/neo.py>Code Example</a>]</p>\\n<p>\\n  <a href=\\\"https://youtu.be/jSLPJBr8QTY\\\">\\n    <img src=https://github.com/petercorke/robotics-toolbox-python/raw/master/docs/figs/neo_youtube.png width=\\\"560\\\">\\n  </a>\\n</p>\\n<p><strong>A Purely-Reactive Manipulability-Maximising Motion Controller</strong>, J. Haviland and P. Corke. In the video, the robot is controlled using the Robotics toolbox for Python.</p>\\n<p>[<a href=\\\"https://arxiv.org/abs/2002.11901\\\">Paper</a>] [<a href=\\\"https://jhavl.github.io/mmc/\\\">Project Website</a>] [<a href=\\\"https://youtu.be/Vu_rcPlaADI\\\">Video</a>] [<a href=https://github.com/petercorke/robotics-toolbox-python/blob/master/examples/mmc.py>Code Example</a>]</p>\\n<p>\\n  <a href=\\\"https://youtu.be/Vu_rcPlaADI\\\">\\n    <img src=https://github.com/petercorke/robotics-toolbox-python/raw/master/docs/figs/mmc_youtube.png width=\\\"560\\\">\\n  </a>\\n</p>\\n<br>\\n\",\"name\":\"Robotics Toolbox Python\",\"type\":\"code\",\"url\":\"https://github.com/petercorke/robotics-toolbox-python\",\"image\":\"/_next/static/images/RobToolBox_RoundLogoB-9563d226662903b6e404b809e72e3235.png\",\"image_fit\":\"contain\",\"src\":\"/content/robotics_toolbox/robotics-toolbox-python.md\",\"id\":\"robotics-toolbox-python\",\"image_position\":\"center\"},{\"content\":\"<h1>OMRON ROS Robot driver</h1>\\n<p><a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<p>The OMRON LD-60 is a capable platform out of the box but has no ROS support. Fortunatelyt he LD-60 s still really a Pioneer at heart and there is significant resources in the public domain which can interface to the platform.</p>\\n<p>This does not replace Mobile Planner. Mobile Planner is still used for map creation and robot configuration. *Note: Mobile planner will run inside Wine on Ubuntu 18.04</p>\\n<p>This driver currently assumes you have a user (which can be set via Mobile Planner) with no password.</p>\\n<img src=\\\"/_next/static/images/omron_robot-76819288aafba3135caac5b90750cf48.jpg\\\" alt=\\\"LD-60 Robot\\\" style=\\\"zoom:33%;\\\" />\\n<h2>Required Parameters</h2>\\n<p><strong>Host IP</strong>: String     e.g. 172.168.1.1</p>\\n<p><strong>Host Port</strong>: String     e.g. 7272</p>\\n<p><strong>User</strong>: String  e.g. omron</p>\\n<h2>Topics</h2>\\n<h3>Published</h3>\\n<ul>\\n<li>/amcl_pose</li>\\n<li>/current_goal</li>\\n<li>/laser</li>\\n<li>/laser_low</li>\\n<li>/bumper</li>\\n<li>/status</li>\\n</ul>\\n<h3>Actions</h3>\\n<ul>\\n<li>/move_base</li>\\n</ul>\\n<h3>Subscribed</h3>\\n<ul>\\n<li>/cmd_vel</li>\\n<li>/simple_goal</li>\\n<li>/set_map</li>\\n</ul>\\n<h2>Getting Started</h2>\\n<p><em>Coming Soon</em></p>\\n\",\"name\":\"ROS Omron Driver\",\"type\":\"code\",\"url\":\"https://github.com/qcr/ros_omron_driver\",\"src\":\"/content/ros-omron-driver.md\",\"id\":\"ros-omron-driver\",\"image_position\":\"center\",\"image\":\"/_next/static/images/omron_robot-76819288aafba3135caac5b90750cf48.jpg\"},{\"content\":\"<h2>Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</h2>\\n<p>This is the source code for the paper titled: &quot;Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation&quot;, pre-print avialable <a href=\\\"https://arxiv.org/abs/1902.07381\\\">here</a>.</p>\\n<p>If you find this work useful, please cite it as:\\nGarg, S., Babu V, M., Dharmasiri, T., Hausler, S., Suenderhauf, N., Kumar, S., Drummond, T., &amp; Milford, M. (2019). Look no deeper: Recognizing places from opposing viewpoints under varying scene appearance using single-view depth estimation. In IEEE International Conference on Robotics and Automation (ICRA), 2019. IEEE.</p>\\n<p>bibtex:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@inproceedings{garg2019look,\\ntitle={Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation},\\nauthor={Garg, Sourav and Babu V, Madhu and Dharmasiri, Thanuja and Hausler, Stephen and Suenderhauf, Niko and Kumar, Swagat and Drummond, Tom and Milford, Michael},\\nbooktitle={IEEE International Conference on Robotics and Automation (ICRA)},\\nyear={2019}\\n}\\n</code></pre>\\n<p><img src=\\\"/_next/static/images/illustration-743ee8feee1528b8dbbe46dea09ab0e9.png\\\" alt=\\\"Illustration of the proposed approach\\\" title=\\\"Illustration of the proposed approach.\\\"></p>\\n<p><img src=\\\"/_next/static/images/topometric-5ce9c835ea539de03099f4f69c57ad17.png\\\" alt=\\\"An image depicting topometric representation.\\\" title=\\\"A topometric representation.\\\"></p>\\n<h4>Requirements</h4>\\n<ul>\\n<li>Ubuntu\\t(Tested on <em>16.04</em>)</li>\\n<li>Jupyter\\t(Tested on <em>4.4.0</em>)</li>\\n<li>Python\\t(Tested on <em>3.5.6</em>)\\n<ul>\\n<li>numpy\\t(Tested on <em>1.15.2</em>)</li>\\n<li>scipy\\t(Tested on <em>1.1.0</em>)</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Optionally, for vis_results.ipynb:</p>\\n<ul>\\n<li>Matplotlib\\t(Tested on <em>2.0.2</em>)</li>\\n</ul>\\n<h4>Download an example dataset and its pre-computed representations</h4>\\n<ol>\\n<li>\\n<p>In <code class=\\\"language-none\\\">seq2single/precomputed/</code>, download <a href=\\\"https://mega.nz/#F!Z4Z3gAzb!KI48uGHJJza90DP7-Kz1kA\\\">pre-computed representations (<em>~10 GB</em>)</a>. Please refer to the <code class=\\\"language-none\\\">seq2single/precomputed/readme.md</code> for instructions on how to compute these representations.</p>\\n</li>\\n<li>\\n<p>[Optional] In <code class=\\\"language-none\\\">seq2single/images/</code>, download <a href=\\\"https://mega.nz/#F!h5QB2ayI!H7p0UCxATd6MUdszMZWNOA\\\">images (<em>~1 GB</em>)</a>. These images are a subset of two different traverses from the <a href=\\\"https://robotcar-dataset.robots.ox.ac.uk/\\\">Oxford Robotcar dataset</a>.</p>\\n</li>\\n</ol>\\n<p>(Note: These download links from Mega.nz require you to first create an account (free))</p>\\n<h4>Run</h4>\\n<ol>\\n<li>The Jupyter notebook seq2single.ipynb first loads the pre-computed global image descriptors to find top matches. These matches are re-ranked with the proposed method using the pre-computed depth masks and dense conv5 features.</li>\\n</ol>\\n<h4>License</h4>\\n<p>The code is released under MIT License.</p>\\n<h2>Related Projects</h2>\\n<p><a href=https://github.com/oravus/DeltaDescriptors>Delta Descriptors (2020)</a></p>\\n<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>\\n<p><a href=https://github.com/oravus/lostX>LoST (2018)</a></p>\\n\",\"name\":\"seq2single\",\"type\":\"code\",\"url\":\"https://github.com/oravus/seq2single\",\"src\":\"/content/visual_place_recognition/seq2single.md\",\"id\":\"seq2single\",\"image_position\":\"center\",\"image\":\"/_next/static/images/illustration-743ee8feee1528b8dbbe46dea09ab0e9.png\"},{\"content\":\"<h1>Spatial Maths for Python</h1>\\n<p><a href=\\\"https://badge.fury.io/py/spatialmath-python\\\"><img src=\\\"https://badge.fury.io/py/spatialmath-python.svg\\\" alt=\\\"PyPI version\\\"></a>\\n<img src=\\\"https://img.shields.io/pypi/pyversions/roboticstoolbox-python.svg\\\" alt=\\\"PyPI - Python Version\\\">\\n<a href=\\\"https://opensource.org/licenses/MIT\\\"><img src=\\\"https://img.shields.io/badge/License-MIT-yellow.svg\\\" alt=\\\"License: MIT\\\"></a>\\n<a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<p><a href=https://github.com/petercorke/spatialmath-python/actions?query=workflow%3Abuild><img src=https://github.com/petercorke/spatialmath-python/workflows/build/badge.svg?branch=master alt=\\\"Build Status\\\"></a>\\n<a href=\\\"https://codecov.io/gh/petercorke/spatialmath-python\\\"><img src=\\\"https://codecov.io/gh/petercorke/spatialmath-python/branch/master/graph/badge.svg\\\" alt=\\\"Coverage\\\"></a>\\n<a href=\\\"https://lgtm.com/projects/g/petercorke/spatialmath-python/context:python\\\"><img src=\\\"https://img.shields.io/lgtm/grade/python/g/petercorke/spatialmath-python.svg?logo=lgtm&amp;logoWidth=18\\\" alt=\\\"Language grade: Python\\\"></a>\\n<a href=\\\"https://pypistats.org/packages/spatialmath-python\\\"><img src=\\\"https://img.shields.io/pypi/dw/spatialmath-python\\\" alt=\\\"PyPI - Downloads\\\"></a>\\n<a href=https://GitHub.com/petercorke/spatialmath-python/stargazers/><img src=\\\"https://img.shields.io/github/stars/petercorke/spatialmath-python.svg?style=social&amp;label=Star\\\" alt=\\\"GitHub stars\\\"></a></p>\\n<table style=\\\"border:0px\\\">\\n<tr style=\\\"border:0px\\\">\\n<td style=\\\"border:0px\\\">\\n<img src=https://github.com/petercorke/spatialmath-python/raw/master/docs/figs/CartesianSnakes_LogoW.png width=\\\"200\\\"></td>\\n<td style=\\\"border:0px\\\">\\nA Python implementation of the <a href=\\\"https://github.com/petercorke/spatial-math\\\">Spatial Math Toolbox for MATLAB<sup>&reg;</sup></a>\\n<ul>\\n<li><a href=\\\"https://github.com/petercorke/spatialmath-python\\\">GitHub repository </a></li>\\n<li><a href=\\\"https://petercorke.github.io/spatialmath-python\\\">Documentation</a></li>\\n<li><a href=\\\"https://github.com/petercorke/spatialmath-python/wiki\\\">Examples and details</a></li>\\n<li><a href=\\\"installation#\\\">Installation</a></li>\\n</ul>\\n</td>\\n</tr>\\n</table>\\n<p>Spatial mathematics capability underpins all of robotics and robotic vision where we need to describe the position, orientation or pose of objects in 2D or 3D spaces.</p>\\n<h1>What it does</h1>\\n<p>The package provides classes to represent pose and orientation in 3D and 2D\\nspace:</p>\\n<table>\\n<thead>\\n<tr>\\n<th>Represents</th>\\n<th>in 3D</th>\\n<th>in 2D</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>pose</td>\\n<td><code class=\\\"language-none\\\">SE3</code> <code class=\\\"language-none\\\">Twist3</code> <code class=\\\"language-none\\\">UnitDualQuaternion</code></td>\\n<td><code class=\\\"language-none\\\">SE2</code> <code class=\\\"language-none\\\">Twist2</code></td>\\n</tr>\\n<tr>\\n<td>orientation</td>\\n<td><code class=\\\"language-none\\\">SO3</code> <code class=\\\"language-none\\\">UnitQuaternion</code></td>\\n<td><code class=\\\"language-none\\\">SO2</code></td>\\n</tr>\\n</tbody>\\n</table>\\n<p>More specifically:</p>\\n<ul>\\n<li><code class=\\\"language-none\\\">SE3</code> matrices belonging to the group SE(3) for position and orientation (pose) in 3-dimensions</li>\\n<li><code class=\\\"language-none\\\">SO3</code> matrices belonging to the group SO(3) for orientation in 3-dimensions</li>\\n<li><code class=\\\"language-none\\\">UnitQuaternion</code> belonging to the group S3 for orientation in 3-dimensions</li>\\n<li><code class=\\\"language-none\\\">Twist3</code> vectors belonging to the group se(3) for pose in 3-dimensions</li>\\n<li><code class=\\\"language-none\\\">UnitDualQuaternion</code> maps to the group SE(3) for position and orientation (pose) in 3-dimensions</li>\\n<li><code class=\\\"language-none\\\">SE2</code> matrices belonging to the group SE(2) for position and orientation (pose) in 2-dimensions</li>\\n<li><code class=\\\"language-none\\\">SO2</code> matrices belonging to the group SO(2) for orientation in 2-dimensions</li>\\n<li><code class=\\\"language-none\\\">Twist2</code> vectors belonging to the group se(2) for pose in 2-dimensions</li>\\n</ul>\\n<p>These classes provide convenience and type safety, as well as methods and overloaded operators to support:</p>\\n<ul>\\n<li>composition, using the <code class=\\\"language-none\\\">*</code> operator</li>\\n<li>point transformation, using the <code class=\\\"language-none\\\">*</code> operator</li>\\n<li>exponent, using the <code class=\\\"language-none\\\">**</code> operator</li>\\n<li>normalization</li>\\n<li>inversion</li>\\n<li>connection to the Lie algebra via matrix exponential and logarithm operations</li>\\n<li>conversion of orientation to/from Euler angles, roll-pitch-yaw angles and angle-axis forms.</li>\\n<li>list operations such as append, insert and get</li>\\n</ul>\\n<p>These are layered over a set of base functions that perform many of the same operations but represent data explicitly in terms of <code class=\\\"language-none\\\">numpy</code> arrays.</p>\\n<p>The class, method and functions names largely mirror those of the MATLAB toolboxes, and the semantics are quite similar.</p>\\n<p><img src=https://github.com/petercorke/spatialmath-python/raw/master/docs/figs/fig1.png alt=\\\"trplot\\\"></p>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/f53aa66b328813113b38f087b50da80f.jpg\\\"><source src=\\\"/_next/static/gifs/f53aa66b328813113b38f087b50da80f.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<h1>Installation</h1>\\n<h2>Using pip</h2>\\n<p>Install a snapshot from PyPI</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">pip install spatialmath-python\\n</code></pre>\\n<h2>From GitHub</h2>\\n<p>Install the current code base from GitHub and pip install a link to that cloned copy</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/petercorke/spatialmath-python.git\\ncd spatialmath-python\\npip install -e .\\n</code></pre>\\n<h2>Dependencies</h2>\\n<p><code class=\\\"language-none\\\">numpy</code>, <code class=\\\"language-none\\\">scipy</code>, <code class=\\\"language-none\\\">matplotlib</code>, <code class=\\\"language-none\\\">ffmpeg</code> (if rendering animations as a movie)</p>\\n<h1>Examples</h1>\\n<h2>High-level classes</h2>\\n<p>These classes abstract the low-level numpy arrays into objects that obey the rules associated with the mathematical groups SO(2), SE(2), SO(3), SE(3) as well as twists and quaternions.</p>\\n<p>Using classes ensures type safety, for example it stops us mixing a 2D homogeneous transformation with a 3D rotation matrix -- both of which are 3x3 matrices.  It also ensures that the internal matrix representation is always a valid member of the relevant group.</p>\\n<p>For example, to create an object representing a rotation of 0.3 radians about the x-axis is simply</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R1 <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">.</span>Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.3</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R1\\n   <span class=\\\"token number\\\">1</span>         <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0</span>          \\n   <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0.955336</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.29552</span>    \\n   <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0.29552</span>   <span class=\\\"token number\\\">0.955336</span>         \\n</code></pre>\\n<p>while a rotation of 30 deg about the z-axis is</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R2 <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">.</span>Rz<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">30</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token string\\\">'deg'</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R2\\n   <span class=\\\"token number\\\">0.866025</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.5</span>       <span class=\\\"token number\\\">0</span>          \\n   <span class=\\\"token number\\\">0.5</span>       <span class=\\\"token number\\\">0.866025</span>  <span class=\\\"token number\\\">0</span>          \\n   <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">1</span>    \\n</code></pre>\\n<p>and the composition of these two rotations is</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R <span class=\\\"token operator\\\">=</span> R1 <span class=\\\"token operator\\\">*</span> R2\\n   <span class=\\\"token number\\\">0.866025</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.5</span>       <span class=\\\"token number\\\">0</span>          \\n   <span class=\\\"token number\\\">0.433013</span>  <span class=\\\"token number\\\">0.75</span>     <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.5</span>        \\n   <span class=\\\"token number\\\">0.25</span>      <span class=\\\"token number\\\">0.433013</span>  <span class=\\\"token number\\\">0.866025</span> \\n</code></pre>\\n<p>We can find the corresponding Euler angles (in radians)</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span> R<span class=\\\"token punctuation\\\">.</span>eul<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\narray<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1.57079633</span><span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">0.52359878</span><span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">2.0943951</span> <span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>Frequently in robotics we want a sequence, a trajectory, of rotation matrices or poses. These pose classes inherit capability from the <code class=\\\"language-none\\\">list</code> class</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>   <span class=\\\"token comment\\\"># the identity</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>R1<span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R<span class=\\\"token punctuation\\\">.</span>append<span class=\\\"token punctuation\\\">(</span>R2<span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">3</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R<span class=\\\"token punctuation\\\">[</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">]</span>\\n   <span class=\\\"token number\\\">1</span>         <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0</span>          \\n   <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0.955336</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.29552</span>    \\n   <span class=\\\"token number\\\">0</span>         <span class=\\\"token number\\\">0.29552</span>   <span class=\\\"token number\\\">0.955336</span>             \\n</code></pre>\\n<p>and this can be used in <code class=\\\"language-none\\\">for</code> loops and list comprehensions.</p>\\n<p>An alternative way of constructing this would be (<code class=\\\"language-none\\\">R1</code>, <code class=\\\"language-none\\\">R2</code> defined above)</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">(</span> <span class=\\\"token punctuation\\\">[</span> SO3<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">,</span> R1<span class=\\\"token punctuation\\\">,</span> R2 <span class=\\\"token punctuation\\\">]</span> <span class=\\\"token punctuation\\\">)</span>       \\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">3</span>\\n</code></pre>\\n<p>Many of the constructors such as <code class=\\\"language-none\\\">.Rx</code>, <code class=\\\"language-none\\\">.Ry</code> and <code class=\\\"language-none\\\">.Rz</code> support vectorization</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> R <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">.</span>Rx<span class=\\\"token punctuation\\\">(</span> np<span class=\\\"token punctuation\\\">.</span>arange<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">2</span><span class=\\\"token operator\\\">*</span>np<span class=\\\"token punctuation\\\">.</span>pi<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">0.2</span><span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">32</span>\\n</code></pre>\\n<p>which has created, in a single line, a list of rotation matrices.</p>\\n<p>Vectorization also applies to the operators, for instance</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> A <span class=\\\"token operator\\\">=</span> R <span class=\\\"token operator\\\">*</span> SO3<span class=\\\"token punctuation\\\">.</span>Ry<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.5</span><span class=\\\"token punctuation\\\">)</span>\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">32</span>\\n</code></pre>\\n<p>will produce a result where each element is the product of each element of the left-hand side with the right-hand side, ie. <code class=\\\"language-none\\\">R[i] * SO3.Ry(0.5)</code>.</p>\\n<p>Similarly</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> A <span class=\\\"token operator\\\">=</span> SO3<span class=\\\"token punctuation\\\">.</span>Ry<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.5</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> R \\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">32</span>\\n</code></pre>\\n<p>will produce a result where each element is the product of the left-hand side with each element of the right-hand side , ie. <code class=\\\"language-none\\\">SO3.Ry(0.5) * R[i] </code>.</p>\\n<p>Finally</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> A <span class=\\\"token operator\\\">=</span> R <span class=\\\"token operator\\\">*</span> R \\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> <span class=\\\"token builtin\\\">len</span><span class=\\\"token punctuation\\\">(</span>R<span class=\\\"token punctuation\\\">)</span>\\n <span class=\\\"token number\\\">32</span>\\n</code></pre>\\n<p>will produce a result where each element is the product of each element of the left-hand side with each element of the right-hand side , ie. <code class=\\\"language-none\\\">R[i] * R[i] </code>.</p>\\n<p>The underlying representation of these classes is a numpy matrix, but the class ensures that the structure of that matrix is valid for the particular group represented: SO(2), SE(2), SO(3), SE(3).  Any operation that is not valid for the group will return a matrix rather than a pose class, for example</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> SO3<span class=\\\"token punctuation\\\">.</span>Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.3</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> <span class=\\\"token number\\\">2</span>\\narray<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span> <span class=\\\"token number\\\">2</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>\\n       <span class=\\\"token punctuation\\\">[</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">1.91067298</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.59104041</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>\\n       <span class=\\\"token punctuation\\\">[</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">0.59104041</span><span class=\\\"token punctuation\\\">,</span>  <span class=\\\"token number\\\">1.91067298</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token operator\\\">>></span><span class=\\\"token operator\\\">></span> SO3<span class=\\\"token punctuation\\\">.</span>Rx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.3</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">-</span> <span class=\\\"token number\\\">1</span>\\narray<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">[</span><span class=\\\"token punctuation\\\">[</span> <span class=\\\"token number\\\">0</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>\\n       <span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.04466351</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1.29552021</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">,</span>\\n       <span class=\\\"token punctuation\\\">[</span><span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">.</span>        <span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.70447979</span><span class=\\\"token punctuation\\\">,</span> <span class=\\\"token operator\\\">-</span><span class=\\\"token number\\\">0.04466351</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">]</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p>We can print and plot these objects as well</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">&gt;&gt;&gt; T = SE3(1,2,3) * SE3.Rx(30, 'deg')\\n&gt;&gt;&gt; T.print()\\n   1         0         0         1          \\n   0         0.866025 -0.5       2          \\n   0         0.5       0.866025  3          \\n   0         0         0         1          \\n\\n&gt;&gt;&gt; T.printline()\\nt =        1,        2,        3; rpy/zyx =       30,        0,        0 deg\\n\\n&gt;&gt;&gt; T.plot()\\n</code></pre>\\n<p><img src=https://github.com/petercorke/spatialmath-python/raw/master/docs/figs/fig1.png alt=\\\"trplot\\\"></p>\\n<p><code class=\\\"language-none\\\">printline</code> is a compact single line format for tabular listing, whereas <code class=\\\"language-none\\\">print</code> shows the underlying matrix and for consoles that support it, it is colorised, with rotational elements in red and translational elements in blue.</p>\\n<p>For more detail checkout the shipped Python notebooks:</p>\\n<ul>\\n<li><a href=https://github.com/petercorke/spatialmath-python/blob/master/spatialmath/gentle-introduction.ipynb>gentle introduction</a></li>\\n<li><a href=https://github.com/petercorke/spatialmath-python/blob/master/spatialmath/introduction.ipynb>deeper introduction</a></li>\\n</ul>\\n<p>You can browse it statically through the links above, or clone the toolbox and run them interactively using <a href=\\\"https://jupyter.org\\\">Jupyter</a> or <a href=\\\"https://jupyter.org\\\">JupyterLab</a>.</p>\\n<h2>Low-level spatial math</h2>\\n<p>Import the low-level transform functions</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">&gt;&gt;&gt; import spatialmath.base as tr\\n</code></pre>\\n<p>We can create a 3D rotation matrix</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">&gt;&gt;&gt; tr.rotx(0.3)\\narray([[ 1.        ,  0.        ,  0.        ],\\n       [ 0.        ,  0.95533649, -0.29552021],\\n       [ 0.        ,  0.29552021,  0.95533649]])\\n\\n&gt;&gt;&gt; tr.rotx(30, unit='deg')\\narray([[ 1.       ,  0.       ,  0.       ],\\n       [ 0.       ,  0.8660254, -0.5      ],\\n       [ 0.       ,  0.5      ,  0.8660254]])\\n</code></pre>\\n<p>The results are <code class=\\\"language-none\\\">numpy</code> arrays so to perform matrix multiplication you need to use the <code class=\\\"language-none\\\">@</code> operator, for example</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rotx(0.3) @ roty(0.2)\\n</code></pre>\\n<p>We also support multiple ways of passing vector information to functions that require it:</p>\\n<ul>\\n<li>as separate positional arguments</li>\\n</ul>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">transl2(1, 2)\\narray([[1., 0., 1.],\\n       [0., 1., 2.],\\n       [0., 0., 1.]])\\n</code></pre>\\n<ul>\\n<li>as a list or a tuple</li>\\n</ul>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">transl2( [1,2] )\\narray([[1., 0., 1.],\\n       [0., 1., 2.],\\n       [0., 0., 1.]])\\n\\ntransl2( (1,2) )\\nOut[444]: \\narray([[1., 0., 1.],\\n       [0., 1., 2.],\\n       [0., 0., 1.]])\\n</code></pre>\\n<ul>\\n<li>or as a <code class=\\\"language-none\\\">numpy</code> array</li>\\n</ul>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">transl2( np.array([1,2]) )\\nOut[445]: \\narray([[1., 0., 1.],\\n       [0., 1., 2.],\\n       [0., 0., 1.]])\\n</code></pre>\\n<p>There is a single module that deals with quaternions, unit or not, and the representation is a <code class=\\\"language-none\\\">numpy</code> array of four elements.  As above, functions can accept the <code class=\\\"language-none\\\">numpy</code> array, a list, dict or <code class=\\\"language-none\\\">numpy</code> row or column vectors.</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">&gt;&gt;&gt; from spatialmath.base.quaternion import *\\n&gt;&gt;&gt; q = qqmul([1,2,3,4], [5,6,7,8])\\n&gt;&gt;&gt; q\\narray([-60,  12,  30,  24])\\n&gt;&gt;&gt; qprint(q)\\n-60.000000 &lt; 12.000000, 30.000000, 24.000000 &gt;\\n&gt;&gt;&gt; qnorm(q)\\n72.24956747275377\\n</code></pre>\\n<h2>Graphics</h2>\\n<p><img src=https://github.com/petercorke/spatialmath-python/raw/master/docs/figs/transforms3d.png alt=\\\"trplot\\\"></p>\\n<p>The functions support various plotting styles</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">trplot( transl(1,2,3), frame='A', rviz=True, width=1, dims=[0, 10, 0, 10, 0, 10])\\ntrplot( transl(3,1, 2), color='red', width=3, frame='B')\\ntrplot( transl(4, 3, 1)@trotx(math.pi/3), color='green', frame='c', dims=[0,4,0,4,0,4])\\n</code></pre>\\n<p>Animation is straightforward</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">tranimate(transl(4, 3, 4)@trotx(2)@troty(-2), frame=' arrow=False, dims=[0, 5], nframes=200)\\n</code></pre>\\n<p>and it can be saved to a file by</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">tranimate(transl(4, 3, 4)@trotx(2)@troty(-2), frame=' arrow=False, dims=[0, 5], nframes=200, movie='out.mp4')\\n</code></pre>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/f53aa66b328813113b38f087b50da80f.jpg\\\"><source src=\\\"/_next/static/gifs/f53aa66b328813113b38f087b50da80f.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>At the moment we can only save as an MP4, but the following incantation will covert that to an animated GIF for embedding in web pages</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">ffmpeg -i out -r 20 -vf &quot;fps=10,scale=640:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse&quot; out.gif\\n</code></pre>\\n<h2>Symbolic support</h2>\\n<p>Some functions have support for symbolic variables, for example</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">import sympy\\n\\ntheta = sym.symbols('theta')\\nprint(rotx(theta))\\n[[1 0 0]\\n [0 cos(theta) -sin(theta)]\\n [0 sin(theta) cos(theta)]]\\n</code></pre>\\n<p>The resulting <code class=\\\"language-none\\\">numpy</code> array is an array of symbolic objects not numbers â€“ the constants are also symbolic objects.  You can read the elements of the matrix</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">a = T[0,0]\\n\\na\\nOut[258]: 1\\n\\ntype(a)\\nOut[259]: int\\n\\na = T[1,1]\\na\\nOut[256]: \\ncos(theta)\\ntype(a)\\nOut[255]: cos\\n</code></pre>\\n<p>We see that the symbolic constants are converted back to Python numeric types on read.</p>\\n<p>Similarly when we assign an element or slice of the symbolic matrix to a numeric value, they are converted to symbolic constants on the way in.</p>\\n\",\"name\":\"Spatialmath Python\",\"type\":\"code\",\"url\":\"https://github.com/petercorke/spatialmath-python\",\"image\":\"/_next/static/images/CartesianSnakes_LogoW-292d336acf7d3ffebbf1da8f9f6ccc9d.png\",\"image_fit\":\"contain\",\"src\":\"/content/robotics_toolbox/spatialmath-python.md\",\"id\":\"spatialmath-python\",\"image_position\":\"center\"},{\"content\":\"<h1>Swift</h1>\\n<p><a href=\\\"https://badge.fury.io/py/swift-sim\\\"><img src=\\\"https://badge.fury.io/py/swift-sim.svg\\\" alt=\\\"PyPI version\\\"></a>\\n<a href=\\\"https://img.shields.io/pypi/pyversions/swift-sim\\\"><img src=\\\"https://img.shields.io/pypi/pyversions/swift-sim\\\" alt=\\\"PyPI - Python Version\\\"></a>\\n<a href=\\\"https://opensource.org/licenses/MIT\\\"><img src=\\\"https://img.shields.io/badge/License-MIT-yellow.svg\\\" alt=\\\"License: MIT\\\"></a>\\n<a href=\\\"https://qcr.github.io\\\"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"></a></p>\\n<p>Swift is a light-weight browser-based simulator built on top of the <a href=https://github.com/petercorke/robotics-toolbox-python>Robotics Toolbox for Python</a>. This simulator provides robotics-specific functionality for rapid prototyping of algorithms, research, and education. Built using Python and Javascript, Swift is cross-platform (Linux, MacOS, and Windows) while also leveraging the ubiquity and support of these languages.</p>\\n<p>Through the <a href=https://github.com/petercorke/robotics-toolbox-python>Robotics Toolbox for Python</a>, Swift can visualise over 30 supplied robot models: well-known contemporary robots from Franka-Emika, Kinova, Universal Robotics, Rethink as well as classical robots such as the Puma 560 and the Stanford arm. Swift is under development and will support mobile robots in the future.</p>\\n<p>Swift provides:</p>\\n<ul>\\n<li>visualisation of mesh objects (Collada and STL files) and primitive shapes;</li>\\n<li>robot visualisation and simulation;</li>\\n<li>recording and saving a video of the simulation;</li>\\n<li>source code which can be read for learning and teaching;</li>\\n</ul>\\n<h2>Installing</h2>\\n<h3>Using pip</h3>\\n<p>Swift is designed to be controlled through the <a href=https://github.com/petercorke/robotics-toolbox-python>Robotics Toolbox for Python</a>. By installing the toolbox through PyPI, swift is installed as a dependency</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">pip3 <span class=\\\"token function\\\">install</span> roboticstoolbox-python\\n</code></pre>\\n<p>Otherwise, Swift can be install by</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\">pip3 <span class=\\\"token function\\\">install</span> swift-sim\\n</code></pre>\\n<h3>From GitHub</h3>\\n<p>To install the latest version from GitHub</p>\\n<pre class=\\\"language-shell\\\"><code class=\\\"language-shell\\\"><span class=\\\"token function\\\">git</span> clone https://github.com/jhavl/swift.git\\n<span class=\\\"token builtin class-name\\\">cd</span> swift\\npip3 <span class=\\\"token function\\\">install</span> -e <span class=\\\"token builtin class-name\\\">.</span>\\n</code></pre>\\n<h2>Code Examples</h2>\\n<h3>Robot Plot</h3>\\n<p>We will load a model of the Franka-Emika Panda robot and plot it. We set the joint angles of the robot into the ready joint configuration qr.</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">import</span> roboticstoolbox <span class=\\\"token keyword\\\">as</span> rp\\n\\npanda <span class=\\\"token operator\\\">=</span> rp<span class=\\\"token punctuation\\\">.</span>models<span class=\\\"token punctuation\\\">.</span>Panda<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\npanda<span class=\\\"token punctuation\\\">.</span>plot<span class=\\\"token punctuation\\\">(</span>q<span class=\\\"token operator\\\">=</span>panda<span class=\\\"token punctuation\\\">.</span>qr<span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p align=\\\"center\\\">\\n <img src=https://github.com/jhavl/swift/blob/master/.github/figures/panda.png>\\n</p>\\n<h3>Resolved-Rate Motion Control</h3>\\n<p>We will load a model of the Franka-Emika Panda robot and make it travel towards a goal pose defined by the variable Tep.</p>\\n<pre class=\\\"language-python\\\"><code class=\\\"language-python\\\"><span class=\\\"token keyword\\\">import</span> roboticstoolbox <span class=\\\"token keyword\\\">as</span> rtb\\n<span class=\\\"token keyword\\\">import</span> spatialmath <span class=\\\"token keyword\\\">as</span> sm\\n<span class=\\\"token keyword\\\">import</span> numpy <span class=\\\"token keyword\\\">as</span> np\\n\\n<span class=\\\"token comment\\\"># Make and instance of the Swift simulator and open it</span>\\nenv <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>backends<span class=\\\"token punctuation\\\">.</span>Swift<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\nenv<span class=\\\"token punctuation\\\">.</span>launch<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Make a panda model and set its joint angles to the ready joint configuration</span>\\npanda <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>models<span class=\\\"token punctuation\\\">.</span>Panda<span class=\\\"token punctuation\\\">(</span><span class=\\\"token punctuation\\\">)</span>\\npanda<span class=\\\"token punctuation\\\">.</span>q <span class=\\\"token operator\\\">=</span> panda<span class=\\\"token punctuation\\\">.</span>qr\\n\\n<span class=\\\"token comment\\\"># Set a desired and effector pose an an offset from the current end-effector pose</span>\\nTep <span class=\\\"token operator\\\">=</span> panda<span class=\\\"token punctuation\\\">.</span>fkine<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> sm<span class=\\\"token punctuation\\\">.</span>SE3<span class=\\\"token punctuation\\\">.</span>Tx<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> sm<span class=\\\"token punctuation\\\">.</span>SE3<span class=\\\"token punctuation\\\">.</span>Ty<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.2</span><span class=\\\"token punctuation\\\">)</span> <span class=\\\"token operator\\\">*</span> sm<span class=\\\"token punctuation\\\">.</span>SE3<span class=\\\"token punctuation\\\">.</span>Tz<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.45</span><span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Add the robot to the simulator</span>\\nenv<span class=\\\"token punctuation\\\">.</span>add<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">)</span>\\n\\n<span class=\\\"token comment\\\"># Simulate the robot while it has not arrived at the goal</span>\\narrived <span class=\\\"token operator\\\">=</span> <span class=\\\"token boolean\\\">False</span>\\n<span class=\\\"token keyword\\\">while</span> <span class=\\\"token keyword\\\">not</span> arrived<span class=\\\"token punctuation\\\">:</span>\\n\\n    <span class=\\\"token comment\\\"># Work out the required end-effector velocity to go towards the goal</span>\\n    v<span class=\\\"token punctuation\\\">,</span> arrived <span class=\\\"token operator\\\">=</span> rtb<span class=\\\"token punctuation\\\">.</span>p_servo<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">.</span>fkine<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">,</span> Tep<span class=\\\"token punctuation\\\">,</span> <span class=\\\"token number\\\">1</span><span class=\\\"token punctuation\\\">)</span>\\n    \\n    <span class=\\\"token comment\\\"># Set the Panda's joint velocities</span>\\n    panda<span class=\\\"token punctuation\\\">.</span>qd <span class=\\\"token operator\\\">=</span> np<span class=\\\"token punctuation\\\">.</span>linalg<span class=\\\"token punctuation\\\">.</span>pinv<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">.</span>jacobe<span class=\\\"token punctuation\\\">(</span>panda<span class=\\\"token punctuation\\\">.</span>q<span class=\\\"token punctuation\\\">)</span><span class=\\\"token punctuation\\\">)</span> @ v\\n    \\n    <span class=\\\"token comment\\\"># Step the simulator by 50 milliseconds</span>\\n    env<span class=\\\"token punctuation\\\">.</span>step<span class=\\\"token punctuation\\\">(</span><span class=\\\"token number\\\">0.05</span><span class=\\\"token punctuation\\\">)</span>\\n</code></pre>\\n<p align=\\\"center\\\">\\n <img src=\\\"/_next/static/gifs/0c7102b6a6bab096effb4fed9f4ba2e5.gif\\\">\\n</p>\\n\",\"name\":\"Swift\",\"type\":\"code\",\"url\":\"https://github.com/jhavl/swift\",\"image\":\"/_next/static/images/panda-08fefd194b35f7baa2af3c22759caa53.png\",\"src\":\"/content/robotics_toolbox/swift.md\",\"id\":\"swift\",\"image_position\":\"center\"}]"},"__N_SSG":true}