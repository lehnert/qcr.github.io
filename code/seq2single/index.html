<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">seq2single</span><a href="https://github.com/oravus/seq2single" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">oravus/seq2single</span><span class="markdown-body mdc-typography--body1"><div><h2>Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</h2>
<p>This is the source code for the paper titled: &quot;Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation&quot;, pre-print avialable <a href="https://arxiv.org/abs/1902.07381">here</a>.</p>
<p>If you find this work useful, please cite it as:
Garg, S., Babu V, M., Dharmasiri, T., Hausler, S., Suenderhauf, N., Kumar, S., Drummond, T., &amp; Milford, M. (2019). Look no deeper: Recognizing places from opposing viewpoints under varying scene appearance using single-view depth estimation. In IEEE International Conference on Robotics and Automation (ICRA), 2019. IEEE.</p>
<p>bibtex:</p>
<pre class="language-none"><code class="language-none">@inproceedings{garg2019look,
title={Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation},
author={Garg, Sourav and Babu V, Madhu and Dharmasiri, Thanuja and Hausler, Stephen and Suenderhauf, Niko and Kumar, Swagat and Drummond, Tom and Milford, Michael},
booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
year={2019}
}
</code></pre>
<p><img src="/_next/static/images/illustration-743ee8feee1528b8dbbe46dea09ab0e9.png" alt="Illustration of the proposed approach" title="Illustration of the proposed approach."></p>
<p><img src="/_next/static/images/topometric-5ce9c835ea539de03099f4f69c57ad17.png" alt="An image depicting topometric representation." title="A topometric representation."></p>
<h4>Requirements</h4>
<ul>
<li>Ubuntu	(Tested on <em>16.04</em>)</li>
<li>Jupyter	(Tested on <em>4.4.0</em>)</li>
<li>Python	(Tested on <em>3.5.6</em>)
<ul>
<li>numpy	(Tested on <em>1.15.2</em>)</li>
<li>scipy	(Tested on <em>1.1.0</em>)</li>
</ul>
</li>
</ul>
<p>Optionally, for vis_results.ipynb:</p>
<ul>
<li>Matplotlib	(Tested on <em>2.0.2</em>)</li>
</ul>
<h4>Download an example dataset and its pre-computed representations</h4>
<ol>
<li>
<p>In <code class="language-none">seq2single/precomputed/</code>, download <a href="https://mega.nz/#F!Z4Z3gAzb!KI48uGHJJza90DP7-Kz1kA">pre-computed representations (<em>~10 GB</em>)</a>. Please refer to the <code class="language-none">seq2single/precomputed/readme.md</code> for instructions on how to compute these representations.</p>
</li>
<li>
<p>[Optional] In <code class="language-none">seq2single/images/</code>, download <a href="https://mega.nz/#F!h5QB2ayI!H7p0UCxATd6MUdszMZWNOA">images (<em>~1 GB</em>)</a>. These images are a subset of two different traverses from the <a href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford Robotcar dataset</a>.</p>
</li>
</ol>
<p>(Note: These download links from Mega.nz require you to first create an account (free))</p>
<h4>Run</h4>
<ol>
<li>The Jupyter notebook seq2single.ipynb first loads the pre-computed global image descriptors to find top matches. These matches are re-ranked with the proposed method using the pre-computed depth masks and dense conv5 features.</li>
</ol>
<h4>License</h4>
<p>The code is released under MIT License.</p>
<h2>Related Projects</h2>
<p><a href=https://github.com/oravus/DeltaDescriptors>Delta Descriptors (2020)</a></p>
<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>
<p><a href=https://github.com/oravus/lostX>LoST (2018)</a></p>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003ch2\u003eLook No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation\u003c/h2\u003e\\n\u003cp\u003eThis is the source code for the paper titled: \u0026quot;Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation\u0026quot;, pre-print avialable \u003ca href=\\\"https://arxiv.org/abs/1902.07381\\\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eIf you find this work useful, please cite it as:\\nGarg, S., Babu V, M., Dharmasiri, T., Hausler, S., Suenderhauf, N., Kumar, S., Drummond, T., \u0026amp; Milford, M. (2019). Look no deeper: Recognizing places from opposing viewpoints under varying scene appearance using single-view depth estimation. In IEEE International Conference on Robotics and Automation (ICRA), 2019. IEEE.\u003c/p\u003e\\n\u003cp\u003ebibtex:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@inproceedings{garg2019look,\\ntitle={Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation},\\nauthor={Garg, Sourav and Babu V, Madhu and Dharmasiri, Thanuja and Hausler, Stephen and Suenderhauf, Niko and Kumar, Swagat and Drummond, Tom and Milford, Michael},\\nbooktitle={IEEE International Conference on Robotics and Automation (ICRA)},\\nyear={2019}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/illustration-743ee8feee1528b8dbbe46dea09ab0e9.png\\\" alt=\\\"Illustration of the proposed approach\\\" title=\\\"Illustration of the proposed approach.\\\"\u003e\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/topometric-5ce9c835ea539de03099f4f69c57ad17.png\\\" alt=\\\"An image depicting topometric representation.\\\" title=\\\"A topometric representation.\\\"\u003e\u003c/p\u003e\\n\u003ch4\u003eRequirements\u003c/h4\u003e\\n\u003cul\u003e\\n\u003cli\u003eUbuntu\\t(Tested on \u003cem\u003e16.04\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003eJupyter\\t(Tested on \u003cem\u003e4.4.0\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003ePython\\t(Tested on \u003cem\u003e3.5.6\u003c/em\u003e)\\n\u003cul\u003e\\n\u003cli\u003enumpy\\t(Tested on \u003cem\u003e1.15.2\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003escipy\\t(Tested on \u003cem\u003e1.1.0\u003c/em\u003e)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eOptionally, for vis_results.ipynb:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eMatplotlib\\t(Tested on \u003cem\u003e2.0.2\u003c/em\u003e)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch4\u003eDownload an example dataset and its pre-computed representations\u003c/h4\u003e\\n\u003col\u003e\\n\u003cli\u003e\\n\u003cp\u003eIn \u003ccode class=\\\"language-none\\\"\u003eseq2single/precomputed/\u003c/code\u003e, download \u003ca href=\\\"https://mega.nz/#F!Z4Z3gAzb!KI48uGHJJza90DP7-Kz1kA\\\"\u003epre-computed representations (\u003cem\u003e~10 GB\u003c/em\u003e)\u003c/a\u003e. Please refer to the \u003ccode class=\\\"language-none\\\"\u003eseq2single/precomputed/readme.md\u003c/code\u003e for instructions on how to compute these representations.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e[Optional] In \u003ccode class=\\\"language-none\\\"\u003eseq2single/images/\u003c/code\u003e, download \u003ca href=\\\"https://mega.nz/#F!h5QB2ayI!H7p0UCxATd6MUdszMZWNOA\\\"\u003eimages (\u003cem\u003e~1 GB\u003c/em\u003e)\u003c/a\u003e. These images are a subset of two different traverses from the \u003ca href=\\\"https://robotcar-dataset.robots.ox.ac.uk/\\\"\u003eOxford Robotcar dataset\u003c/a\u003e.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003cp\u003e(Note: These download links from Mega.nz require you to first create an account (free))\u003c/p\u003e\\n\u003ch4\u003eRun\u003c/h4\u003e\\n\u003col\u003e\\n\u003cli\u003eThe Jupyter notebook seq2single.ipynb first loads the pre-computed global image descriptors to find top matches. These matches are re-ranked with the proposed method using the pre-computed depth masks and dense conv5 features.\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003ch4\u003eLicense\u003c/h4\u003e\\n\u003cp\u003eThe code is released under MIT License.\u003c/p\u003e\\n\u003ch2\u003eRelated Projects\u003c/h2\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/DeltaDescriptors\u003eDelta Descriptors (2020)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/CoarseHash\u003eCoarseHash (2020)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/lostX\u003eLoST (2018)\u003c/a\u003e\u003c/p\u003e\\n\",\"name\":\"seq2single\",\"type\":\"code\",\"url\":\"https://github.com/oravus/seq2single\",\"src\":\"/content/visual_place_recognition/seq2single.md\",\"id\":\"seq2single\",\"image_position\":\"center\",\"image\":\"/_next/static/images/illustration-743ee8feee1528b8dbbe46dea09ab0e9.png\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"seq2single"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>