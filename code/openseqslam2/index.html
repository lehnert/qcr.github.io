<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">OpenSeqSLAM2</span><a href="https://github.com/qcr/openseqslam2" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">qcr/openseqslam2</span><span class="markdown-body mdc-typography--body1"><div><h1>OpenSeqSLAM2.0 Toolbox</h1>
<p><img src="/_next/static/images/openseqslam2-e3f1e85549e34343dc852ed782b11093.png" alt="The various interactive screens in OpenSeqSLAM2"></p>
<p>OpenSeqSLAM2.0 is a MATLAB toolbox that allows users to thoroughly explore the SeqSLAM method in addressing the visual place recognition problem. The visual place recognition problem is centred around recognising a previously traversed route, regardless of whether it is seen during the day or night, in clear or inclement conditions, or in summer or winter. Recognising previously traversed routes is a crucial capability of navigating robots. Through the graphical interfaces packaged in OpenSeqSLAM2 users are able to:</p>
<ul>
<li>explore a number of previously published variations to the SeqSLAM method (including search and match selection methods);</li>
<li>visually track progress;</li>
<li>interactively tune parameters;</li>
<li>dynamically reconfigure matching parameters while viewing results;</li>
<li>explore precision-recall statistics;</li>
<li>visualise difference matrices, match sequence images, and image pre-processing steps;</li>
<li>view and export matching videos;</li>
<li>automatically optimise selection thresholds against a ground truth;</li>
<li>sweep any numeric parameter value through a batch operation mode; and</li>
<li>operate in headless mode with parallelisation available.</li>
</ul>
<p>The toolbox is open-source and downloadable from the <a href=https://github.com/qcr/openseqslam2/releases>releases tab</a>. All we ask is that if you use OpenSeqSLAM2 in any academic work, that you include a reference to corresponding publication (bibtex is available at the bottom of the page).</p>
<h2>How to use the toolbox</h2>
<p>The toolbox is designed to be simple to use (it runs out of the box without any initial configuration required). To run the toolbox, simple run the command below (with the toolbox root directory in your MATLAB path):</p>
<pre class="language-matlab"><code class="language-matlab"><span class="token function">OpenSeqSLAM2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>There are a number of default configuration files included in the <code class="language-none">.config</code> directory which showcase the capabilities of the toolbox. To use a configuration file, open the toolbox as described above, then use the <code class="language-none">Import config</code> button. A summary of the features showcased in each of the configuration files is included below:</p>
<ul>
<li><strong><code class="language-none">'images_same'</code></strong>: The trimmed Nordland dataset images, with the same dataset used as both reference and query. Trajectory based search is used, and a velocity-based ground truth is included, but not used for auto-optimisation of match threshold.</li>
<li><strong><code class="language-none">'images_diff'</code></strong>: The trimmed Nordland dataset images, with the summer traversal used as the reference dataset and the winter traversal as the query. Trajectory based search is used, and a *.csv based ground truth is used for auto-optimising the match threshold selection.</li>
<li><strong><code class="language-none">'videos_same'</code></strong>: The day night video dataset, with the same video used as both the reference and query dataset. Trajectory based search is used, with no ground truth provided.</li>
<li><strong><code class="language-none">'videos_diff'</code></strong>: The day night video dataset, with the day traversal used as the reference dataset and the night traversal as the query. Trajectory based search is used, with no ground truth provided.</li>
<li><strong><code class="language-none">'hybrid_search'</code></strong>: Same as <code class="language-none">'videos_diff'</code>, but the hybrid search is used instead of trajectory search.</li>
<li><strong><code class="language-none">'no_gui'</code></strong>: Same as <code class="language-none">'videos_diff'</code>, but the progress is presented in the console rather than GUI and no results GUI is shown (tip: run OpenSeqSLAM2(‘<configpath>/no_gui.xml’) to see how the toolbox can run entirely headless)</li>
<li><strong><code class="language-none">'batch_with_gui'</code></strong>: Same as <code class="language-none">'images_diff'</code>, but a batch parameter sweep of the sequence length parameter is performed. The progress GUI shows the progress of the individual iteration and overall in separate windows.</li>
<li><strong><code class="language-none">'parrallelised_batch'</code></strong>: Same as <code class="language-none">'batch_with_gui'</code>, but the parameter sweep is done in parallel mode (which cannot be performed with the Progress GUI). The parallel mode will use a worker for each core available in the host CPU.</li>
<li><strong><code class="language-none">'default'</code></strong>: is set to <code class="language-none">'images_diff'</code></li>
</ul>
<p><em><em>Note:</em> the programs in the <code class="language-none">./bin</code> directory can be run standalone by providing the appropriate results / config structs as arguments if you would like to use only a specific part of the pipeline (i.e. only configuration, or progress wrapped execution, or viewing results).</em></p>
<h2>Citation details</h2>
<p>If using the toolbox in any academic work, please include the following citation:</p>
<pre class="language-bibtex"><code class="language-bibtex">@ARTICLE{2018openseqslam2,
   author = {{Talbot}, B. and {Garg}, S. and {Milford}, M.},
    title = &quot;{OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions}&quot;,
  journal = {ArXiv e-prints},
archivePrefix = &quot;arXiv&quot;,
   eprint = {1804.02156},
 primaryClass = &quot;cs.RO&quot;,
 keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
     year = 2018,
    month = apr,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402156T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
</code></pre>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003ch1\u003eOpenSeqSLAM2.0 Toolbox\u003c/h1\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/openseqslam2-e3f1e85549e34343dc852ed782b11093.png\\\" alt=\\\"The various interactive screens in OpenSeqSLAM2\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eOpenSeqSLAM2.0 is a MATLAB toolbox that allows users to thoroughly explore the SeqSLAM method in addressing the visual place recognition problem. The visual place recognition problem is centred around recognising a previously traversed route, regardless of whether it is seen during the day or night, in clear or inclement conditions, or in summer or winter. Recognising previously traversed routes is a crucial capability of navigating robots. Through the graphical interfaces packaged in OpenSeqSLAM2 users are able to:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eexplore a number of previously published variations to the SeqSLAM method (including search and match selection methods);\u003c/li\u003e\\n\u003cli\u003evisually track progress;\u003c/li\u003e\\n\u003cli\u003einteractively tune parameters;\u003c/li\u003e\\n\u003cli\u003edynamically reconfigure matching parameters while viewing results;\u003c/li\u003e\\n\u003cli\u003eexplore precision-recall statistics;\u003c/li\u003e\\n\u003cli\u003evisualise difference matrices, match sequence images, and image pre-processing steps;\u003c/li\u003e\\n\u003cli\u003eview and export matching videos;\u003c/li\u003e\\n\u003cli\u003eautomatically optimise selection thresholds against a ground truth;\u003c/li\u003e\\n\u003cli\u003esweep any numeric parameter value through a batch operation mode; and\u003c/li\u003e\\n\u003cli\u003eoperate in headless mode with parallelisation available.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eThe toolbox is open-source and downloadable from the \u003ca href=https://github.com/qcr/openseqslam2/releases\u003ereleases tab\u003c/a\u003e. All we ask is that if you use OpenSeqSLAM2 in any academic work, that you include a reference to corresponding publication (bibtex is available at the bottom of the page).\u003c/p\u003e\\n\u003ch2\u003eHow to use the toolbox\u003c/h2\u003e\\n\u003cp\u003eThe toolbox is designed to be simple to use (it runs out of the box without any initial configuration required). To run the toolbox, simple run the command below (with the toolbox root directory in your MATLAB path):\u003c/p\u003e\\n\u003cpre class=\\\"language-matlab\\\"\u003e\u003ccode class=\\\"language-matlab\\\"\u003e\u003cspan class=\\\"token function\\\"\u003eOpenSeqSLAM2\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e;\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThere are a number of default configuration files included in the \u003ccode class=\\\"language-none\\\"\u003e.config\u003c/code\u003e directory which showcase the capabilities of the toolbox. To use a configuration file, open the toolbox as described above, then use the \u003ccode class=\\\"language-none\\\"\u003eImport config\u003c/code\u003e button. A summary of the features showcased in each of the configuration files is included below:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'images_same'\u003c/code\u003e\u003c/strong\u003e: The trimmed Nordland dataset images, with the same dataset used as both reference and query. Trajectory based search is used, and a velocity-based ground truth is included, but not used for auto-optimisation of match threshold.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'images_diff'\u003c/code\u003e\u003c/strong\u003e: The trimmed Nordland dataset images, with the summer traversal used as the reference dataset and the winter traversal as the query. Trajectory based search is used, and a *.csv based ground truth is used for auto-optimising the match threshold selection.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'videos_same'\u003c/code\u003e\u003c/strong\u003e: The day night video dataset, with the same video used as both the reference and query dataset. Trajectory based search is used, with no ground truth provided.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'videos_diff'\u003c/code\u003e\u003c/strong\u003e: The day night video dataset, with the day traversal used as the reference dataset and the night traversal as the query. Trajectory based search is used, with no ground truth provided.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'hybrid_search'\u003c/code\u003e\u003c/strong\u003e: Same as \u003ccode class=\\\"language-none\\\"\u003e'videos_diff'\u003c/code\u003e, but the hybrid search is used instead of trajectory search.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'no_gui'\u003c/code\u003e\u003c/strong\u003e: Same as \u003ccode class=\\\"language-none\\\"\u003e'videos_diff'\u003c/code\u003e, but the progress is presented in the console rather than GUI and no results GUI is shown (tip: run OpenSeqSLAM2(‘\u003cconfigpath\u003e/no_gui.xml’) to see how the toolbox can run entirely headless)\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'batch_with_gui'\u003c/code\u003e\u003c/strong\u003e: Same as \u003ccode class=\\\"language-none\\\"\u003e'images_diff'\u003c/code\u003e, but a batch parameter sweep of the sequence length parameter is performed. The progress GUI shows the progress of the individual iteration and overall in separate windows.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'parrallelised_batch'\u003c/code\u003e\u003c/strong\u003e: Same as \u003ccode class=\\\"language-none\\\"\u003e'batch_with_gui'\u003c/code\u003e, but the parameter sweep is done in parallel mode (which cannot be performed with the Progress GUI). The parallel mode will use a worker for each core available in the host CPU.\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ccode class=\\\"language-none\\\"\u003e'default'\u003c/code\u003e\u003c/strong\u003e: is set to \u003ccode class=\\\"language-none\\\"\u003e'images_diff'\u003c/code\u003e\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003e\u003cem\u003e\u003cem\u003eNote:\u003c/em\u003e the programs in the \u003ccode class=\\\"language-none\\\"\u003e./bin\u003c/code\u003e directory can be run standalone by providing the appropriate results / config structs as arguments if you would like to use only a specific part of the pipeline (i.e. only configuration, or progress wrapped execution, or viewing results).\u003c/em\u003e\u003c/p\u003e\\n\u003ch2\u003eCitation details\u003c/h2\u003e\\n\u003cp\u003eIf using the toolbox in any academic work, please include the following citation:\u003c/p\u003e\\n\u003cpre class=\\\"language-bibtex\\\"\u003e\u003ccode class=\\\"language-bibtex\\\"\u003e@ARTICLE{2018openseqslam2,\\n   author = {{Talbot}, B. and {Garg}, S. and {Milford}, M.},\\n    title = \u0026quot;{OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions}\u0026quot;,\\n  journal = {ArXiv e-prints},\\narchivePrefix = \u0026quot;arXiv\u0026quot;,\\n   eprint = {1804.02156},\\n primaryClass = \u0026quot;cs.RO\u0026quot;,\\n keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},\\n     year = 2018,\\n    month = apr,\\n   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402156T},\\n  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\",\"name\":\"OpenSeqSLAM2\",\"type\":\"code\",\"url\":\"https://github.com/qcr/openseqslam2\",\"src\":\"/content/visual_place_recognition/openseqslam2.md\",\"id\":\"openseqslam2\",\"image_position\":\"center\",\"image\":\"/_next/static/images/openseqslam2-e3f1e85549e34343dc852ed782b11093.png\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"openseqslam2"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>