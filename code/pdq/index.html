<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">Probability-based Detection Quality (PDQ)</span><a href="https://github.com/david2611/pdq_evaluation" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">david2611/pdq_evaluation</span><span class="markdown-body mdc-typography--body1"><div><p><a href="https://qcr.github.io"><img src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt="QUT Centre for Robotics Open Source"></a></p>
<h1>Probability-based Detection Quality (PDQ)</h1>
<p>This repository contains the implementation of the probability-based detection quality (PDQ) evaluation measure.
This enables <strong>quantitative</strong> analysis of the <strong>spatial and semantic uncertainties</strong> output by a probabilistic object detecttion (PrOD) system.
This repository provides tools for analysing PrOD detections and classical detections using mAP, moLRP, and PDQ (note that PDQ results will be low for a classical detector and mAP and moLRP scores will likely be low for PrOD detections).
Evaluation can be performed both on COCO formatted data and on RVC1 (PrOD challenge) formatted data.
The repository also provides visualization tools to enable fine-grained analysis of PDQ results as shown below.</p>
<p><img src="/_next/static/images/PDQ_Examples-360b2ea2f2877553dc4e8ef94bb13bde.jpg" alt="PrOD evaluation visualization image examples"></p>
<p>The code here, particularly for evaluating RVC1 data is based heavily on the PrOD challenge code which can be found
here: https://github.com/jskinn/rvchallenge-evaluation</p>
<p>Note that some extra funcitonality for PDQ outside of what is reported in the original paper and challenge is also provided such as evaluating results using the bounding boxes of the ground-truth segmentation masks, probabilistic segmentation evaluation, a greedy alternative to PDQ.</p>
<p>For further details on the robotic vision challenges please see the following links for more details:</p>
<ul>
<li>Robotic Vision Challenges Homepage: http://roboticvisionchallenge.org/</li>
<li>PrOD Main Challenge Page: https://competitions.codalab.org/competitions/20597</li>
<li>PrOD Continuous Challenge Page: https://competitions.codalab.org/competitions/20595</li>
</ul>
<h1>Citing PDQ</h1>
<p>If you are using PDQ in your research, please cite the paper below:</p>
<pre class="language-none"><code class="language-none">@inproceedings{hall2020probabilistic,
  title={Probabilistic object detection: Definition and evaluation},
  author={Hall, David and Dayoub, Feras and Skinner, John and Zhang, Haoyang and Miller, Dimity and Corke, Peter and Carneiro, Gustavo and Angelova, Anelia and S{\&quot;u}nderhauf, Niko},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={1031--1040},
  year={2020}
}
</code></pre>
<h1>Setup</h1>
<h2>Install all python requirements</h2>
<p>This code comes with a requirements.txt file.
Make sure you have installed all libraries as part of your working environment.</p>
<h2>Install COCO mAP API</h2>
<p>After installing all requirements, you will need to have a fully installed implementation of the COCO API located
somewhere on your machine.
You can download this API here https://github.com/cocodataset/cocoapi.</p>
<p>Once this is downloaded and installed, you need to adjust the system path on line 11 of coco_mAP.py and line 16 of
read_files.py to match the PythonAPI folder of your COCO API installation.</p>
<h2>Add LRP Evaluation Code</h2>
<p>You will also require code for using LRP evaluation measures.
To do this you need to simply copy the cocoevalLRP.py file from the LRP github repository to the pycocotools folder within the PythonAPI.
You can download the specific file here https://github.com/cancam/LRP/blob/master/cocoLRPapi-master/PythonAPI/pycocotools/cocoevalLRP.py
You can clone the original repository here https://github.com/cancam/LRP.</p>
<p>After cocoevalLRP.py is located in your pycocotools folder, simply adjust the system path on line 11 of coco_LRP.py to match your PythonAPI folder.</p>
<h1>Usage</h1>
<p>All evaluation code is run on detections saved in .json files formatted as required by the RVC outlined later on.
A variation to this is also available for probabilistic segmentation format also described later.
If you are evaluating on COCO data and have saved detections in COCO format, you can convert to RVC1 format using
<em>file_convert-coco_to_rvc1.py</em>
When you have the appropriate files, you can evaluate on mAP, moLRP, and PDQ with <em>evaluate.py</em>.
After evaluation is complete, you can visualise your detections for a sequence of images w.r.t. PDQ using
<em>visualise_pdq_analysis.py</em></p>
<p>Evaluation is currently organised so that you can evaluate either on COCO data, or on RVC1 data. Note that RVC1 data
expects multiple sequences rather than a single folder of data.</p>
<h2>RVC1 Detection Format</h2>
<p>RVC1 detections are saved in a single .json file per sequence being evaluated. Each .json file is formatted as follows:</p>
<pre class="language-none"><code class="language-none">{
  &quot;classes&quot;: [&lt;an ordered list of class names&gt;],
  &quot;detections&quot;: [
    [
      {
        &quot;bbox&quot;: [x1, y1, x2, y2],
        &quot;covars&quot;: [
          [[xx1, xy1],[xy1, yy1]],
          [[xx2, xy2],[xy2, yy2]]
        ],
        &quot;label_probs&quot;: [&lt;an ordered list of probabilities for each class&gt;]
      },
      {
      }
    ],
    [],
    []
    ...
  ]
}
</code></pre>
<h3>Important Notes</h3>
<p>The two covariance matrices in <code class="language-none">covars</code> need to be positive semi-definite in order for the code to work. A covariance matrix <code class="language-none">C</code> is positive semi-definite when its eigenvalues are not negative. You can easily check this condition in python with the following function:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">is_pos_semidefinite</span><span class="token punctuation">(</span>C<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>eigvals<span class="token punctuation">(</span>C<span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">)</span>
</code></pre>
<h3>Probabilistic Segmentation Detections</h3>
<p>We now accommodate a way to submit probabilistic segmentation detections.
For this format, a .npy file for each image stores all detection probabilistic segmentation heatmaps for that image.
This 3D array's shape is m x h x w where m is the number of segmentation masks, h is the image height, and w is the
image width.
Each detection dictionary now contains the location the .npy file associated with the detection and the mask id for the
specific detection.
You may also define a bounding box to replace the probabilistic segmentation for bounding-box detections and define a
chosen class to use for mAP and moLRP evaluation (rather than always using max class of label_probs).</p>
<p>Expected format for probabilistic segmentation detection files is as follows:</p>
<pre class="language-none"><code class="language-none">{
  &quot;classes&quot;: [&lt;an ordered list of class names&gt;],
  &quot;detections&quot;: [
    [
      {
        &quot;label_probs&quot;: [&lt;an ordered list of probabilities for each class&gt;],
        &quot;masks_file&quot;: &quot;&lt;location of .npy file holding probabilistic segmentation mask&gt;&quot;,
        &quot;mask_id&quot;: &lt;index of this detection's mask in mask_file's numpy array&gt;,
        &quot;label&quot;: &lt;chosen label within label_probs&gt; (optional),
        &quot;bbox&quot;: [x1, y1, x2, y2] (optional for use in mAP and moLRP),
      },
      {
      }
    ],
    [],
    []
    ...
  ]
}
</code></pre>
<h2>file_convert_coco_to_rvc1.py</h2>
<p>To convert coco detections to rvc format simply run:</p>
<p><code class="language-none">python file_convert_coco_to_rvc1.py --coco_gt &lt;gt_json_file&gt; --coco_det &lt;det_json_file&gt; --rvc1_det &lt;output_json_file&gt;</code></p>
<p>where <code class="language-none">&lt;gt_json_file&gt;</code> is the coco format ground-truth json filename, <code class="language-none">det_json_file</code> is the coco format detection
json filename, and <code class="language-none">output_file</code> is the json filename you will save your rvc1 formatted detections json file.</p>
<h3>Important Notes</h3>
<p>By default, coco json format does not come with the predicted scores for all the classes available, in which case the conversion script will just
extract the score of the chosen class and distribute remaining probability across all others classes. However, this will produce
incorrect measures of label quality because it is the probability estimated by the detector for the object's ground-truth class, which might not
correspond to the chosen class. To facilitate correct measurements, if a detection element in the coco json file (<code class="language-none">det_json_file</code>) comes with a
key <code class="language-none">all_scores</code>, the conversion script will consider it as an array of all the scores, and use it instead of the default behaviour.</p>
<p>Also, by default, coco json format does not consider the existence of a covariance matrix which is needed for PDQ calculations. The conversion
script assigns by default a zero'ed covariance matrix, but if a detection element in the coco json file (<code class="language-none">det_json_file</code>) comes with a
key <code class="language-none">covars</code>, the conversion script will use that covariance matrix instead of the default one with zeros. Please refer to the previous section <code class="language-none">RVC1 Detection Format</code> for further information on how <code class="language-none">covars</code> should be formatted in the json file.</p>
<h2>evaluate.py</h2>
<p>To perform full evaluation simply run:</p>
<p><code class="language-none">python evaluate.py --test_set &lt;test_type&gt; --gt_loc &lt;gt_location&gt; --det_loc &lt;det_location&gt; --save_folder &lt;save_folder&gt; --set_cov &lt;cov&gt; --num_workers &lt;num_workers&gt;</code></p>
<p>Optional flags for new functionality include <code class="language-none">--bbox_gt</code>, <code class="language-none">--segment_mode</code>, <code class="language-none">--greedy_mode</code>, and <code class="language-none">--prob_seg</code>.
There is also an <code class="language-none">--mAP_heatmap</code> flag but that should not generally be used.</p>
<ul>
<li>
<p><code class="language-none">&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>
</li>
<li>
<p><code class="language-none">&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of
ground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code class="language-none">&lt;test_type&gt;</code></p>
</li>
<li>
<p><code class="language-none">&lt;det_loc&gt;</code> is a string defining either the location of a detection .json file (coco data) or a folder of .json files for
multiple sequences (rvc1 data). Which one it is interpreted as is defined by <code class="language-none">&lt;test_type&gt;</code>.
Note that these detection files must be in rvc1 format.</p>
</li>
<li>
<p><code class="language-none">&lt;save_folder&gt;</code> is a string defining the folder where analysis will be stored in form of scores.txt, and files for visualisations</p>
</li>
<li>
<p><code class="language-none">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of detections.</p>
</li>
<li>
<p><code class="language-none">--bbox_gt</code> flag states that all ground-truth should be teated as bounding boxes for PDQ analysis.
All pixels within the bounding box will be used for analysis and there will be no &quot;ignored&quot; pixels. This enables
use of datasets with no segmentation information provided they are stored in COCO ground-truth format.</p>
</li>
<li>
<p><code class="language-none">--segment_mode</code> flag states that evaluation is performed per-pixel on the ground-truth segments with no &quot;ignored&quot;
pixels to accommodate box-shaped detections. This should only be used if evaluating a probabilistic segmentation
detection system.</p>
</li>
<li>
<p><code class="language-none">--greedy_mode</code> flag states that assignment of detections to ground-truth objects based upon pPDQ scores is done
greedily rather than optimal assignment. Greedy mode can be faster for some applications but does not match &quot;official&quot;
PDQ process and there may be some minuscule difference in score/behaviour.</p>
</li>
<li>
<p><code class="language-none">--prob_seg</code> flag states that detection.json file is formatted for probabilistic segmentation detections as outlined
above.</p>
</li>
<li>
<p><code class="language-none">--mAP_heatmap</code> flag should not generally be used but enables mAP/moLRP evaluation to be based not upon corners
defined by PBox/BBox detections, but that encompass all pixels of the detection above given threshold of probability
(0.0027).</p>
</li>
<li>
<p><code class="language-none">--num_workers</code> number of parallel worker processes to use in the CPU when making the calculations for the PDQ score. By default, this value is 6.</p>
</li>
</ul>
<p>For further details, please consult the code.</p>
<h3>Important Notes</h3>
<p>For consistency reasons, unlike the original rvc1 evaluation code, we do not multiply PDQ by 100 to provide it as a percentage.
PDQ is also labelled as &quot;PDQ&quot; in scores.txt rather than simply &quot;score&quot;.</p>
<p>For anyone unfamiliar with moLRP based measures, these values are losses and not qualities like all other provided measures.
To transform these results from losses to qualities simply take 1 - moLRP.</p>
<p>Newly implemented modes <code class="language-none">--segment_mode</code>, <code class="language-none">--bbox_gt</code>, <code class="language-none">greedy_mode</code> are not used for the RVC1 challenge but can be
useful for developing research in probabilistic segmentation, when your dataset does not have a segmentation mask, or
when time is critical, respectively.</p>
<h2>visualise_pdq_analysis.py</h2>
<p>To create visualisations for probabilistic detections and PDQ analysis on a single sequence of images run:</p>
<p><code class="language-none">python visualise_pdq_analysis.py --data_type &lt;test_type&gt; --ground_truth &lt;gt_location&gt; --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --gt_analysis &lt;gt_analysis_file&gt; --det_analysis &lt;det_analysis_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --colour_mode &lt;colour_mode&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt; --full_info</code></p>
<p>where:</p>
<ul>
<li>
<p><code class="language-none">&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>
</li>
<li>
<p><code class="language-none">&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of
ground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code class="language-none">&lt;test_type&gt;</code></p>
</li>
<li>
<p><code class="language-none">&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>
</li>
<li>
<p><code class="language-none">&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>
</li>
<li>
<p><code class="language-none">&lt;gt_analysis&gt;</code> a string defining the ground-truth analysis .json file matching the sequence to be visualised.
Must also correspond to the detection .json file being visualised.</p>
</li>
<li>
<p><code class="language-none">&lt;det_analysis&gt;</code> a string defining the detection analysis .json file matching the sequence to be visualised.
Must also correspond to the detection .json file being visualised.</p>
</li>
<li>
<p><code class="language-none">&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code class="language-none">&lt;gt_imgs_location&gt;</code></p>
</li>
<li>
<p><code class="language-none">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections. <strong>This must match the set covariance used in evaluate.py</strong></p>
</li>
<li>
<p><code class="language-none">&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>
</li>
<li>
<p><code class="language-none">&lt;colour_mode&gt;</code> is a string defining whether correct and incorrect results are coloured green and red ('gr') or blue and orange ('bo') respectively.
Default option is blue and orange.</p>
</li>
<li>
<p><code class="language-none">&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').
Ellipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.
Arrows show 2 x standard deviation along the major axes of the Gaussian.
Default option is 'ellipse'</p>
</li>
<li>
<p><code class="language-none">&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.
For example <code class="language-none">--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for &quot;cat.jpg&quot;, &quot;dog.jpg&quot;, and &quot;whale.jpg&quot;.</p>
</li>
<li>
<p><code class="language-none">--full_info</code> is an optional flag defining whether full pairwise quality analysis should be written for TP detections. <strong>Recommended setting for in-depth analysis</strong></p>
</li>
</ul>
<p>For further details, please consult the code.</p>
<h3>Important Notes</h3>
<p>Consistency must be kept between ground-truth analysis, detection analysis, and detection .json files in order to provide meaningful visualisation.</p>
<p>If the evaluation which produced the ground-truth analysis and detection analysis used a set covariance input, you must
provide that same set covariance when generating visualisations.</p>
<p>New modes such as using probabilistic segmentation detections (<code class="language-none">--prob_seg</code>) in segment mode (<code class="language-none">--segment_mode</code>)
or using bounding_box ground-truth (<code class="language-none">--bbox_gt</code>) in the evaluation code are <strong>NOT</strong> yet supported.</p>
<h2>visualise_prob_detections.py</h2>
<p>To create visualisations for probabilistic detections on a single sequence of images run:</p>
<p><code class="language-none">python visualise_prob_detections.py --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt;</code></p>
<p>where:</p>
<ul>
<li>
<p><code class="language-none">&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>
</li>
<li>
<p><code class="language-none">&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>
</li>
<li>
<p><code class="language-none">&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code class="language-none">&lt;gt_imgs_location&gt;</code></p>
</li>
<li>
<p><code class="language-none">&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections.</p>
</li>
<li>
<p><code class="language-none">&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>
</li>
<li>
<p><code class="language-none">&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').
Ellipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.
Arrows show 2 x standard deviation along the major axes of the Gaussian.
Default option is 'ellipse'</p>
</li>
<li>
<p><code class="language-none">&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.
For example <code class="language-none">--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for &quot;cat.jpg&quot;, &quot;dog.jpg&quot;, and &quot;whale.jpg&quot;.</p>
</li>
</ul>
<p>For further details, please consult the code.</p>
<h3>Important Notes</h3>
<p>Order of detections in detections.json file must match the order of the images as stored in the ground-truth images
folder.</p>
<p>New modes such as using probabilistic segmentation detections (<code class="language-none">--prob_seg</code>) in the evaluation code are
<strong>NOT</strong> yet supported.</p>
<h1>Acknowledgements</h1>
<p>Development of the probability-based detection quality evaluation measure was directly supported by:</p>
<p><img src="/_next/static/images/acrv_logo_small-e816f01e0557cf5cee1e9eb709d9a5e5.png" alt="Australian Centre for Robotic Vision"></p>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003cp\u003e\u003ca href=\\\"https://qcr.github.io\\\"\u003e\u003cimg src=https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg alt=\\\"QUT Centre for Robotics Open Source\\\"\u003e\u003c/a\u003e\u003c/p\u003e\\n\u003ch1\u003eProbability-based Detection Quality (PDQ)\u003c/h1\u003e\\n\u003cp\u003eThis repository contains the implementation of the probability-based detection quality (PDQ) evaluation measure.\\nThis enables \u003cstrong\u003equantitative\u003c/strong\u003e analysis of the \u003cstrong\u003espatial and semantic uncertainties\u003c/strong\u003e output by a probabilistic object detecttion (PrOD) system.\\nThis repository provides tools for analysing PrOD detections and classical detections using mAP, moLRP, and PDQ (note that PDQ results will be low for a classical detector and mAP and moLRP scores will likely be low for PrOD detections).\\nEvaluation can be performed both on COCO formatted data and on RVC1 (PrOD challenge) formatted data.\\nThe repository also provides visualization tools to enable fine-grained analysis of PDQ results as shown below.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/PDQ_Examples-360b2ea2f2877553dc4e8ef94bb13bde.jpg\\\" alt=\\\"PrOD evaluation visualization image examples\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eThe code here, particularly for evaluating RVC1 data is based heavily on the PrOD challenge code which can be found\\nhere: https://github.com/jskinn/rvchallenge-evaluation\u003c/p\u003e\\n\u003cp\u003eNote that some extra funcitonality for PDQ outside of what is reported in the original paper and challenge is also provided such as evaluating results using the bounding boxes of the ground-truth segmentation masks, probabilistic segmentation evaluation, a greedy alternative to PDQ.\u003c/p\u003e\\n\u003cp\u003eFor further details on the robotic vision challenges please see the following links for more details:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eRobotic Vision Challenges Homepage: http://roboticvisionchallenge.org/\u003c/li\u003e\\n\u003cli\u003ePrOD Main Challenge Page: https://competitions.codalab.org/competitions/20597\u003c/li\u003e\\n\u003cli\u003ePrOD Continuous Challenge Page: https://competitions.codalab.org/competitions/20595\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch1\u003eCiting PDQ\u003c/h1\u003e\\n\u003cp\u003eIf you are using PDQ in your research, please cite the paper below:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@inproceedings{hall2020probabilistic,\\n  title={Probabilistic object detection: Definition and evaluation},\\n  author={Hall, David and Dayoub, Feras and Skinner, John and Zhang, Haoyang and Miller, Dimity and Corke, Peter and Carneiro, Gustavo and Angelova, Anelia and S{\\\\\u0026quot;u}nderhauf, Niko},\\n  booktitle={The IEEE Winter Conference on Applications of Computer Vision},\\n  pages={1031--1040},\\n  year={2020}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch1\u003eSetup\u003c/h1\u003e\\n\u003ch2\u003eInstall all python requirements\u003c/h2\u003e\\n\u003cp\u003eThis code comes with a requirements.txt file.\\nMake sure you have installed all libraries as part of your working environment.\u003c/p\u003e\\n\u003ch2\u003eInstall COCO mAP API\u003c/h2\u003e\\n\u003cp\u003eAfter installing all requirements, you will need to have a fully installed implementation of the COCO API located\\nsomewhere on your machine.\\nYou can download this API here https://github.com/cocodataset/cocoapi.\u003c/p\u003e\\n\u003cp\u003eOnce this is downloaded and installed, you need to adjust the system path on line 11 of coco_mAP.py and line 16 of\\nread_files.py to match the PythonAPI folder of your COCO API installation.\u003c/p\u003e\\n\u003ch2\u003eAdd LRP Evaluation Code\u003c/h2\u003e\\n\u003cp\u003eYou will also require code for using LRP evaluation measures.\\nTo do this you need to simply copy the cocoevalLRP.py file from the LRP github repository to the pycocotools folder within the PythonAPI.\\nYou can download the specific file here https://github.com/cancam/LRP/blob/master/cocoLRPapi-master/PythonAPI/pycocotools/cocoevalLRP.py\\nYou can clone the original repository here https://github.com/cancam/LRP.\u003c/p\u003e\\n\u003cp\u003eAfter cocoevalLRP.py is located in your pycocotools folder, simply adjust the system path on line 11 of coco_LRP.py to match your PythonAPI folder.\u003c/p\u003e\\n\u003ch1\u003eUsage\u003c/h1\u003e\\n\u003cp\u003eAll evaluation code is run on detections saved in .json files formatted as required by the RVC outlined later on.\\nA variation to this is also available for probabilistic segmentation format also described later.\\nIf you are evaluating on COCO data and have saved detections in COCO format, you can convert to RVC1 format using\\n\u003cem\u003efile_convert-coco_to_rvc1.py\u003c/em\u003e\\nWhen you have the appropriate files, you can evaluate on mAP, moLRP, and PDQ with \u003cem\u003eevaluate.py\u003c/em\u003e.\\nAfter evaluation is complete, you can visualise your detections for a sequence of images w.r.t. PDQ using\\n\u003cem\u003evisualise_pdq_analysis.py\u003c/em\u003e\u003c/p\u003e\\n\u003cp\u003eEvaluation is currently organised so that you can evaluate either on COCO data, or on RVC1 data. Note that RVC1 data\\nexpects multiple sequences rather than a single folder of data.\u003c/p\u003e\\n\u003ch2\u003eRVC1 Detection Format\u003c/h2\u003e\\n\u003cp\u003eRVC1 detections are saved in a single .json file per sequence being evaluated. Each .json file is formatted as follows:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e{\\n  \u0026quot;classes\u0026quot;: [\u0026lt;an ordered list of class names\u0026gt;],\\n  \u0026quot;detections\u0026quot;: [\\n    [\\n      {\\n        \u0026quot;bbox\u0026quot;: [x1, y1, x2, y2],\\n        \u0026quot;covars\u0026quot;: [\\n          [[xx1, xy1],[xy1, yy1]],\\n          [[xx2, xy2],[xy2, yy2]]\\n        ],\\n        \u0026quot;label_probs\u0026quot;: [\u0026lt;an ordered list of probabilities for each class\u0026gt;]\\n      },\\n      {\\n      }\\n    ],\\n    [],\\n    []\\n    ...\\n  ]\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3\u003eImportant Notes\u003c/h3\u003e\\n\u003cp\u003eThe two covariance matrices in \u003ccode class=\\\"language-none\\\"\u003ecovars\u003c/code\u003e need to be positive semi-definite in order for the code to work. A covariance matrix \u003ccode class=\\\"language-none\\\"\u003eC\u003c/code\u003e is positive semi-definite when its eigenvalues are not negative. You can easily check this condition in python with the following function:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003edef\u003c/span\u003e \u003cspan class=\\\"token function\\\"\u003eis_pos_semidefinite\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eC\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n    \u003cspan class=\\\"token keyword\\\"\u003ereturn\u003c/span\u003e np\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003e\u003cspan class=\\\"token builtin\\\"\u003eall\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003enp\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003elinalg\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eeigvals\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eC\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e \u003cspan class=\\\"token operator\\\"\u003e\u003e=\u003c/span\u003e \u003cspan class=\\\"token number\\\"\u003e0\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3\u003eProbabilistic Segmentation Detections\u003c/h3\u003e\\n\u003cp\u003eWe now accommodate a way to submit probabilistic segmentation detections.\\nFor this format, a .npy file for each image stores all detection probabilistic segmentation heatmaps for that image.\\nThis 3D array's shape is m x h x w where m is the number of segmentation masks, h is the image height, and w is the\\nimage width.\\nEach detection dictionary now contains the location the .npy file associated with the detection and the mask id for the\\nspecific detection.\\nYou may also define a bounding box to replace the probabilistic segmentation for bounding-box detections and define a\\nchosen class to use for mAP and moLRP evaluation (rather than always using max class of label_probs).\u003c/p\u003e\\n\u003cp\u003eExpected format for probabilistic segmentation detection files is as follows:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e{\\n  \u0026quot;classes\u0026quot;: [\u0026lt;an ordered list of class names\u0026gt;],\\n  \u0026quot;detections\u0026quot;: [\\n    [\\n      {\\n        \u0026quot;label_probs\u0026quot;: [\u0026lt;an ordered list of probabilities for each class\u0026gt;],\\n        \u0026quot;masks_file\u0026quot;: \u0026quot;\u0026lt;location of .npy file holding probabilistic segmentation mask\u0026gt;\u0026quot;,\\n        \u0026quot;mask_id\u0026quot;: \u0026lt;index of this detection's mask in mask_file's numpy array\u0026gt;,\\n        \u0026quot;label\u0026quot;: \u0026lt;chosen label within label_probs\u0026gt; (optional),\\n        \u0026quot;bbox\u0026quot;: [x1, y1, x2, y2] (optional for use in mAP and moLRP),\\n      },\\n      {\\n      }\\n    ],\\n    [],\\n    []\\n    ...\\n  ]\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2\u003efile_convert_coco_to_rvc1.py\u003c/h2\u003e\\n\u003cp\u003eTo convert coco detections to rvc format simply run:\u003c/p\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003epython file_convert_coco_to_rvc1.py --coco_gt \u0026lt;gt_json_file\u0026gt; --coco_det \u0026lt;det_json_file\u0026gt; --rvc1_det \u0026lt;output_json_file\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003cp\u003ewhere \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_json_file\u0026gt;\u003c/code\u003e is the coco format ground-truth json filename, \u003ccode class=\\\"language-none\\\"\u003edet_json_file\u003c/code\u003e is the coco format detection\\njson filename, and \u003ccode class=\\\"language-none\\\"\u003eoutput_file\u003c/code\u003e is the json filename you will save your rvc1 formatted detections json file.\u003c/p\u003e\\n\u003ch3\u003eImportant Notes\u003c/h3\u003e\\n\u003cp\u003eBy default, coco json format does not come with the predicted scores for all the classes available, in which case the conversion script will just\\nextract the score of the chosen class and distribute remaining probability across all others classes. However, this will produce\\nincorrect measures of label quality because it is the probability estimated by the detector for the object's ground-truth class, which might not\\ncorrespond to the chosen class. To facilitate correct measurements, if a detection element in the coco json file (\u003ccode class=\\\"language-none\\\"\u003edet_json_file\u003c/code\u003e) comes with a\\nkey \u003ccode class=\\\"language-none\\\"\u003eall_scores\u003c/code\u003e, the conversion script will consider it as an array of all the scores, and use it instead of the default behaviour.\u003c/p\u003e\\n\u003cp\u003eAlso, by default, coco json format does not consider the existence of a covariance matrix which is needed for PDQ calculations. The conversion\\nscript assigns by default a zero'ed covariance matrix, but if a detection element in the coco json file (\u003ccode class=\\\"language-none\\\"\u003edet_json_file\u003c/code\u003e) comes with a\\nkey \u003ccode class=\\\"language-none\\\"\u003ecovars\u003c/code\u003e, the conversion script will use that covariance matrix instead of the default one with zeros. Please refer to the previous section \u003ccode class=\\\"language-none\\\"\u003eRVC1 Detection Format\u003c/code\u003e for further information on how \u003ccode class=\\\"language-none\\\"\u003ecovars\u003c/code\u003e should be formatted in the json file.\u003c/p\u003e\\n\u003ch2\u003eevaluate.py\u003c/h2\u003e\\n\u003cp\u003eTo perform full evaluation simply run:\u003c/p\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003epython evaluate.py --test_set \u0026lt;test_type\u0026gt; --gt_loc \u0026lt;gt_location\u0026gt; --det_loc \u0026lt;det_location\u0026gt; --save_folder \u0026lt;save_folder\u0026gt; --set_cov \u0026lt;cov\u0026gt; --num_workers \u0026lt;num_workers\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003cp\u003eOptional flags for new functionality include \u003ccode class=\\\"language-none\\\"\u003e--bbox_gt\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003e--segment_mode\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003e--greedy_mode\u003c/code\u003e, and \u003ccode class=\\\"language-none\\\"\u003e--prob_seg\u003c/code\u003e.\\nThere is also an \u003ccode class=\\\"language-none\\\"\u003e--mAP_heatmap\u003c/code\u003e flag but that should not generally be used.\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;test_type\u0026gt;\u003c/code\u003e is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_location\u0026gt;\u003c/code\u003e is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;test_type\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;det_loc\u0026gt;\u003c/code\u003e is a string defining either the location of a detection .json file (coco data) or a folder of .json files for\\nmultiple sequences (rvc1 data). Which one it is interpreted as is defined by \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;test_type\u0026gt;\u003c/code\u003e.\\nNote that these detection files must be in rvc1 format.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;save_folder\u0026gt;\u003c/code\u003e is a string defining the folder where analysis will be stored in form of scores.txt, and files for visualisations\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;cov\u0026gt;\u003c/code\u003e is an optional value defining set covariance for the corners of detections.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--bbox_gt\u003c/code\u003e flag states that all ground-truth should be teated as bounding boxes for PDQ analysis.\\nAll pixels within the bounding box will be used for analysis and there will be no \u0026quot;ignored\u0026quot; pixels. This enables\\nuse of datasets with no segmentation information provided they are stored in COCO ground-truth format.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--segment_mode\u003c/code\u003e flag states that evaluation is performed per-pixel on the ground-truth segments with no \u0026quot;ignored\u0026quot;\\npixels to accommodate box-shaped detections. This should only be used if evaluating a probabilistic segmentation\\ndetection system.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--greedy_mode\u003c/code\u003e flag states that assignment of detections to ground-truth objects based upon pPDQ scores is done\\ngreedily rather than optimal assignment. Greedy mode can be faster for some applications but does not match \u0026quot;official\u0026quot;\\nPDQ process and there may be some minuscule difference in score/behaviour.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--prob_seg\u003c/code\u003e flag states that detection.json file is formatted for probabilistic segmentation detections as outlined\\nabove.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--mAP_heatmap\u003c/code\u003e flag should not generally be used but enables mAP/moLRP evaluation to be based not upon corners\\ndefined by PBox/BBox detections, but that encompass all pixels of the detection above given threshold of probability\\n(0.0027).\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--num_workers\u003c/code\u003e number of parallel worker processes to use in the CPU when making the calculations for the PDQ score. By default, this value is 6.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eFor further details, please consult the code.\u003c/p\u003e\\n\u003ch3\u003eImportant Notes\u003c/h3\u003e\\n\u003cp\u003eFor consistency reasons, unlike the original rvc1 evaluation code, we do not multiply PDQ by 100 to provide it as a percentage.\\nPDQ is also labelled as \u0026quot;PDQ\u0026quot; in scores.txt rather than simply \u0026quot;score\u0026quot;.\u003c/p\u003e\\n\u003cp\u003eFor anyone unfamiliar with moLRP based measures, these values are losses and not qualities like all other provided measures.\\nTo transform these results from losses to qualities simply take 1 - moLRP.\u003c/p\u003e\\n\u003cp\u003eNewly implemented modes \u003ccode class=\\\"language-none\\\"\u003e--segment_mode\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003e--bbox_gt\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003egreedy_mode\u003c/code\u003e are not used for the RVC1 challenge but can be\\nuseful for developing research in probabilistic segmentation, when your dataset does not have a segmentation mask, or\\nwhen time is critical, respectively.\u003c/p\u003e\\n\u003ch2\u003evisualise_pdq_analysis.py\u003c/h2\u003e\\n\u003cp\u003eTo create visualisations for probabilistic detections and PDQ analysis on a single sequence of images run:\u003c/p\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003epython visualise_pdq_analysis.py --data_type \u0026lt;test_type\u0026gt; --ground_truth \u0026lt;gt_location\u0026gt; --gt_img_folder \u0026lt;gt_imgs_location\u0026gt; --det_json \u0026lt;det_json_file\u0026gt; --gt_analysis \u0026lt;gt_analysis_file\u0026gt; --det_analysis \u0026lt;det_analysis_file\u0026gt; --save_folder \u0026lt;save_folder_location\u0026gt; --set_cov \u0026lt;cov\u0026gt; --img_type \u0026lt;ext\u0026gt; --colour_mode \u0026lt;colour_mode\u0026gt; --corner_mode \u0026lt;corner_mode\u0026gt; --img_set \u0026lt;list_of_img_names\u0026gt; --full_info\u003c/code\u003e\u003c/p\u003e\\n\u003cp\u003ewhere:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;test_type\u0026gt;\u003c/code\u003e is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_location\u0026gt;\u003c/code\u003e is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;test_type\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_imgs_location\u0026gt;\u003c/code\u003e a string defining the folder where ground-truth images for the sequence are stored.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;det_json_file\u0026gt;\u003c/code\u003e a string defining the detection .json file matching the sequence to be visualised\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_analysis\u0026gt;\u003c/code\u003e a string defining the ground-truth analysis .json file matching the sequence to be visualised.\\nMust also correspond to the detection .json file being visualised.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;det_analysis\u0026gt;\u003c/code\u003e a string defining the detection analysis .json file matching the sequence to be visualised.\\nMust also correspond to the detection .json file being visualised.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;save_folder_location\u0026gt;\u003c/code\u003e a string defining the folder where image visualisations will be saved. Must be different to the \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_imgs_location\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;cov\u0026gt;\u003c/code\u003e is an optional value defining set covariance for the corners of the detections. \u003cstrong\u003eThis must match the set covariance used in evaluate.py\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;img_type\u0026gt;\u003c/code\u003e is a string defining what image type the ground-truth is provided in. For example 'jpg'.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;colour_mode\u0026gt;\u003c/code\u003e is a string defining whether correct and incorrect results are coloured green and red ('gr') or blue and orange ('bo') respectively.\\nDefault option is blue and orange.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;corner_mode\u0026gt;\u003c/code\u003e is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\\nArrows show 2 x standard deviation along the major axes of the Gaussian.\\nDefault option is 'ellipse'\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;list_of_img_names\u0026gt;\u003c/code\u003e is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\\nFor example \u003ccode class=\\\"language-none\\\"\u003e--img_set cat.jpg dog.jpg whale.jpg\u003c/code\u003e would only draw visualisations for \u0026quot;cat.jpg\u0026quot;, \u0026quot;dog.jpg\u0026quot;, and \u0026quot;whale.jpg\u0026quot;.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e--full_info\u003c/code\u003e is an optional flag defining whether full pairwise quality analysis should be written for TP detections. \u003cstrong\u003eRecommended setting for in-depth analysis\u003c/strong\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eFor further details, please consult the code.\u003c/p\u003e\\n\u003ch3\u003eImportant Notes\u003c/h3\u003e\\n\u003cp\u003eConsistency must be kept between ground-truth analysis, detection analysis, and detection .json files in order to provide meaningful visualisation.\u003c/p\u003e\\n\u003cp\u003eIf the evaluation which produced the ground-truth analysis and detection analysis used a set covariance input, you must\\nprovide that same set covariance when generating visualisations.\u003c/p\u003e\\n\u003cp\u003eNew modes such as using probabilistic segmentation detections (\u003ccode class=\\\"language-none\\\"\u003e--prob_seg\u003c/code\u003e) in segment mode (\u003ccode class=\\\"language-none\\\"\u003e--segment_mode\u003c/code\u003e)\\nor using bounding_box ground-truth (\u003ccode class=\\\"language-none\\\"\u003e--bbox_gt\u003c/code\u003e) in the evaluation code are \u003cstrong\u003eNOT\u003c/strong\u003e yet supported.\u003c/p\u003e\\n\u003ch2\u003evisualise_prob_detections.py\u003c/h2\u003e\\n\u003cp\u003eTo create visualisations for probabilistic detections on a single sequence of images run:\u003c/p\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003epython visualise_prob_detections.py --gt_img_folder \u0026lt;gt_imgs_location\u0026gt; --det_json \u0026lt;det_json_file\u0026gt; --save_folder \u0026lt;save_folder_location\u0026gt; --set_cov \u0026lt;cov\u0026gt; --img_type \u0026lt;ext\u0026gt; --corner_mode \u0026lt;corner_mode\u0026gt; --img_set \u0026lt;list_of_img_names\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003cp\u003ewhere:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_imgs_location\u0026gt;\u003c/code\u003e a string defining the folder where ground-truth images for the sequence are stored.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;det_json_file\u0026gt;\u003c/code\u003e a string defining the detection .json file matching the sequence to be visualised\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;save_folder_location\u0026gt;\u003c/code\u003e a string defining the folder where image visualisations will be saved. Must be different to the \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;gt_imgs_location\u0026gt;\u003c/code\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;cov\u0026gt;\u003c/code\u003e is an optional value defining set covariance for the corners of the detections.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;img_type\u0026gt;\u003c/code\u003e is a string defining what image type the ground-truth is provided in. For example 'jpg'.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;corner_mode\u0026gt;\u003c/code\u003e is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\\nArrows show 2 x standard deviation along the major axes of the Gaussian.\\nDefault option is 'ellipse'\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003ccode class=\\\"language-none\\\"\u003e\u0026lt;list_of_img_names\u0026gt;\u003c/code\u003e is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\\nFor example \u003ccode class=\\\"language-none\\\"\u003e--img_set cat.jpg dog.jpg whale.jpg\u003c/code\u003e would only draw visualisations for \u0026quot;cat.jpg\u0026quot;, \u0026quot;dog.jpg\u0026quot;, and \u0026quot;whale.jpg\u0026quot;.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eFor further details, please consult the code.\u003c/p\u003e\\n\u003ch3\u003eImportant Notes\u003c/h3\u003e\\n\u003cp\u003eOrder of detections in detections.json file must match the order of the images as stored in the ground-truth images\\nfolder.\u003c/p\u003e\\n\u003cp\u003eNew modes such as using probabilistic segmentation detections (\u003ccode class=\\\"language-none\\\"\u003e--prob_seg\u003c/code\u003e) in the evaluation code are\\n\u003cstrong\u003eNOT\u003c/strong\u003e yet supported.\u003c/p\u003e\\n\u003ch1\u003eAcknowledgements\u003c/h1\u003e\\n\u003cp\u003eDevelopment of the probability-based detection quality evaluation measure was directly supported by:\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/acrv_logo_small-e816f01e0557cf5cee1e9eb709d9a5e5.png\\\" alt=\\\"Australian Centre for Robotic Vision\\\"\u003e\u003c/p\u003e\\n\",\"name\":\"Probability-based Detection Quality (PDQ)\",\"type\":\"code\",\"url\":\"https://github.com/david2611/pdq_evaluation\",\"image\":\"/_next/static/images/qcr_web_img-1fefd82c6dfb30c8aeeb9c4fbfbe7253.jpg\",\"image_fit\":\"contain\",\"src\":\"/content/pdq.md\",\"id\":\"pdq\",\"image_position\":\"center\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"pdq"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>