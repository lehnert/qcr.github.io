<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">LoST-X</span><a href="https://github.com/oravus/lostX" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+CjxzdmcKICAgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIgogICB4bWxuczpjYz0iaHR0cDovL2NyZWF0aXZlY29tbW9ucy5vcmcvbnMjIgogICB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiCiAgIHhtbG5zOnN2Zz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciCiAgIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIKICAgeG1sbnM6c29kaXBvZGk9Imh0dHA6Ly9zb2RpcG9kaS5zb3VyY2Vmb3JnZS5uZXQvRFREL3NvZGlwb2RpLTAuZHRkIgogICB4bWxuczppbmtzY2FwZT0iaHR0cDovL3d3dy5pbmtzY2FwZS5vcmcvbmFtZXNwYWNlcy9pbmtzY2FwZSIKICAgc29kaXBvZGk6ZG9jbmFtZT0iZ2l0aHViLnN2ZyIKICAgaW5rc2NhcGU6dmVyc2lvbj0iMS4wICgxLjArcjczKzEpIgogICBpZD0ic3ZnMTM4MyIKICAgdmVyc2lvbj0iMS4xIgogICB2aWV3Qm94PSIwIDAgMTEuNDkzMTQ3IDExLjIwOTQ2NyIKICAgaGVpZ2h0PSIxMS4yMDk0NjdtbSIKICAgd2lkdGg9IjExLjQ5MzE0N21tIj4KICA8ZGVmcwogICAgIGlkPSJkZWZzMTM3NyIgLz4KICA8c29kaXBvZGk6bmFtZWR2aWV3CiAgICAgaW5rc2NhcGU6d2luZG93LW1heGltaXplZD0iMSIKICAgICBpbmtzY2FwZTp3aW5kb3cteT0iMzYyIgogICAgIGlua3NjYXBlOndpbmRvdy14PSIwIgogICAgIGlua3NjYXBlOndpbmRvdy1oZWlnaHQ9IjEwNTIiCiAgICAgaW5rc2NhcGU6d2luZG93LXdpZHRoPSIxOTIwIgogICAgIHNob3dncmlkPSJmYWxzZSIKICAgICBpbmtzY2FwZTpkb2N1bWVudC1yb3RhdGlvbj0iMCIKICAgICBpbmtzY2FwZTpjdXJyZW50LWxheWVyPSJsYXllcjEiCiAgICAgaW5rc2NhcGU6ZG9jdW1lbnQtdW5pdHM9Im1tIgogICAgIGlua3NjYXBlOmN5PSIxMjYuMjk1MTUiCiAgICAgaW5rc2NhcGU6Y3g9IjE4Ljk2OTk1MSIKICAgICBpbmtzY2FwZTp6b29tPSIwLjk4ODg0NzEiCiAgICAgaW5rc2NhcGU6cGFnZXNoYWRvdz0iMiIKICAgICBpbmtzY2FwZTpwYWdlb3BhY2l0eT0iMC4wIgogICAgIGJvcmRlcm9wYWNpdHk9IjEuMCIKICAgICBib3JkZXJjb2xvcj0iIzY2NjY2NiIKICAgICBwYWdlY29sb3I9IiNmZmZmZmYiCiAgICAgaWQ9ImJhc2UiIC8+CiAgPG1ldGFkYXRhCiAgICAgaWQ9Im1ldGFkYXRhMTM4MCI+CiAgICA8cmRmOlJERj4KICAgICAgPGNjOldvcmsKICAgICAgICAgcmRmOmFib3V0PSIiPgogICAgICAgIDxkYzpmb3JtYXQ+aW1hZ2Uvc3ZnK3htbDwvZGM6Zm9ybWF0PgogICAgICAgIDxkYzp0eXBlCiAgICAgICAgICAgcmRmOnJlc291cmNlPSJodHRwOi8vcHVybC5vcmcvZGMvZGNtaXR5cGUvU3RpbGxJbWFnZSIgLz4KICAgICAgICA8ZGM6dGl0bGU+PC9kYzp0aXRsZT4KICAgICAgPC9jYzpXb3JrPgogICAgPC9yZGY6UkRGPgogIDwvbWV0YWRhdGE+CiAgPGcKICAgICB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMTAxLjMyMzAzLC05OC4yMTQ5NTkpIgogICAgIGlkPSJsYXllcjEiCiAgICAgaW5rc2NhcGU6Z3JvdXBtb2RlPSJsYXllciIKICAgICBpbmtzY2FwZTpsYWJlbD0iTGF5ZXIgMSI+CiAgICA8ZwogICAgICAgdHJhbnNmb3JtPSJtYXRyaXgoMC4zNTI3Nzc3NywwLDAsLTAuMzUyNzc3NzcsMTA3LjA2OTA3LDk4LjIxNDk1OSkiCiAgICAgICBpZD0iZzIyIj4KICAgICAgPHBhdGgKICAgICAgICAgaWQ9InBhdGgyNCIKICAgICAgICAgc3R5bGU9ImZpbGw6IzFiMTgxNztmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6ZXZlbm9kZDtzdHJva2U6bm9uZSIKICAgICAgICAgZD0ibSAwLDAgYyAtOC45OTUsMCAtMTYuMjg4LC03LjI5MyAtMTYuMjg4LC0xNi4yOSAwLC03LjE5NyA0LjY2NywtMTMuMzAyIDExLjE0LC0xNS40NTcgMC44MTUsLTAuMTQ5IDEuMTEyLDAuMzU0IDEuMTEyLDAuNzg2IDAsMC4zODYgLTAuMDE0LDEuNDExIC0wLjAyMiwyLjc3IC00LjUzMSwtMC45ODQgLTUuNDg3LDIuMTg0IC01LjQ4NywyLjE4NCAtMC43NDEsMS44ODEgLTEuODA5LDIuMzgyIC0xLjgwOSwyLjM4MiAtMS40NzksMS4wMTEgMC4xMTIsMC45OTEgMC4xMTIsMC45OTEgMS42MzUsLTAuMTE2IDIuNDk1LC0xLjY3OSAyLjQ5NSwtMS42NzkgMS40NTMsLTIuNDg5IDMuODEzLC0xLjc3IDQuNzQxLC0xLjM1NCAwLjE0OCwxLjA1MyAwLjU2OCwxLjc3MSAxLjAzNCwyLjE3OCAtMy42MTcsMC40MTEgLTcuNDIsMS44MDkgLTcuNDIsOC4wNTEgMCwxLjc3OCAwLjYzNSwzLjIzMiAxLjY3Nyw0LjM3MSAtMC4xNjgsMC40MTIgLTAuNzI3LDIuMDY4IDAuMTU5LDQuMzExIDAsMCAxLjM2OCwwLjQzOCA0LjQ4LC0xLjY3IDEuMjk5LDAuMzYxIDIuNjkzLDAuNTQyIDQuMDc4LDAuNTQ4IDEuMzgzLC0wLjAwNiAyLjc3NywtMC4xODcgNC4wNzgsLTAuNTQ4IDMuMTEsMi4xMDggNC40NzUsMS42NyA0LjQ3NSwxLjY3IDAuODg5LC0yLjI0MyAwLjMzLC0zLjg5OSAwLjE2MiwtNC4zMTEgMS4wNDQsLTEuMTM5IDEuNjc1LC0yLjU5MyAxLjY3NSwtNC4zNzEgMCwtNi4yNTggLTMuODA5LC03LjYzNSAtNy40MzgsLTguMDM4IDAuNTg1LC0wLjUwMyAxLjEwNiwtMS40OTcgMS4xMDYsLTMuMDE3IDAsLTIuMTc3IC0wLjAyLC0zLjkzNCAtMC4wMiwtNC40NjggMCwtMC40MzYgMC4yOTMsLTAuOTQzIDEuMTIsLTAuNzg0IDYuNDY4LDIuMTU5IDExLjEzMSw4LjI2IDExLjEzMSwxNS40NTUgQyAxNi4yOTEsLTcuMjkzIDguOTk3LDAgMCwwIiAvPgogICAgPC9nPgogIDwvZz4KPC9zdmc+Cg==)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">oravus/lostX</span><span class="markdown-body mdc-typography--body1"><div><h1>LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics</h1>
<p>This is the source code for the paper titled - &quot;LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics&quot;, pre-print available <a href="https://arxiv.org/abs/1804.05526">here</a>.</p>
<p>An example output image showing Keypoint Correspondences:</p>
<p><img src="/_next/static/images/day-night-keypoint-correspondence-place-recognition-591f10ce5848add43b1423bfc16eafe6.jpg" alt="An example output image showing Keypoint Correspondences" title="Keypoint Correspondences using LoST-X"></p>
<p>Flowchart of the proposed approach:</p>
<p><img src="/_next/static/images/LoST-Flowchart-Visual_Place_Recognition-5bb346cb2e424ba34d9ac4118afe3800.jpg" alt="Flowchart of the proposed approach" title="Flowchart for the proposed approach - LoST-X"></p>
<p>If you find this work useful, please cite it as:<br>
Sourav Garg, Niko Sunderhauf, and Michael Milford. LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics. Proceedings of Robotics: Science and Systems XIV, 2018.<br>
bibtex:</p>
<pre class="language-none"><code class="language-none">@article{garg2018lost,
title={LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics},
author={Garg, Sourav and Suenderhauf, Niko and Milford, Michael},
journal={Proceedings of Robotics: Science and Systems XIV},
year={2018}
}
</code></pre>
<p>RefineNet's citation as mentioned on their <a href=https://github.com/guosheng/refinenet>Github page</a>.</p>
<h2>Setup and Run</h2>
<h4>Dependencies</h4>
<ul>
<li>Ubuntu        (Tested on <em>14.04</em>)</li>
<li><a href="https://arxiv.org/abs/1611.06612">RefineNet</a>
<ul>
<li>Required primarily for visual semantic information. Convolutional feature maps based dense descriptors are also extracted from the same.</li>
<li>A <a href=https://github.com/oravus/refinenet>modified fork</a> of RefineNet's <a href=https://github.com/guosheng/refinenet>code</a> is used in this work to simultaneously store convolutional dense descriptors.</li>
<li>Requires Matlab      (Tested on <em>2017a</em>)</li>
</ul>
</li>
<li>Python        (Tested on <em>2.7</em>)
<ul>
<li>numpy       (Tested on <em>1.11.1</em>, <em>1.14.2</em>)</li>
<li>scipy       (Tested on <em>0.13.3</em>, <em>0.17.1</em>)</li>
<li>skimage     (Minimum Required <em>0.13.1</em>)</li>
<li>sklearn     (Tested on <em>0.14.1</em>, <em>0.19.1</em>)</li>
<li>h5py        (Tested on <em>2.7.1</em>)</li>
</ul>
</li>
<li>Docker (optional, recommended, tested on <em>17.12.0-ce</em>)
<ul>
<li><a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">Official page for install instructions</a></li>
</ul>
</li>
</ul>
<h4>Download</h4>
<ol>
<li>In your workspace, clone the repositories:<pre class="language-none"><code class="language-none">git clone https://github.com/oravus/lostX.git
cd lostX
git clone https://github.com/oravus/refinenet.git
</code></pre>
NOTE: If you download this repository as a zip, the refineNet's fork will not get downloaded automatically, being a git submodule.</li>
<li>Download the Resnet-101 model pre-trained on Cityscapes dataset from <a href="https://drive.google.com/drive/folders/1U2c1N6QJdzB_8HBgXb7mJ6Qk66JDBHI9">here</a> or <a href="https://pan.baidu.com/s/1nxf2muP#list/path=%2Frefinenet_public_new%2Frefinenet_released%2Frefinenet_res101&amp;parentPath=%2Frefinenet_public_new%2Frefinenet_released">here</a>. More details on RefineNet's <a href=https://github.com/guosheng/refinenet>Github page</a>.
<ul>
<li>Place the downloaded model's <code class="language-none">.mat</code> file in the <code class="language-none">refinenet/model_trained/</code> directory.</li>
</ul>
</li>
<li>If you are using docker, download the docker image:<pre class="language-none"><code class="language-none">docker pull souravgarg/vpr-lost-kc:v1
</code></pre>
</li>
</ol>
<h4>Run</h4>
<ol>
<li>
<p>Generate and store semantic labels and dense convolutional descriptors from RefineNet's <em>conv5</em> layer
In the MATLAB workspace, from the <code class="language-none">refinenet/main/</code> directory, run:</p>
<pre class="language-none"><code class="language-none">demo_predict_mscale_cityscapes
</code></pre>
<p>The above will use the sample dataset from <code class="language-none">refinenet/datasets/</code> directory. You can set path to your data in <code class="language-none">demo_predict_mscale_cityscapes.m</code> through variable <code class="language-none">datasetName</code> and <code class="language-none">img_data_dir</code>.<br>
You might have to run <code class="language-none">vl_compilenn</code> before running the demo, please refer to the instructions for running refinenet in their official <a href=https://github.com/guosheng/refinenet>Readme.md</a></p>
</li>
<li>
<p>[For Docker users]<br>
If you have an environment with python and other dependencies installed, skip this step, otherwise run a docker container:</p>
<pre class="language-none"><code class="language-none">docker run -it -v PATH_TO_YOUR_HOME_DIRECTORY/:/workspace/ souravgarg/vpr-lost-kc:v1 /bin/bash
</code></pre>
<p>From within the docker container, navigate to <code class="language-none">lostX/lost_kc/</code> repository.<br>
<code class="language-none">-v</code> option mounts the <em>PATH_TO_YOUR_HOME_DIRECTORY</em> to <em>/workspace</em> directory within the docker container.</p>
</li>
<li>
<p>Reformat and pre-process RefineNet's output from <code class="language-none">lostX/lost_kc/</code> directory:</p>
<pre class="language-none"><code class="language-none">python reformat_data.py -p $PATH_TO_REFINENET_OUTPUT
</code></pre>
<p>$PATH_TO_REFINENET_OUTPUT is set to be the parent directory of <code class="language-none">predict_result_full</code>, for example, <em>../refinenet/cache_data/test_examples_cityscapes/1-s_result_20180427152622_predict_custom_data/predict_result_1/</em></p>
</li>
<li>
<p>Compute LoST descriptor:</p>
<pre class="language-none"><code class="language-none">python LoST.py -p $PATH_TO_REFINENET_OUTPUT 
</code></pre>
</li>
<li>
<p>Repeat step 1, 3, and 4 to generate output for the other dataset by setting the variable <code class="language-none">datasetName</code> to <code class="language-none">2-s</code>.</p>
</li>
<li>
<p>Perform place matching using LoST descriptors based difference matrix and Keypoint Correspondences:</p>
<pre class="language-none"><code class="language-none">python match_lost_kc.py -n 10 -f 0 -p1 $PATH_TO_REFINENET_OUTPUT_1  -p2 $PATH_TO_REFINENET_OUTPUT_2
</code></pre>
</li>
</ol>
<p>Note: Run <code class="language-none">python FILENAME -h</code> for any of the python source files in Step 3, 4, and 6 for description of arguments passed to those files.</p>
<h2>License</h2>
<p>The code is released under MIT License.</p>
<h2>Related Projects</h2>
<p><a href=https://github.com/oravus/DeltaDescriptors>Delta Descriptors (2020)</a></p>
<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>
<p><a href=https://github.com/oravus/seq2single>seq2single (2019)</a></p>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003ch1\u003eLoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics\u003c/h1\u003e\\n\u003cp\u003eThis is the source code for the paper titled - \u0026quot;LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics\u0026quot;, pre-print available \u003ca href=\\\"https://arxiv.org/abs/1804.05526\\\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eAn example output image showing Keypoint Correspondences:\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/day-night-keypoint-correspondence-place-recognition-591f10ce5848add43b1423bfc16eafe6.jpg\\\" alt=\\\"An example output image showing Keypoint Correspondences\\\" title=\\\"Keypoint Correspondences using LoST-X\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eFlowchart of the proposed approach:\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/LoST-Flowchart-Visual_Place_Recognition-5bb346cb2e424ba34d9ac4118afe3800.jpg\\\" alt=\\\"Flowchart of the proposed approach\\\" title=\\\"Flowchart for the proposed approach - LoST-X\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eIf you find this work useful, please cite it as:\u003cbr\u003e\\nSourav Garg, Niko Sunderhauf, and Michael Milford. LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics. Proceedings of Robotics: Science and Systems XIV, 2018.\u003cbr\u003e\\nbibtex:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@article{garg2018lost,\\ntitle={LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics},\\nauthor={Garg, Sourav and Suenderhauf, Niko and Milford, Michael},\\njournal={Proceedings of Robotics: Science and Systems XIV},\\nyear={2018}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eRefineNet's citation as mentioned on their \u003ca href=https://github.com/guosheng/refinenet\u003eGithub page\u003c/a\u003e.\u003c/p\u003e\\n\u003ch2\u003eSetup and Run\u003c/h2\u003e\\n\u003ch4\u003eDependencies\u003c/h4\u003e\\n\u003cul\u003e\\n\u003cli\u003eUbuntu        (Tested on \u003cem\u003e14.04\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003e\u003ca href=\\\"https://arxiv.org/abs/1611.06612\\\"\u003eRefineNet\u003c/a\u003e\\n\u003cul\u003e\\n\u003cli\u003eRequired primarily for visual semantic information. Convolutional feature maps based dense descriptors are also extracted from the same.\u003c/li\u003e\\n\u003cli\u003eA \u003ca href=https://github.com/oravus/refinenet\u003emodified fork\u003c/a\u003e of RefineNet's \u003ca href=https://github.com/guosheng/refinenet\u003ecode\u003c/a\u003e is used in this work to simultaneously store convolutional dense descriptors.\u003c/li\u003e\\n\u003cli\u003eRequires Matlab      (Tested on \u003cem\u003e2017a\u003c/em\u003e)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003ePython        (Tested on \u003cem\u003e2.7\u003c/em\u003e)\\n\u003cul\u003e\\n\u003cli\u003enumpy       (Tested on \u003cem\u003e1.11.1\u003c/em\u003e, \u003cem\u003e1.14.2\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003escipy       (Tested on \u003cem\u003e0.13.3\u003c/em\u003e, \u003cem\u003e0.17.1\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003eskimage     (Minimum Required \u003cem\u003e0.13.1\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003esklearn     (Tested on \u003cem\u003e0.14.1\u003c/em\u003e, \u003cem\u003e0.19.1\u003c/em\u003e)\u003c/li\u003e\\n\u003cli\u003eh5py        (Tested on \u003cem\u003e2.7.1\u003c/em\u003e)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003eDocker (optional, recommended, tested on \u003cem\u003e17.12.0-ce\u003c/em\u003e)\\n\u003cul\u003e\\n\u003cli\u003e\u003ca href=\\\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\\\"\u003eOfficial page for install instructions\u003c/a\u003e\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch4\u003eDownload\u003c/h4\u003e\\n\u003col\u003e\\n\u003cli\u003eIn your workspace, clone the repositories:\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003egit clone https://github.com/oravus/lostX.git\\ncd lostX\\ngit clone https://github.com/oravus/refinenet.git\\n\u003c/code\u003e\u003c/pre\u003e\\nNOTE: If you download this repository as a zip, the refineNet's fork will not get downloaded automatically, being a git submodule.\u003c/li\u003e\\n\u003cli\u003eDownload the Resnet-101 model pre-trained on Cityscapes dataset from \u003ca href=\\\"https://drive.google.com/drive/folders/1U2c1N6QJdzB_8HBgXb7mJ6Qk66JDBHI9\\\"\u003ehere\u003c/a\u003e or \u003ca href=\\\"https://pan.baidu.com/s/1nxf2muP#list/path=%2Frefinenet_public_new%2Frefinenet_released%2Frefinenet_res101\u0026amp;parentPath=%2Frefinenet_public_new%2Frefinenet_released\\\"\u003ehere\u003c/a\u003e. More details on RefineNet's \u003ca href=https://github.com/guosheng/refinenet\u003eGithub page\u003c/a\u003e.\\n\u003cul\u003e\\n\u003cli\u003ePlace the downloaded model's \u003ccode class=\\\"language-none\\\"\u003e.mat\u003c/code\u003e file in the \u003ccode class=\\\"language-none\\\"\u003erefinenet/model_trained/\u003c/code\u003e directory.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003c/li\u003e\\n\u003cli\u003eIf you are using docker, download the docker image:\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003edocker pull souravgarg/vpr-lost-kc:v1\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003ch4\u003eRun\u003c/h4\u003e\\n\u003col\u003e\\n\u003cli\u003e\\n\u003cp\u003eGenerate and store semantic labels and dense convolutional descriptors from RefineNet's \u003cem\u003econv5\u003c/em\u003e layer\\nIn the MATLAB workspace, from the \u003ccode class=\\\"language-none\\\"\u003erefinenet/main/\u003c/code\u003e directory, run:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003edemo_predict_mscale_cityscapes\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe above will use the sample dataset from \u003ccode class=\\\"language-none\\\"\u003erefinenet/datasets/\u003c/code\u003e directory. You can set path to your data in \u003ccode class=\\\"language-none\\\"\u003edemo_predict_mscale_cityscapes.m\u003c/code\u003e through variable \u003ccode class=\\\"language-none\\\"\u003edatasetName\u003c/code\u003e and \u003ccode class=\\\"language-none\\\"\u003eimg_data_dir\u003c/code\u003e.\u003cbr\u003e\\nYou might have to run \u003ccode class=\\\"language-none\\\"\u003evl_compilenn\u003c/code\u003e before running the demo, please refer to the instructions for running refinenet in their official \u003ca href=https://github.com/guosheng/refinenet\u003eReadme.md\u003c/a\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e[For Docker users]\u003cbr\u003e\\nIf you have an environment with python and other dependencies installed, skip this step, otherwise run a docker container:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003edocker run -it -v PATH_TO_YOUR_HOME_DIRECTORY/:/workspace/ souravgarg/vpr-lost-kc:v1 /bin/bash\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eFrom within the docker container, navigate to \u003ccode class=\\\"language-none\\\"\u003elostX/lost_kc/\u003c/code\u003e repository.\u003cbr\u003e\\n\u003ccode class=\\\"language-none\\\"\u003e-v\u003c/code\u003e option mounts the \u003cem\u003ePATH_TO_YOUR_HOME_DIRECTORY\u003c/em\u003e to \u003cem\u003e/workspace\u003c/em\u003e directory within the docker container.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003eReformat and pre-process RefineNet's output from \u003ccode class=\\\"language-none\\\"\u003elostX/lost_kc/\u003c/code\u003e directory:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003epython reformat_data.py -p $PATH_TO_REFINENET_OUTPUT\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e$PATH_TO_REFINENET_OUTPUT is set to be the parent directory of \u003ccode class=\\\"language-none\\\"\u003epredict_result_full\u003c/code\u003e, for example, \u003cem\u003e../refinenet/cache_data/test_examples_cityscapes/1-s_result_20180427152622_predict_custom_data/predict_result_1/\u003c/em\u003e\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003eCompute LoST descriptor:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003epython LoST.py -p $PATH_TO_REFINENET_OUTPUT \\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003eRepeat step 1, 3, and 4 to generate output for the other dataset by setting the variable \u003ccode class=\\\"language-none\\\"\u003edatasetName\u003c/code\u003e to \u003ccode class=\\\"language-none\\\"\u003e2-s\u003c/code\u003e.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003ePerform place matching using LoST descriptors based difference matrix and Keypoint Correspondences:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003epython match_lost_kc.py -n 10 -f 0 -p1 $PATH_TO_REFINENET_OUTPUT_1  -p2 $PATH_TO_REFINENET_OUTPUT_2\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003cp\u003eNote: Run \u003ccode class=\\\"language-none\\\"\u003epython FILENAME -h\u003c/code\u003e for any of the python source files in Step 3, 4, and 6 for description of arguments passed to those files.\u003c/p\u003e\\n\u003ch2\u003eLicense\u003c/h2\u003e\\n\u003cp\u003eThe code is released under MIT License.\u003c/p\u003e\\n\u003ch2\u003eRelated Projects\u003c/h2\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/DeltaDescriptors\u003eDelta Descriptors (2020)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/CoarseHash\u003eCoarseHash (2020)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/seq2single\u003eseq2single (2019)\u003c/a\u003e\u003c/p\u003e\\n\",\"name\":\"LoST-X\",\"type\":\"code\",\"url\":\"https://github.com/oravus/lostX\",\"src\":\"/content/visual_place_recognition/lost.md\",\"id\":\"lost\",\"image_position\":\"center\",\"image\":\"/_next/static/images/day-night-keypoint-correspondence-place-recognition-591f10ce5848add43b1423bfc16eafe6.jpg\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"lost"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>