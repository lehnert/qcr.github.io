<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">Delta Descriptors</span><a href="https://github.com/oravus/DeltaDescriptors" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">oravus/DeltaDescriptors</span><span class="markdown-body mdc-typography--body1"><div><h1>Delta Descriptors</h1>
<p>Source code for the paper - &quot;Delta Descriptors: Change-Based Place Representation for Robust Visual Localization&quot;, published in IEEE Robotics and Automation Letters (RA-L) 2020 and to be presented at IROS 2020. [<a href="https://arxiv.org/abs/2006.05700">arXiv</a>] [<a href="https://ieeexplore.ieee.org/document/9128035">IEEE Xplore</a>][<a href="https://www.youtube.com/watch?v=qY4VobAoLPY">YouTube</a>]</p>
<p>We propose Delta Descriptor, defined as a high-dimensional signed vector of change measured across the places observed along a route. Using a difference-based description, places can be effectively recognized despite significant appearance variations.
<img src="/_next/static/images/ral-iros-2020-delta-descriptors-schematic-f941eee94161c1b37bce3429b2adadc3.png" alt="Schematic of the proposed approach" title="Schematic of the proposed approach - Delta Descriptors">
Images on the left are from the <a href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford Robotcar</a> dataset.</p>
<h2>Requirements</h2>
<pre class="language-none"><code class="language-none">matplotlib==2.0.2
numpy==1.15.2
tqdm==4.29.1
scipy==1.1.0
scikit_learn==0.23.1
</code></pre>
<p>See <code class="language-none">requirements.txt</code>, generated using <code class="language-none">pipreqs==0.4.10</code> and <code class="language-none">python3.5.6</code></p>
<h2>Usage</h2>
<h4>Download this Repository and the Nordland dataset (part)</h4>
<p>The dataset used in our paper is available <a href="https://zenodo.org/record/4016653#.X1WmYM8zZCV">here</a> (or use commands as below). Note that the download only comprises a small part (~1 GB) of the original Nordland videos released <a href="https://nrkbeta.no/2013/01/15/nordlandsbanen-minute-by-minute-season-by-season/">here</a>. These videos were first used for visual place recognition in <a href="https://www.tu-chemnitz.de/etit/proaut/publications/openseqslam.pdf">this</a> paper.</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">git</span> clone https://github.com/oravus/DeltaDescriptors.git
<span class="token builtin class-name">cd</span> DeltaDescriptors/
<span class="token function">mkdir</span> data/
<span class="token builtin class-name">cd</span> data/
<span class="token function">wget</span> https://zenodo.org/record/4016653/files/nordland-part-2020.zip
<span class="token function">unzip</span> nordland-part-2020.zip
</code></pre>
<p>The zip contains two folders: summer and winter, where each one of them comprises 1750 images which were used for experiments conducted in our paper.</p>
<h4>Describe and Match</h4>
<p>Delta Descriptors are defined on top of global image descriptors, for example, NetVLAD (<a href=https://github.com/oravus/DeltaDescriptors/tree/master/thirdparty>Update 05 Sep 2020: see our python wrapper</a>). Given such descriptors, compute Delta Descriptors and match across two traverses as below:</p>
<pre class="language-shell"><code class="language-shell">python src/main.py --genDesc --genMatch -l <span class="token number">16</span> -d delta -ip1 <span class="token operator">&lt;</span>full_path_of_desc.npy<span class="token operator">></span> -ip2 <span class="token operator">&lt;</span>full_path_of_query_desc.npy<span class="token operator">></span>
</code></pre>
<p>The input descriptor data is assumed to be a 2D tensor of shape <code class="language-none">[numImages,numDescDims]</code>. The computed descriptors are stored in <code class="language-none">.npy</code> format and the match results are stored in <code class="language-none">.npz</code> format comprising a dict of two arrays: <code class="language-none">matchInds</code> (matched reference index per query image) and <code class="language-none">matchDists</code> (corresponding distance value). By default, output is stored in the <code class="language-none">./out</code> folder but can also be specified via <code class="language-none">--outPath</code> argument. To see all the options, use:</p>
<pre class="language-shell"><code class="language-shell">python src/main.py --help
</code></pre>
<p>The options <code class="language-none">--genDesc</code> and <code class="language-none">--genMatch</code> can be used in isolation or together, see example usage below.</p>
<h4>Describe only</h4>
<p>In order to compute only the descriptors for a single traverse, use:</p>
<pre class="language-shell"><code class="language-shell">python src/main.py --genDesc -l <span class="token number">16</span> -d delta -ip1 <span class="token operator">&lt;</span>full_path_of_desc.npy<span class="token operator">></span>
</code></pre>
<h4>Match only</h4>
<p>For only computing matches, given the descriptors (Delta or some other), use:</p>
<pre class="language-shell"><code class="language-shell">python src/main.py --genMatch -ip1 <span class="token operator">&lt;</span>full_path_of_desc.npy<span class="token operator">></span> -ip2 <span class="token operator">&lt;</span>full_path_of_query_desc.npy<span class="token operator">></span>
</code></pre>
<h4>Evaluate only</h4>
<pre class="language-shell"><code class="language-shell">python src/main.py --eval -mop <span class="token operator">&lt;</span>full_path_of_match_output.npz<span class="token operator">></span>
</code></pre>
<p>or evaluate directly with <code class="language-none">--genMatch</code> (and possibly <code class="language-none">--genDesc</code>) flag:</p>
<pre class="language-shell"><code class="language-shell">python src/main.py --eval --genMatch -ip1 <span class="token operator">&lt;</span>full_path_of_desc.npy<span class="token operator">></span> -ip2 <span class="token operator">&lt;</span>full_path_of_query_desc.npy<span class="token operator">></span>
</code></pre>
<p>Currently, only Nordland dataset-style (1-to-1 frame correspondence) evaluation is supported, GPS/INS coordinates-based evaluation, for example, for Oxford Robotcar dataset to be added soon. Evalution code can be used to generate PR curves and the code in its current form prints Precision @ 100% Recall for localization radius of 1, 5, 10 and 20 (frames).</p>
<h2>Citation</h2>
<p>If you find this code or our work useful, cite it as below:</p>
<pre class="language-none"><code class="language-none">@article{garg2020delta,
  title={Delta Descriptors: Change-Based Place Representation for Robust Visual Localization},
  author={Garg, Sourav and Harwood, Ben and Anand, Gaurangi and Milford, Michael},
  journal={IEEE Robotics and Automation Letters},
  year={2020},
  publisher={IEEE},
  volume={5},
  number={4},
  pages={5120-5127},  
}
</code></pre>
<h2>License</h2>
<p>The code is released under MIT License.</p>
<h2>Related Projects</h2>
<p><a href=https://github.com/oravus/CoarseHash>CoarseHash (2020)</a></p>
<p><a href=https://github.com/oravus/seq2single>seq2single (2019)</a></p>
<p><a href=https://github.com/oravus/lostX>LoST (2018)</a></p>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003ch1\u003eDelta Descriptors\u003c/h1\u003e\\n\u003cp\u003eSource code for the paper - \u0026quot;Delta Descriptors: Change-Based Place Representation for Robust Visual Localization\u0026quot;, published in IEEE Robotics and Automation Letters (RA-L) 2020 and to be presented at IROS 2020. [\u003ca href=\\\"https://arxiv.org/abs/2006.05700\\\"\u003earXiv\u003c/a\u003e] [\u003ca href=\\\"https://ieeexplore.ieee.org/document/9128035\\\"\u003eIEEE Xplore\u003c/a\u003e][\u003ca href=\\\"https://www.youtube.com/watch?v=qY4VobAoLPY\\\"\u003eYouTube\u003c/a\u003e]\u003c/p\u003e\\n\u003cp\u003eWe propose Delta Descriptor, defined as a high-dimensional signed vector of change measured across the places observed along a route. Using a difference-based description, places can be effectively recognized despite significant appearance variations.\\n\u003cimg src=\\\"/_next/static/images/ral-iros-2020-delta-descriptors-schematic-f941eee94161c1b37bce3429b2adadc3.png\\\" alt=\\\"Schematic of the proposed approach\\\" title=\\\"Schematic of the proposed approach - Delta Descriptors\\\"\u003e\\nImages on the left are from the \u003ca href=\\\"https://robotcar-dataset.robots.ox.ac.uk/\\\"\u003eOxford Robotcar\u003c/a\u003e dataset.\u003c/p\u003e\\n\u003ch2\u003eRequirements\u003c/h2\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003ematplotlib==2.0.2\\nnumpy==1.15.2\\ntqdm==4.29.1\\nscipy==1.1.0\\nscikit_learn==0.23.1\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eSee \u003ccode class=\\\"language-none\\\"\u003erequirements.txt\u003c/code\u003e, generated using \u003ccode class=\\\"language-none\\\"\u003epipreqs==0.4.10\u003c/code\u003e and \u003ccode class=\\\"language-none\\\"\u003epython3.5.6\u003c/code\u003e\u003c/p\u003e\\n\u003ch2\u003eUsage\u003c/h2\u003e\\n\u003ch4\u003eDownload this Repository and the Nordland dataset (part)\u003c/h4\u003e\\n\u003cp\u003eThe dataset used in our paper is available \u003ca href=\\\"https://zenodo.org/record/4016653#.X1WmYM8zZCV\\\"\u003ehere\u003c/a\u003e (or use commands as below). Note that the download only comprises a small part (~1 GB) of the original Nordland videos released \u003ca href=\\\"https://nrkbeta.no/2013/01/15/nordlandsbanen-minute-by-minute-season-by-season/\\\"\u003ehere\u003c/a\u003e. These videos were first used for visual place recognition in \u003ca href=\\\"https://www.tu-chemnitz.de/etit/proaut/publications/openseqslam.pdf\\\"\u003ethis\u003c/a\u003e paper.\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003e\u003cspan class=\\\"token function\\\"\u003egit\u003c/span\u003e clone https://github.com/oravus/DeltaDescriptors.git\\n\u003cspan class=\\\"token builtin class-name\\\"\u003ecd\u003c/span\u003e DeltaDescriptors/\\n\u003cspan class=\\\"token function\\\"\u003emkdir\u003c/span\u003e data/\\n\u003cspan class=\\\"token builtin class-name\\\"\u003ecd\u003c/span\u003e data/\\n\u003cspan class=\\\"token function\\\"\u003ewget\u003c/span\u003e https://zenodo.org/record/4016653/files/nordland-part-2020.zip\\n\u003cspan class=\\\"token function\\\"\u003eunzip\u003c/span\u003e nordland-part-2020.zip\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe zip contains two folders: summer and winter, where each one of them comprises 1750 images which were used for experiments conducted in our paper.\u003c/p\u003e\\n\u003ch4\u003eDescribe and Match\u003c/h4\u003e\\n\u003cp\u003eDelta Descriptors are defined on top of global image descriptors, for example, NetVLAD (\u003ca href=https://github.com/oravus/DeltaDescriptors/tree/master/thirdparty\u003eUpdate 05 Sep 2020: see our python wrapper\u003c/a\u003e). Given such descriptors, compute Delta Descriptors and match across two traverses as below:\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --genDesc --genMatch -l \u003cspan class=\\\"token number\\\"\u003e16\u003c/span\u003e -d delta -ip1 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e -ip2 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_query_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe input descriptor data is assumed to be a 2D tensor of shape \u003ccode class=\\\"language-none\\\"\u003e[numImages,numDescDims]\u003c/code\u003e. The computed descriptors are stored in \u003ccode class=\\\"language-none\\\"\u003e.npy\u003c/code\u003e format and the match results are stored in \u003ccode class=\\\"language-none\\\"\u003e.npz\u003c/code\u003e format comprising a dict of two arrays: \u003ccode class=\\\"language-none\\\"\u003ematchInds\u003c/code\u003e (matched reference index per query image) and \u003ccode class=\\\"language-none\\\"\u003ematchDists\u003c/code\u003e (corresponding distance value). By default, output is stored in the \u003ccode class=\\\"language-none\\\"\u003e./out\u003c/code\u003e folder but can also be specified via \u003ccode class=\\\"language-none\\\"\u003e--outPath\u003c/code\u003e argument. To see all the options, use:\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --help\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe options \u003ccode class=\\\"language-none\\\"\u003e--genDesc\u003c/code\u003e and \u003ccode class=\\\"language-none\\\"\u003e--genMatch\u003c/code\u003e can be used in isolation or together, see example usage below.\u003c/p\u003e\\n\u003ch4\u003eDescribe only\u003c/h4\u003e\\n\u003cp\u003eIn order to compute only the descriptors for a single traverse, use:\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --genDesc -l \u003cspan class=\\\"token number\\\"\u003e16\u003c/span\u003e -d delta -ip1 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch4\u003eMatch only\u003c/h4\u003e\\n\u003cp\u003eFor only computing matches, given the descriptors (Delta or some other), use:\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --genMatch -ip1 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e -ip2 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_query_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch4\u003eEvaluate only\u003c/h4\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --eval -mop \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_match_output.npz\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eor evaluate directly with \u003ccode class=\\\"language-none\\\"\u003e--genMatch\u003c/code\u003e (and possibly \u003ccode class=\\\"language-none\\\"\u003e--genDesc\u003c/code\u003e) flag:\u003c/p\u003e\\n\u003cpre class=\\\"language-shell\\\"\u003e\u003ccode class=\\\"language-shell\\\"\u003epython src/main.py --eval --genMatch -ip1 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e -ip2 \u003cspan class=\\\"token operator\\\"\u003e\u0026lt;\u003c/span\u003efull_path_of_query_desc.npy\u003cspan class=\\\"token operator\\\"\u003e\u003e\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eCurrently, only Nordland dataset-style (1-to-1 frame correspondence) evaluation is supported, GPS/INS coordinates-based evaluation, for example, for Oxford Robotcar dataset to be added soon. Evalution code can be used to generate PR curves and the code in its current form prints Precision @ 100% Recall for localization radius of 1, 5, 10 and 20 (frames).\u003c/p\u003e\\n\u003ch2\u003eCitation\u003c/h2\u003e\\n\u003cp\u003eIf you find this code or our work useful, cite it as below:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@article{garg2020delta,\\n  title={Delta Descriptors: Change-Based Place Representation for Robust Visual Localization},\\n  author={Garg, Sourav and Harwood, Ben and Anand, Gaurangi and Milford, Michael},\\n  journal={IEEE Robotics and Automation Letters},\\n  year={2020},\\n  publisher={IEEE},\\n  volume={5},\\n  number={4},\\n  pages={5120-5127},  \\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2\u003eLicense\u003c/h2\u003e\\n\u003cp\u003eThe code is released under MIT License.\u003c/p\u003e\\n\u003ch2\u003eRelated Projects\u003c/h2\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/CoarseHash\u003eCoarseHash (2020)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/seq2single\u003eseq2single (2019)\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=https://github.com/oravus/lostX\u003eLoST (2018)\u003c/a\u003e\u003c/p\u003e\\n\",\"name\":\"Delta Descriptors\",\"type\":\"code\",\"url\":\"https://github.com/oravus/DeltaDescriptors\",\"src\":\"/content/visual_place_recognition/delta-descriptors.md\",\"id\":\"delta-descriptors\",\"image_position\":\"center\",\"image\":\"/_next/static/images/ral-iros-2020-delta-descriptors-schematic-f941eee94161c1b37bce3429b2adadc3.png\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"delta-descriptors"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>