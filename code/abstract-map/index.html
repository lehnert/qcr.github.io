<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">Abstract Map (Python)</span><a href="https://github.com/btalb/abstract_map" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">btalb/abstract_map</span><span class="markdown-body mdc-typography--body1"><div><p align=center><strong>~ Please see the <a href="https://btalb.github.io/abstract_map/">abstract map site</a> for further details about the research publication ~</strong></p>
<h1>The Abstract Map - using symbols to navigate</h1>
<p><img src="/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png" alt="The abstract map in action"></p>
<p>This repository provides the implementation of the abstract map used in our <a href="https://doi.org/10.1109/TCDS.2020.2993855">IEEE TCDS journal</a>. The implementation, done in Python, includes the following features:</p>
<ul>
<li>a novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, &amp; collision models)</li>
<li>a visualiser &amp; text-based commentator for introspection of your navigation system (both shown in videos on the <a href="https://btalb.github.io/abstract_map/">repository website</a>)</li>
<li>easy ROS bindings for getting up &amp; running in simulation or on a real robot</li>
<li>tag readers &amp; interpreters for extracting symbolic spatial information from <a href="http://wiki.ros.org/apriltag_ros">AprilTags</a></li>
<li>configuration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see <a href="https://doi.org/10.1109/TCDS.2020.2993855">the paper</a> for further details)</li>
<li>serialisation methods for passing an entire abstract map state between machines, or saving to file</li>
</ul>
<p>Please see our other related repositories for further resources, and related parts of the abstract map studies:</p>
<ul>
<li><strong><a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a>:</strong> all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT</li>
<li><strong><a href=https://github.com/btalb/abstract_map_app>abstract_map_app</a>:</strong> mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags &amp; present the mapped symbolic spatial information in real time)</li>
</ul>
<h2>Getting up &amp; running with the abstract map</h2>
<p><em>Note: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the <a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a> package</em></p>
<h3>Setting up your environment</h3>
<p>Clone the repo &amp; install all Python dependencies:</p>
<pre class="language-none"><code class="language-none">git clone https://github.com/btalb/abstract_map
pip install -r abstract_map/requirements.txt
</code></pre>
<p>Add the new package to your ROS workspace at <code class="language-none">&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>
<pre class="language-none"><code class="language-none">ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/
</code></pre>
<p>Install all of the listed ROS dependencies, and build the package:</p>
<pre class="language-none"><code class="language-none">cd &lt;ROS_WS&gt;/src/
rosdep install abstract_map
cd &lt;ROS_WS&gt;
catkin_make
</code></pre>
<h3>Running the Zoo experiments</h3>
<p>Start the experiment (this will try &amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):</p>
<pre class="language-none"><code class="language-none">roslaunch abstract_map experiment.launch
</code></pre>
<p><em>(please see <a href=https://github.com/btalb/abstract_map_simulator/issues/1>this issue</a> for details if you get the spam of TF based errors... which probably shouldn't even be errors... )</em></p>
<p>In another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:</p>
<pre class="language-none"><code class="language-none">rosrun abstract_map hierarchy_publisher
</code></pre>
<p>This will use the hierarchy available in <code class="language-none">./experiments/zoo_hierarchy.xml</code> by default. Feel free to make your own if you would like to do different experiments.</p>
<p>Start the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):</p>
<pre class="language-none"><code class="language-none">rosrun abstract_map visualiser
</code></pre>
<pre class="language-none"><code class="language-none">rosrun abstract_map visualiser --dark
</code></pre>
<p><img src="/_next/static/images/abstract_map_light_vs_dark-ae93c3e7b8419b56719b5d876dd150f4.png" alt="Visualise the abstract map with dark or light colours"></p>
<p>Finally, start the abstract map with a goal, and watch it attempt to complete the navigation task:</p>
<pre class="language-none"><code class="language-none">roslaunch abstract_map abstract_map.launch goal:=Lion
</code></pre>
<p>If you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in &quot;observe mode&quot;.</p>
<h2>Acknowledgements &amp; Citing our work</h2>
<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href="https://research.qut.edu.au/qcr/">QUT Centre for Robotics</a>.</p>
<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>
<pre class="language-none"><code class="language-none">@ARTICLE{9091567,  
    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  
    journal={IEEE Transactions on Cognitive and Developmental Systems},   
    title={Robot Navigation in Unseen Spaces using an Abstract Map},   
    year={2020},  
    volume={},  
    number={},  
    pages={1-1},
    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},
    doi={10.1109/TCDS.2020.2993855},
    ISSN={2379-8939},
    month={},}
}
</code></pre>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003cp align=center\u003e\u003cstrong\u003e~ Please see the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003eabstract map site\u003c/a\u003e for further details about the research publication ~\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eThe Abstract Map - using symbols to navigate\u003c/h1\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\\\" alt=\\\"The abstract map in action\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eThis repository provides the implementation of the abstract map used in our \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003eIEEE TCDS journal\u003c/a\u003e. The implementation, done in Python, includes the following features:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003ea novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, \u0026amp; collision models)\u003c/li\u003e\\n\u003cli\u003ea visualiser \u0026amp; text-based commentator for introspection of your navigation system (both shown in videos on the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003erepository website\u003c/a\u003e)\u003c/li\u003e\\n\u003cli\u003eeasy ROS bindings for getting up \u0026amp; running in simulation or on a real robot\u003c/li\u003e\\n\u003cli\u003etag readers \u0026amp; interpreters for extracting symbolic spatial information from \u003ca href=\\\"http://wiki.ros.org/apriltag_ros\\\"\u003eAprilTags\u003c/a\u003e\u003c/li\u003e\\n\u003cli\u003econfiguration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003ethe paper\u003c/a\u003e for further details)\u003c/li\u003e\\n\u003cli\u003eserialisation methods for passing an entire abstract map state between machines, or saving to file\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003ePlease see our other related repositories for further resources, and related parts of the abstract map studies:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ca href=https://github.com/btalb/abstract_map_simulator\u003eabstract_map_simulator\u003c/a\u003e:\u003c/strong\u003e all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ca href=https://github.com/btalb/abstract_map_app\u003eabstract_map_app\u003c/a\u003e:\u003c/strong\u003e mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags \u0026amp; present the mapped symbolic spatial information in real time)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eGetting up \u0026amp; running with the abstract map\u003c/h2\u003e\\n\u003cp\u003e\u003cem\u003eNote: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the \u003ca href=https://github.com/btalb/abstract_map_simulator\u003eabstract_map_simulator\u003c/a\u003e package\u003c/em\u003e\u003c/p\u003e\\n\u003ch3\u003eSetting up your environment\u003c/h3\u003e\\n\u003cp\u003eClone the repo \u0026amp; install all Python dependencies:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003egit clone https://github.com/btalb/abstract_map\\npip install -r abstract_map/requirements.txt\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eAdd the new package to your ROS workspace at \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;ROS_WS\u0026gt;/\u003c/code\u003e by linking in the cloned repository:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eln -s \u0026lt;LOCATION_REPO_WAS_CLONED_ABOVE\u0026gt; \u0026lt;ROS_WS\u0026gt;/src/\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eInstall all of the listed ROS dependencies, and build the package:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003ecd \u0026lt;ROS_WS\u0026gt;/src/\\nrosdep install abstract_map\\ncd \u0026lt;ROS_WS\u0026gt;\\ncatkin_make\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3\u003eRunning the Zoo experiments\u003c/h3\u003e\\n\u003cp\u003eStart the experiment (this will try \u0026amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eroslaunch abstract_map experiment.launch\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e\u003cem\u003e(please see \u003ca href=https://github.com/btalb/abstract_map_simulator/issues/1\u003ethis issue\u003c/a\u003e for details if you get the spam of TF based errors... which probably shouldn't even be errors... )\u003c/em\u003e\u003c/p\u003e\\n\u003cp\u003eIn another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map hierarchy_publisher\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThis will use the hierarchy available in \u003ccode class=\\\"language-none\\\"\u003e./experiments/zoo_hierarchy.xml\u003c/code\u003e by default. Feel free to make your own if you would like to do different experiments.\u003c/p\u003e\\n\u003cp\u003eStart the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map visualiser\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map visualiser --dark\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/abstract_map_light_vs_dark-ae93c3e7b8419b56719b5d876dd150f4.png\\\" alt=\\\"Visualise the abstract map with dark or light colours\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eFinally, start the abstract map with a goal, and watch it attempt to complete the navigation task:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eroslaunch abstract_map abstract_map.launch goal:=Lion\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eIf you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in \u0026quot;observe mode\u0026quot;.\u003c/p\u003e\\n\u003ch2\u003eAcknowledgements \u0026amp; Citing our work\u003c/h2\u003e\\n\u003cp\u003eThis work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the \u003ca href=\\\"https://research.qut.edu.au/qcr/\\\"\u003eQUT Centre for Robotics\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eIf you use this software in your research, or for comparisons, please kindly cite our work:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\",\"name\":\"Abstract Map (Python)\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map\",\"src\":\"/content/human_cues/abstract-map.md\",\"id\":\"abstract-map\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"abstract-map"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>