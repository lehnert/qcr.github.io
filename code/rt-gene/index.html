<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">Real-Time Gaze (RT-GENE) and Blink Estimation (RT-BENE)</span><a href="https://github.com/Tobias-Fischer/rt_gene" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">Tobias-Fischer/rt_gene</span><span class="markdown-body mdc-typography--body1"><div><h1>RT-GENE &amp; RT-BENE: Real-Time Eye Gaze and Blink Estimation in Natural Environments</h1>
<p><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg?style=flat-square" alt="License: CC BY-NC-SA 4.0"></a>
<a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md><img src="http://hits.dwyl.io/Tobias-Fischer/rt_gene.svg" alt="HitCount"></a>
<a href=https://github.com/Tobias-Fischer/rt_gene/stargazers><img src="https://img.shields.io/github/stars/Tobias-Fischer/rt_gene.svg?style=flat-square" alt="stars"></a>
<a href=https://github.com/Tobias-Fischer/rt_gene/issues><img src="https://img.shields.io/github/issues/Tobias-Fischer/rt_gene.svg?style=flat-square" alt="GitHub issues"></a>
<a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md><img src="https://img.shields.io/github/repo-size/Tobias-Fischer/rt_gene.svg?style=flat-square" alt="GitHub repo size"></a></p>
<p><a href="https://paperswithcode.com/sota/gaze-estimation-on-mpii-gaze?p=rt-gene-real-time-eye-gaze-estimation-in?style=square"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-mpii-gaze&amp;style=flat-square" alt="PWC"></a>
<a href="https://paperswithcode.com/sota/gaze-estimation-on-rt-gene?p=rt-gene-real-time-eye-gaze-estimation-in"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-rt-gene&amp;style=flat-square" alt="PWC"></a>
<a href="https://paperswithcode.com/sota/gaze-estimation-on-ut-multi-view?p=rt-gene-real-time-eye-gaze-estimation-in"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-ut-multi-view&amp;style=flat-square" alt="PWC"></a></p>
<p><a href="https://paperswithcode.com/sota/blink-estimation-on-eyeblink8?p=rt-bene-a-dataset-and-baselines-for-real-time"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-eyeblink8&amp;style=flat-square" alt="PWC"></a>
<a href="https://paperswithcode.com/sota/blink-estimation-on-researcher-s-night?p=rt-bene-a-dataset-and-baselines-for-real-time"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-researcher-s-night&amp;style=flat-square" alt="PWC"></a>
<a href="https://paperswithcode.com/sota/blink-estimation-on-rt-bene?p=rt-bene-a-dataset-and-baselines-for-real-time"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-rt-bene&amp;style=flat-square" alt="PWC"></a></p>
<p>This repository contains code and dataset references for two papers: RT-GENE (Gaze Estimation; ECCV2018) and RT-BENE (Blink Estimation; ICCV2019 Workshops).</p>
<h2>RT-GENE (Gaze Estimation)</h2>
<h3>License + Attribution</h3>
<p>The RT-GENE code is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>. Commercial usage is not permitted. If you use this dataset or the code in a scientific publication, please cite the following <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html">paper</a>:</p>
<p><img src="/_next/static/images/paper_abstract-f5758078c2e378a46ec9b98b4176e657.jpg" alt="Paper abstract"></p>
<pre class="language-none"><code class="language-none">@inproceedings{FischerECCV2018,
author = {Tobias Fischer and Hyung Jin Chang and Yiannis Demiris},
title = {{RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments}},
booktitle = {European Conference on Computer Vision},
year = {2018},
month = {September},
pages = {339--357}
}
</code></pre>
<p>This work was supported in part by the Samsung Global Research Outreach program, and in part by the EU Horizon 2020 Project PAL (643783-RIA).</p>
<h3>Overview + Accompanying Dataset</h3>
<p>The code is split into four parts, each having its own README contained. There is also an accompanying <a href="https://zenodo.org/record/2529036">dataset</a> <a href="https://goo.gl/tfUaDm">(alternative link)</a> to the code. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: <a href="https://www.imperial.ac.uk/personal-robotics/software/">https://www.imperial.ac.uk/personal-robotics/software/</a>.</p>
<h4>RT-GENE ROS package</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene>rt_gene</a> directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time.</p>
<p align="center">
  <img src="/_next/static/gifs/5b7fb335144a426aa0bf9f3a74f130a6.gif" alt="RT-GENE inference example"/>
</p>
<h4>RT-GENE Standalone Version</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_standalone>rt_gene_standalone</a> directory contains instructions for eye gaze estimation given a set of images. It shares code with the <code class="language-none">rt_gene</code> package (above), in particular the code in <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_gene>rt_gene/src/rt_gene</a>.</p>
<h4>RT-GENE Inpainting</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_inpainting>rt_gene_inpainting</a> directory contains code to inpaint the region covered by the eyetracking glasses.</p>
<p><img src="/_next/static/images/inpaint_example-e34160d4c6d03d7e4eacf7795a7d3e02.jpg" alt="Inpaining example"></p>
<h4>RT-GENE Model Training</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_model_training>rt_gene_model_training</a> directory allows using the inpainted images to train a deep neural network for eye gaze estimation.</p>
<p align="center">
  <img src="/_next/static/images/accuracy_prl-10df02076ab749718e69f8cd75d3aec9.jpg" alt="Accuracy on RT-GENE dataset"/>
</p>
<h2>RT-BENE (Blink Estimation)</h2>
<h3>License + Attribution</h3>
<p>The RT-BENE code is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>. Commercial usage is not permitted. If you use our blink estimation code or dataset, please cite the relevant <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.html">paper</a>:</p>
<pre class="language-none"><code class="language-none">@inproceedings{CortaceroICCV2019W,
author={Kevin Cortacero and Tobias Fischer and Yiannis Demiris},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshops},
title = {RT-BENE: A Dataset and Baselines for Real-Time Blink Estimation in Natural Environments},
year = {2019},
}
</code></pre>
<p>RT-BENE was supported by the EU Horizon 2020 Project PAL (643783-RIA) and a Royal Academy of Engineering Chair in Emerging Technologies to Yiannis Demiris.</p>
<h3>Overview + Accompanying Dataset</h3>
<p>The code is split into several parts, each having its own README. There is also an associated <a href="https://zenodo.org/record/3685316">RT-BENE dataset</a>. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: <a href="https://www.imperial.ac.uk/personal-robotics/software/">https://www.imperial.ac.uk/personal-robotics/software/</a>. Please note that a lot of the code is shared with RT-GENE (see above), hence there are many references to RT-GENE below.</p>
<p><img src="/_next/static/images/rt_bene_overview-3f326aa400ba8af7178e4bd4f1b92698.png" alt="Paper overview"></p>
<h4>RT-BENE ROS package</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene>rt_gene</a> directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time. For blink estimation, please refer to the <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/scripts/estimate_blink.py>estimate_blink.py</a> file.</p>
<p align="center">
  <img src="/_next/static/gifs/364f43474c2cb421d206d8d484b293c0.gif" alt="RT-BENE inference example"/>
</p>
<h4>RT-BENE Standalone Version</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_standalone>rt_bene_standalone</a> directory contains instructions for blink estimation given a set of images. It makes use of the code in <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_bene>rt_gene/src/rt_bene</a>.</p>
<h4>RT-BENE Model Training</h4>
<p>The <a href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_model_training>rt_bene_model_training</a> directory contains the code required to train models with the labels contained in the RT-BENE dataset (see below). We will soon at evaluation code in this directory, too.</p>
<h4>RT-BENE Dataset</h4>
<p><img src="/_next/static/images/rt_bene_labels-efb80b606e00709e04146a1ed7a0938c.png" alt="RT-BENE labels"></p>
<p>We manually annotated images contained in the &quot;noglasses&quot; part of the RT-GENE dataset. The <a href="https://zenodo.org/record/3685316">RT-BENE dataset on Zenodo</a> contains the eye image patches and associated annotations to train the blink models.</p>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003ch1\u003eRT-GENE \u0026amp; RT-BENE: Real-Time Eye Gaze and Blink Estimation in Natural Environments\u003c/h1\u003e\\n\u003cp\u003e\u003ca href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\"\u003e\u003cimg src=\\\"https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg?style=flat-square\\\" alt=\\\"License: CC BY-NC-SA 4.0\\\"\u003e\u003c/a\u003e\\n\u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md\u003e\u003cimg src=\\\"http://hits.dwyl.io/Tobias-Fischer/rt_gene.svg\\\" alt=\\\"HitCount\\\"\u003e\u003c/a\u003e\\n\u003ca href=https://github.com/Tobias-Fischer/rt_gene/stargazers\u003e\u003cimg src=\\\"https://img.shields.io/github/stars/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"stars\\\"\u003e\u003c/a\u003e\\n\u003ca href=https://github.com/Tobias-Fischer/rt_gene/issues\u003e\u003cimg src=\\\"https://img.shields.io/github/issues/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"GitHub issues\\\"\u003e\u003c/a\u003e\\n\u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/README.md\u003e\u003cimg src=\\\"https://img.shields.io/github/repo-size/Tobias-Fischer/rt_gene.svg?style=flat-square\\\" alt=\\\"GitHub repo size\\\"\u003e\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-mpii-gaze?p=rt-gene-real-time-eye-gaze-estimation-in?style=square\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-mpii-gaze\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\\n\u003ca href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-rt-gene?p=rt-gene-real-time-eye-gaze-estimation-in\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-rt-gene\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\\n\u003ca href=\\\"https://paperswithcode.com/sota/gaze-estimation-on-ut-multi-view?p=rt-gene-real-time-eye-gaze-estimation-in\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-gene-real-time-eye-gaze-estimation-in/gaze-estimation-on-ut-multi-view\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003e\u003ca href=\\\"https://paperswithcode.com/sota/blink-estimation-on-eyeblink8?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-eyeblink8\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\\n\u003ca href=\\\"https://paperswithcode.com/sota/blink-estimation-on-researcher-s-night?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-researcher-s-night\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\\n\u003ca href=\\\"https://paperswithcode.com/sota/blink-estimation-on-rt-bene?p=rt-bene-a-dataset-and-baselines-for-real-time\\\"\u003e\u003cimg src=\\\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rt-bene-a-dataset-and-baselines-for-real-time/blink-estimation-on-rt-bene\u0026amp;style=flat-square\\\" alt=\\\"PWC\\\"\u003e\u003c/a\u003e\u003c/p\u003e\\n\u003cp\u003eThis repository contains code and dataset references for two papers: RT-GENE (Gaze Estimation; ECCV2018) and RT-BENE (Blink Estimation; ICCV2019 Workshops).\u003c/p\u003e\\n\u003ch2\u003eRT-GENE (Gaze Estimation)\u003c/h2\u003e\\n\u003ch3\u003eLicense + Attribution\u003c/h3\u003e\\n\u003cp\u003eThe RT-GENE code is licensed under \u003ca href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\"\u003eCC BY-NC-SA 4.0\u003c/a\u003e. Commercial usage is not permitted. If you use this dataset or the code in a scientific publication, please cite the following \u003ca href=\\\"http://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html\\\"\u003epaper\u003c/a\u003e:\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/paper_abstract-f5758078c2e378a46ec9b98b4176e657.jpg\\\" alt=\\\"Paper abstract\\\"\u003e\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@inproceedings{FischerECCV2018,\\nauthor = {Tobias Fischer and Hyung Jin Chang and Yiannis Demiris},\\ntitle = {{RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments}},\\nbooktitle = {European Conference on Computer Vision},\\nyear = {2018},\\nmonth = {September},\\npages = {339--357}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThis work was supported in part by the Samsung Global Research Outreach program, and in part by the EU Horizon 2020 Project PAL (643783-RIA).\u003c/p\u003e\\n\u003ch3\u003eOverview + Accompanying Dataset\u003c/h3\u003e\\n\u003cp\u003eThe code is split into four parts, each having its own README contained. There is also an accompanying \u003ca href=\\\"https://zenodo.org/record/2529036\\\"\u003edataset\u003c/a\u003e \u003ca href=\\\"https://goo.gl/tfUaDm\\\"\u003e(alternative link)\u003c/a\u003e to the code. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: \u003ca href=\\\"https://www.imperial.ac.uk/personal-robotics/software/\\\"\u003ehttps://www.imperial.ac.uk/personal-robotics/software/\u003c/a\u003e.\u003c/p\u003e\\n\u003ch4\u003eRT-GENE ROS package\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene\u003ert_gene\u003c/a\u003e directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time.\u003c/p\u003e\\n\u003cp align=\\\"center\\\"\u003e\\n  \u003cimg src=\\\"/_next/static/gifs/5b7fb335144a426aa0bf9f3a74f130a6.gif\\\" alt=\\\"RT-GENE inference example\\\"/\u003e\\n\u003c/p\u003e\\n\u003ch4\u003eRT-GENE Standalone Version\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_standalone\u003ert_gene_standalone\u003c/a\u003e directory contains instructions for eye gaze estimation given a set of images. It shares code with the \u003ccode class=\\\"language-none\\\"\u003ert_gene\u003c/code\u003e package (above), in particular the code in \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_gene\u003ert_gene/src/rt_gene\u003c/a\u003e.\u003c/p\u003e\\n\u003ch4\u003eRT-GENE Inpainting\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_inpainting\u003ert_gene_inpainting\u003c/a\u003e directory contains code to inpaint the region covered by the eyetracking glasses.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/inpaint_example-e34160d4c6d03d7e4eacf7795a7d3e02.jpg\\\" alt=\\\"Inpaining example\\\"\u003e\u003c/p\u003e\\n\u003ch4\u003eRT-GENE Model Training\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene_model_training\u003ert_gene_model_training\u003c/a\u003e directory allows using the inpainted images to train a deep neural network for eye gaze estimation.\u003c/p\u003e\\n\u003cp align=\\\"center\\\"\u003e\\n  \u003cimg src=\\\"/_next/static/images/accuracy_prl-10df02076ab749718e69f8cd75d3aec9.jpg\\\" alt=\\\"Accuracy on RT-GENE dataset\\\"/\u003e\\n\u003c/p\u003e\\n\u003ch2\u003eRT-BENE (Blink Estimation)\u003c/h2\u003e\\n\u003ch3\u003eLicense + Attribution\u003c/h3\u003e\\n\u003cp\u003eThe RT-BENE code is licensed under \u003ca href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\"\u003eCC BY-NC-SA 4.0\u003c/a\u003e. Commercial usage is not permitted. If you use our blink estimation code or dataset, please cite the relevant \u003ca href=\\\"http://openaccess.thecvf.com/content_ICCVW_2019/html/GAZE/Cortacero_RT-BENE_A_Dataset_and_Baselines_for_Real-Time_Blink_Estimation_in_ICCVW_2019_paper.html\\\"\u003epaper\u003c/a\u003e:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@inproceedings{CortaceroICCV2019W,\\nauthor={Kevin Cortacero and Tobias Fischer and Yiannis Demiris},\\nbooktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshops},\\ntitle = {RT-BENE: A Dataset and Baselines for Real-Time Blink Estimation in Natural Environments},\\nyear = {2019},\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eRT-BENE was supported by the EU Horizon 2020 Project PAL (643783-RIA) and a Royal Academy of Engineering Chair in Emerging Technologies to Yiannis Demiris.\u003c/p\u003e\\n\u003ch3\u003eOverview + Accompanying Dataset\u003c/h3\u003e\\n\u003cp\u003eThe code is split into several parts, each having its own README. There is also an associated \u003ca href=\\\"https://zenodo.org/record/3685316\\\"\u003eRT-BENE dataset\u003c/a\u003e. For more information, other datasets and more open-source software please visit the Personal Robotic Lab's website: \u003ca href=\\\"https://www.imperial.ac.uk/personal-robotics/software/\\\"\u003ehttps://www.imperial.ac.uk/personal-robotics/software/\u003c/a\u003e. Please note that a lot of the code is shared with RT-GENE (see above), hence there are many references to RT-GENE below.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/rt_bene_overview-3f326aa400ba8af7178e4bd4f1b92698.png\\\" alt=\\\"Paper overview\\\"\u003e\u003c/p\u003e\\n\u003ch4\u003eRT-BENE ROS package\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene\u003ert_gene\u003c/a\u003e directory contains a ROS package for real-time eye gaze and blink estimation. This contains all the code required at inference time. For blink estimation, please refer to the \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/scripts/estimate_blink.py\u003eestimate_blink.py\u003c/a\u003e file.\u003c/p\u003e\\n\u003cp align=\\\"center\\\"\u003e\\n  \u003cimg src=\\\"/_next/static/gifs/364f43474c2cb421d206d8d484b293c0.gif\\\" alt=\\\"RT-BENE inference example\\\"/\u003e\\n\u003c/p\u003e\\n\u003ch4\u003eRT-BENE Standalone Version\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_standalone\u003ert_bene_standalone\u003c/a\u003e directory contains instructions for blink estimation given a set of images. It makes use of the code in \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_gene/src/rt_bene\u003ert_gene/src/rt_bene\u003c/a\u003e.\u003c/p\u003e\\n\u003ch4\u003eRT-BENE Model Training\u003c/h4\u003e\\n\u003cp\u003eThe \u003ca href=https:/github.com/Tobias-Fischer/rt_gene/blob/master/rt_bene_model_training\u003ert_bene_model_training\u003c/a\u003e directory contains the code required to train models with the labels contained in the RT-BENE dataset (see below). We will soon at evaluation code in this directory, too.\u003c/p\u003e\\n\u003ch4\u003eRT-BENE Dataset\u003c/h4\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/rt_bene_labels-efb80b606e00709e04146a1ed7a0938c.png\\\" alt=\\\"RT-BENE labels\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eWe manually annotated images contained in the \u0026quot;noglasses\u0026quot; part of the RT-GENE dataset. The \u003ca href=\\\"https://zenodo.org/record/3685316\\\"\u003eRT-BENE dataset on Zenodo\u003c/a\u003e contains the eye image patches and associated annotations to train the blink models.\u003c/p\u003e\\n\",\"name\":\"Real-Time Gaze (RT-GENE) and Blink Estimation (RT-BENE)\",\"type\":\"code\",\"url\":\"https://github.com/Tobias-Fischer/rt_gene\",\"src\":\"/content/rt-gene.md\",\"id\":\"rt-gene\",\"image_position\":\"center\",\"image\":\"/_next/static/images/paper_abstract-f5758078c2e378a46ec9b98b4176e657.jpg\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"rt-gene"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>