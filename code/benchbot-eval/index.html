<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">BenchBot Evaluation Tools</span><a href="https://github.com/qcr/benchbot_eval" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">qcr/benchbot_eval</span><span class="markdown-body mdc-typography--body1"><div><p><strong>NOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation (although it can be installed independently through pip and run on results files if desired). For a working BenchBot system, please install the BenchBot software stack by following the instructions <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>
<h1>BenchBot Evaluation</h1>
<p>BenchBot Evaluation is a library of functions used to call evaluation methods. These methods are installed through the <a href=https://github.com/qcr/benchbot-addons>BenchBot Add-ons Manager</a>, and evaluate the performance of a BenchBot system against the metric. The easiest way to use this module is through the helper scripts provided with the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a>.</p>
<h2>Installing and performing evaluation with BenchBot Evaluation</h2>
<p>BenchBot Evaluation is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:</p>
<pre class="language-none"><code class="language-none">u@pc:~$ pip install .
</code></pre>
<p>Although evaluation is best run from within the BenchBot software stack, it can be run in isolation if desired. The following code snippet shows how to perform evaluation with the <code class="language-none">'omq'</code> method from Python:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_eval<span class="token punctuation">.</span>evaluator <span class="token keyword">import</span> Evaluator<span class="token punctuation">,</span> Validator

Validator<span class="token punctuation">(</span>results_file<span class="token punctuation">)</span><span class="token punctuation">.</span>validate_results_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
Evaluator<span class="token punctuation">(</span><span class="token string">'omq'</span><span class="token punctuation">,</span> scores_file<span class="token punctuation">)</span><span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p>This prints the final scores to the screen and saves them to a file using the following inputs:</p>
<ul>
<li><code class="language-none">results_file</code>: points to the JSON file with the output from your experiment</li>
<li><code class="language-none">ground_truth_folder</code>: the directory containing the relevant environment ground truth JSON files</li>
<li><code class="language-none">save_file</code>: is where final scores are to be saved</li>
</ul>
<h2>How add-ons interact with BenchBot Evaluation</h2>
<p>Two types of add-ons are used in the BenchBot Evaluation process: format definitions, and evaluation methods. An evaluation method's YAML file defines what results formats and ground truth formats the method supports. This means:</p>
<ul>
<li>this package requires installation of the <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager</a> for interacting with installed add-ons</li>
<li>the <code class="language-none">results_file</code> must be a valid instance of a supported format</li>
<li>there must be a valid ground truth available in a supported format, for the same environment as the results</li>
<li>validity is determined by the format-specific validation function described in the format's YAML file</li>
</ul>
<p>Please see the <a href=https://github.com/qcr/benchbot_addons>BenchBot Add-ons Manager's documentation</a> for further details on the different types of add-ons.</p>
<h2>Creating valid results and ground truth files</h2>
<p>The <a href=https://github.com/qcr/benchbot>BenchBot software stack</a> includes tools to assist in creating results and ground truth files:</p>
<ul>
<li>
<p><strong>results:</strong> are best created using the <code class="language-none">empty_results()</code> and <code class="language-none">results_functions()</code> helper functions in the <a href=https://github.com/qcr/benchbot_api>BenchBot API</a>, which automatically populate metadata for your current task and environment.</p>
</li>
<li>
<p><strong>ground truths:</strong> this package includes a <code class="language-none">GroundTruthCreator</code> class to aid in creating ground truths of a specific format, for a specific environment. Example use includes:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_eval<span class="token punctuation">.</span>ground_truth_creator <span class="token keyword">import</span> GroundTruthCreator

gtc <span class="token operator">=</span> GroundTruthCreator<span class="token punctuation">(</span><span class="token string">'object_map_ground_truth'</span><span class="token punctuation">,</span> <span class="token string">'miniroom:1'</span><span class="token punctuation">)</span>
gt <span class="token operator">=</span> gtc<span class="token punctuation">.</span>create_empty<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>gtc<span class="token punctuation">.</span>functions<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># ['create', 'create_object']</span>
gt<span class="token punctuation">[</span><span class="token string">'ground_truth'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'objects'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> gtc<span class="token punctuation">.</span>functions<span class="token punctuation">(</span><span class="token string">'create_object'</span><span class="token punctuation">)</span>
</code></pre>
</li>
</ul>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003cp\u003e\u003cstrong\u003eNOTE: this software is part of the BenchBot software stack, and not intended to be run in isolation (although it can be installed independently through pip and run on results files if desired). For a working BenchBot system, please install the BenchBot software stack by following the instructions \u003ca href=https://github.com/qcr/benchbot\u003ehere\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eBenchBot Evaluation\u003c/h1\u003e\\n\u003cp\u003eBenchBot Evaluation is a library of functions used to call evaluation methods. These methods are installed through the \u003ca href=https://github.com/qcr/benchbot-addons\u003eBenchBot Add-ons Manager\u003c/a\u003e, and evaluate the performance of a BenchBot system against the metric. The easiest way to use this module is through the helper scripts provided with the \u003ca href=https://github.com/qcr/benchbot\u003eBenchBot software stack\u003c/a\u003e.\u003c/p\u003e\\n\u003ch2\u003eInstalling and performing evaluation with BenchBot Evaluation\u003c/h2\u003e\\n\u003cp\u003eBenchBot Evaluation is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eu@pc:~$ pip install .\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eAlthough evaluation is best run from within the BenchBot software stack, it can be run in isolation if desired. The following code snippet shows how to perform evaluation with the \u003ccode class=\\\"language-none\\\"\u003e'omq'\u003c/code\u003e method from Python:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_eval\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eevaluator \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e Evaluator\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e Validator\\n\\nValidator\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eresults_file\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003evalidate_results_data\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\nEvaluator\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'omq'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e scores_file\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eevaluate\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThis prints the final scores to the screen and saves them to a file using the following inputs:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003eresults_file\u003c/code\u003e: points to the JSON file with the output from your experiment\u003c/li\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003eground_truth_folder\u003c/code\u003e: the directory containing the relevant environment ground truth JSON files\u003c/li\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003esave_file\u003c/code\u003e: is where final scores are to be saved\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eHow add-ons interact with BenchBot Evaluation\u003c/h2\u003e\\n\u003cp\u003eTwo types of add-ons are used in the BenchBot Evaluation process: format definitions, and evaluation methods. An evaluation method's YAML file defines what results formats and ground truth formats the method supports. This means:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003ethis package requires installation of the \u003ca href=https://github.com/qcr/benchbot_addons\u003eBenchBot Add-ons Manager\u003c/a\u003e for interacting with installed add-ons\u003c/li\u003e\\n\u003cli\u003ethe \u003ccode class=\\\"language-none\\\"\u003eresults_file\u003c/code\u003e must be a valid instance of a supported format\u003c/li\u003e\\n\u003cli\u003ethere must be a valid ground truth available in a supported format, for the same environment as the results\u003c/li\u003e\\n\u003cli\u003evalidity is determined by the format-specific validation function described in the format's YAML file\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003ePlease see the \u003ca href=https://github.com/qcr/benchbot_addons\u003eBenchBot Add-ons Manager's documentation\u003c/a\u003e for further details on the different types of add-ons.\u003c/p\u003e\\n\u003ch2\u003eCreating valid results and ground truth files\u003c/h2\u003e\\n\u003cp\u003eThe \u003ca href=https://github.com/qcr/benchbot\u003eBenchBot software stack\u003c/a\u003e includes tools to assist in creating results and ground truth files:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003cstrong\u003eresults:\u003c/strong\u003e are best created using the \u003ccode class=\\\"language-none\\\"\u003eempty_results()\u003c/code\u003e and \u003ccode class=\\\"language-none\\\"\u003eresults_functions()\u003c/code\u003e helper functions in the \u003ca href=https://github.com/qcr/benchbot_api\u003eBenchBot API\u003c/a\u003e, which automatically populate metadata for your current task and environment.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\\n\u003cp\u003e\u003cstrong\u003eground truths:\u003c/strong\u003e this package includes a \u003ccode class=\\\"language-none\\\"\u003eGroundTruthCreator\u003c/code\u003e class to aid in creating ground truths of a specific format, for a specific environment. Example use includes:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_eval\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eground_truth_creator \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e GroundTruthCreator\\n\\ngtc \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e GroundTruthCreator\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'object_map_ground_truth'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token string\\\"\u003e'miniroom:1'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\ngt \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e gtc\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003ecreate_empty\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e;\u003c/span\u003e\\n\u003cspan class=\\\"token keyword\\\"\u003eprint\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003egtc\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003efunctions\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e  \u003cspan class=\\\"token comment\\\"\u003e# ['create', 'create_object']\u003c/span\u003e\\ngt\u003cspan class=\\\"token punctuation\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'ground_truth'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e]\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'objects'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e]\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"token number\\\"\u003e0\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e]\u003c/span\u003e \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e gtc\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003efunctions\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'create_object'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\",\"name\":\"BenchBot Evaluation Tools\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_eval\",\"src\":\"/content/benchbot/benchbot-eval.md\",\"id\":\"benchbot-eval\",\"image_position\":\"center\",\"image\":\"/_next/static/images/qcr_logo_light_filled-b2f2ba81b0ef111afdf9fa7264fb4adf.png\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"benchbot-eval"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>