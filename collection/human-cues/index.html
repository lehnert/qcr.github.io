<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4380956d6a83f408d69e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4380956d6a83f408d69e.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/collection/%5Bcollection%5D-86b3253722d3b302f18a.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="collection_heading__2LrBw mdc-typography--headline3">Human Cues for Robot Navigation</span><a href="https://btalb.github.io/abstract_map" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Go to collection website</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0iYmxhY2siIHdpZHRoPSIyNHB4IiBoZWlnaHQ9IjI0cHgiPjxwYXRoIGQ9Ik0wIDBoMjR2MjRIMHoiIGZpbGw9Im5vbmUiLz48cGF0aCBkPSJNMTEuOTkgMkM2LjQ3IDIgMiA2LjQ4IDIgMTJzNC40NyAxMCA5Ljk5IDEwQzE3LjUyIDIyIDIyIDE3LjUyIDIyIDEyUzE3LjUyIDIgMTEuOTkgMnptNi45MyA2aC0yLjk1Yy0uMzItMS4yNS0uNzgtMi40NS0xLjM4LTMuNTYgMS44NC42MyAzLjM3IDEuOTEgNC4zMyAzLjU2ek0xMiA0LjA0Yy44MyAxLjIgMS40OCAyLjUzIDEuOTEgMy45NmgtMy44MmMuNDMtMS40MyAxLjA4LTIuNzYgMS45MS0zLjk2ek00LjI2IDE0QzQuMSAxMy4zNiA0IDEyLjY5IDQgMTJzLjEtMS4zNi4yNi0yaDMuMzhjLS4wOC42Ni0uMTQgMS4zMi0uMTQgMiAwIC42OC4wNiAxLjM0LjE0IDJINC4yNnptLjgyIDJoMi45NWMuMzIgMS4yNS43OCAyLjQ1IDEuMzggMy41Ni0xLjg0LS42My0zLjM3LTEuOS00LjMzLTMuNTZ6bTIuOTUtOEg1LjA4Yy45Ni0xLjY2IDIuNDktMi45MyA0LjMzLTMuNTZDOC44MSA1LjU1IDguMzUgNi43NSA4LjAzIDh6TTEyIDE5Ljk2Yy0uODMtMS4yLTEuNDgtMi41My0xLjkxLTMuOTZoMy44MmMtLjQzIDEuNDMtMS4wOCAyLjc2LTEuOTEgMy45NnpNMTQuMzQgMTRIOS42NmMtLjA5LS42Ni0uMTYtMS4zMi0uMTYtMiAwLS42OC4wNy0xLjM1LjE2LTJoNC42OGMuMDkuNjUuMTYgMS4zMi4xNiAyIDAgLjY4LS4wNyAxLjM0LS4xNiAyem0uMjUgNS41NmMuNi0xLjExIDEuMDYtMi4zMSAxLjM4LTMuNTZoMi45NWMtLjk2IDEuNjUtMi40OSAyLjkzLTQuMzMgMy41NnpNMTYuMzYgMTRjLjA4LS42Ni4xNC0xLjMyLjE0LTIgMC0uNjgtLjA2LTEuMzQtLjE0LTJoMy4zOGMuMTYuNjQuMjYgMS4zMS4yNiAycy0uMSAxLjM2LS4yNiAyaC0zLjM4eiIvPjwvc3ZnPg==)"></i></button></a><span class="markdown-body mdc-typography--body1"><div><p>The Human Cues for Robot Navigation ARC Discovery Project (DP140103216) investigated how a robot can navigate using the same navigation cues humans use when navigating built environments. Types of navigation cues targeted include labels, directional signs, signboards, maps &amp; floor plans, navigational gestures, and spoken directions &amp; descriptions. The main contribution from this work is the abstract map, a navigational tool that allows a robot to employ symbolic spatial information in its navigation of unseen spaces.</p>
</div> </span><span class="collection_subheading__2pmz1 mdc-typography--headline4">Code Repositories</span><div class="collection_cards__2s4k3"><div class="mdc-elevation--z4 mdc-elevation-transition card_card__3y3tW mdc-card"><div class="card_clickable___QgLM mdc-card__primary-action"><img src="/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png" class="card_media__1I_sY" style="object-position:center"/><div class="card_footer__2lBtj"><span class="card_extra__1-p0a card_url__3ScCn mdc-typography--body2">btalb/abstract_map</span><span class="mdc-typography--body1">Abstract Map (Python)</span></div></div></div><div class="mdc-elevation--z4 mdc-elevation-transition card_card__3y3tW mdc-card"><div class="card_clickable___QgLM mdc-card__primary-action"><img src="/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png" class="card_media__1I_sY" style="object-position:center"/><div class="card_footer__2lBtj"><span class="card_extra__1-p0a card_url__3ScCn mdc-typography--body2">btalb/abstract_map_simulator</span><span class="mdc-typography--body1">2D Simulator for Zoo Experiments</span></div></div></div><div class="mdc-elevation--z4 mdc-elevation-transition card_card__3y3tW mdc-card"><div class="card_clickable___QgLM mdc-card__primary-action"><video autoplay="" loop="" muted="" poster="/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg" class="card_media__1I_sY" style="object-position:center"><source src="/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm" type="video/webm"/></video><div class="card_footer__2lBtj"><span class="card_extra__1-p0a card_url__3ScCn mdc-typography--body2">btalb/abstract_map_app</span><span class="mdc-typography--body1">Android App for Human Participants</span></div></div></div></div></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"collectionData":"{\"content\":\"\u003cp\u003eThe Human Cues for Robot Navigation ARC Discovery Project (DP140103216) investigated how a robot can navigate using the same navigation cues humans use when navigating built environments. Types of navigation cues targeted include labels, directional signs, signboards, maps \u0026amp; floor plans, navigational gestures, and spoken directions \u0026amp; descriptions. The main contribution from this work is the abstract map, a navigational tool that allows a robot to employ symbolic spatial information in its navigation of unseen spaces.\u003c/p\u003e\\n\",\"name\":\"Human Cues for Robot Navigation\",\"type\":\"collection\",\"url\":\"https://btalb.github.io/abstract_map\",\"code\":[{\"content\":\"\u003cp align=center\u003e\u003cstrong\u003e~ Please see the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003eabstract map site\u003c/a\u003e for further details about the research publication ~\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eThe Abstract Map - using symbols to navigate\u003c/h1\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\\\" alt=\\\"The abstract map in action\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eThis repository provides the implementation of the abstract map used in our \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003eIEEE TCDS journal\u003c/a\u003e. The implementation, done in Python, includes the following features:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003ea novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, \u0026amp; collision models)\u003c/li\u003e\\n\u003cli\u003ea visualiser \u0026amp; text-based commentator for introspection of your navigation system (both shown in videos on the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003erepository website\u003c/a\u003e)\u003c/li\u003e\\n\u003cli\u003eeasy ROS bindings for getting up \u0026amp; running in simulation or on a real robot\u003c/li\u003e\\n\u003cli\u003etag readers \u0026amp; interpreters for extracting symbolic spatial information from \u003ca href=\\\"http://wiki.ros.org/apriltag_ros\\\"\u003eAprilTags\u003c/a\u003e\u003c/li\u003e\\n\u003cli\u003econfiguration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003ethe paper\u003c/a\u003e for further details)\u003c/li\u003e\\n\u003cli\u003eserialisation methods for passing an entire abstract map state between machines, or saving to file\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003ePlease see our other related repositories for further resources, and related parts of the abstract map studies:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ca href=https://github.com/btalb/abstract_map_simulator\u003eabstract_map_simulator\u003c/a\u003e:\u003c/strong\u003e all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT\u003c/li\u003e\\n\u003cli\u003e\u003cstrong\u003e\u003ca href=https://github.com/btalb/abstract_map_app\u003eabstract_map_app\u003c/a\u003e:\u003c/strong\u003e mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags \u0026amp; present the mapped symbolic spatial information in real time)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eGetting up \u0026amp; running with the abstract map\u003c/h2\u003e\\n\u003cp\u003e\u003cem\u003eNote: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the \u003ca href=https://github.com/btalb/abstract_map_simulator\u003eabstract_map_simulator\u003c/a\u003e package\u003c/em\u003e\u003c/p\u003e\\n\u003ch3\u003eSetting up your environment\u003c/h3\u003e\\n\u003cp\u003eClone the repo \u0026amp; install all Python dependencies:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003egit clone https://github.com/btalb/abstract_map\\npip install -r abstract_map/requirements.txt\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eAdd the new package to your ROS workspace at \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;ROS_WS\u0026gt;/\u003c/code\u003e by linking in the cloned repository:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eln -s \u0026lt;LOCATION_REPO_WAS_CLONED_ABOVE\u0026gt; \u0026lt;ROS_WS\u0026gt;/src/\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eInstall all of the listed ROS dependencies, and build the package:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003ecd \u0026lt;ROS_WS\u0026gt;/src/\\nrosdep install abstract_map\\ncd \u0026lt;ROS_WS\u0026gt;\\ncatkin_make\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3\u003eRunning the Zoo experiments\u003c/h3\u003e\\n\u003cp\u003eStart the experiment (this will try \u0026amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eroslaunch abstract_map experiment.launch\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e\u003cem\u003e(please see \u003ca href=https://github.com/btalb/abstract_map_simulator/issues/1\u003ethis issue\u003c/a\u003e for details if you get the spam of TF based errors... which probably shouldn't even be errors... )\u003c/em\u003e\u003c/p\u003e\\n\u003cp\u003eIn another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map hierarchy_publisher\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThis will use the hierarchy available in \u003ccode class=\\\"language-none\\\"\u003e./experiments/zoo_hierarchy.xml\u003c/code\u003e by default. Feel free to make your own if you would like to do different experiments.\u003c/p\u003e\\n\u003cp\u003eStart the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map visualiser\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003erosrun abstract_map visualiser --dark\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/abstract_map_light_vs_dark-ae93c3e7b8419b56719b5d876dd150f4.png\\\" alt=\\\"Visualise the abstract map with dark or light colours\\\"\u003e\u003c/p\u003e\\n\u003cp\u003eFinally, start the abstract map with a goal, and watch it attempt to complete the navigation task:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eroslaunch abstract_map abstract_map.launch goal:=Lion\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eIf you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in \u0026quot;observe mode\u0026quot;.\u003c/p\u003e\\n\u003ch2\u003eAcknowledgements \u0026amp; Citing our work\u003c/h2\u003e\\n\u003cp\u003eThis work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the \u003ca href=\\\"https://research.qut.edu.au/qcr/\\\"\u003eQUT Centre for Robotics\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eIf you use this software in your research, or for comparisons, please kindly cite our work:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\",\"name\":\"Abstract Map (Python)\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map\",\"src\":\"/content/human_cues/abstract-map.md\",\"id\":\"abstract-map\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"},{\"content\":\"\u003cp align=center\u003e\u003cstrong\u003e~ Please see the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003eabstract map site\u003c/a\u003e for further details about the research publication ~\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eUsing the Abstract Map in a 2D Stage simulation\u003c/h1\u003e\\n\u003cp\u003e\u003cimg src=\\\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\\\" alt=\\\"2D Stage simulation with with simulated tags\\\"\u003e\u003c/p\u003e\\n\u003cp\u003ePackage contains everything needed to simulate the zoo experiments performed in our \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003eIEEE TCDS journal\u003c/a\u003e. The package includes:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eworld \u0026amp; launch files for a stage simulation of the GP-S11 environment on QUT's Gardens Point campus\u003c/li\u003e\\n\u003cli\u003ea tool for creating simulated tags in an environment \u0026amp; saving them to file,\u003c/li\u003e\\n\u003cli\u003elaunch \u0026amp; config files for using the move_base navigation stack with gmapping to explore unseen simulated environments\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eInstalling the abstract map simulator\u003c/h2\u003e\\n\u003cp\u003e\u003cem\u003eNote: this is just the simulator; to use the abstract map with the simulator please make sure you use the \u003ca href=https://github.com/btalb/abstract_map\u003eabstract_map\u003c/a\u003e package\u003c/em\u003e\u003c/p\u003e\\n\u003cp\u003eClone the repo \u0026amp; install all Python dependencies:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003egit clone https://github.com/btalb/abstract_map_simulator\\npip install -r abstract_map_simulator/requirements.txt\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eAdd the new package to your ROS workspace at \u003ccode class=\\\"language-none\\\"\u003e\u0026lt;ROS_WS\u0026gt;/\u003c/code\u003e by linking in the cloned repository:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eln -s \u0026lt;LOCATION_REPO_WAS_CLONED_ABOVE\u0026gt; \u0026lt;ROS_WS\u0026gt;/src/\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eInstall all of the listed ROS dependencies, and build the package:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003ecd \u0026lt;ROS_WS\u0026gt;/src/\\nrosdep install abstract_map_simulator\\ncd \u0026lt;ROS_WS\u0026gt;\\ncatkin_make\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2\u003eAcknowledgements \u0026amp; Citing our work\u003c/h2\u003e\\n\u003cp\u003eThis work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the \u003ca href=\\\"https://research.qut.edu.au/qcr/\\\"\u003eQUT Centre for Robotics\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eIf you use this software in your research, or for comparisons, please kindly cite our work:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\",\"name\":\"2D Simulator for Zoo Experiments\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_simulator\",\"src\":\"/content/human_cues/abstract-map-simulator.md\",\"id\":\"abstract-map-simulator\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\"},{\"content\":\"\u003cp align=center\u003e\u003cstrong\u003e~ Please see the \u003ca href=\\\"https://btalb.github.io/abstract_map/\\\"\u003eabstract map site\u003c/a\u003e for further details about the research publication ~\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eApp for the Human vs Abstract Map Zoo Experiments\u003c/h1\u003e\\n\u003cp\u003e\u003cvideo autoplay loop poster=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\\\"\u003e\u003csource src=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\\\" type=\\\"video/webm\\\"/\u003e\u003c/video\u003e\u003c/p\u003e\\n\u003cp\u003eThis repository contains the mobile application used by human participants in the zoo experiments described in our \u003ca href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\"\u003eIEEE TCDS journal\u003c/a\u003e. The app, created with Android Studio, includes the following:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eopening screen for users to select experiment name \u0026amp; goal location\u003c/li\u003e\\n\u003cli\u003elive display of the camera to help users correctly capture a tag\u003c/li\u003e\\n\u003cli\u003einstant visual feedback when a tag is detected, with colouring to denote whether symbolic spatial information is not the goal (red), navigation information (orange), or the goal (green)\u003c/li\u003e\\n\u003cli\u003eexperiment definitions \u0026amp; tag mappings are creatable via the same XML style used in the \u003ca href=https://github.com/btalb/abstract_map\u003eabstract_map\u003c/a\u003e package\u003c/li\u003e\\n\u003cli\u003eintegration with the \u003ca href=https://github.com/AprilRobotics/apriltag\u003enative C AprilTags\u003c/a\u003e using the Android NDK\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch2\u003eDeveloping \u0026amp; producing the app\u003c/h2\u003e\\n\u003cp\u003eThe project should be directly openable using Android Studio.\u003c/p\u003e\\n\u003cp\u003ePlease keep in mind that this app was last developed in 2019, and Android Studio often introduces minor breaking changes with new versions. Often you will have to tweak things like Gradle versions / syntax etc. to get a project working with newer versions. Android Studio is very good though with pointing out where it sees errors and offering suggestions for how to resolve them.\u003c/p\u003e\\n\u003cp\u003eOnce you have the project open, you should be able to compile the app and load it directly onto a device without issues.\u003c/p\u003e\\n\u003ch2\u003eAcknowledgements \u0026amp; Citing our work\u003c/h2\u003e\\n\u003cp\u003eThis work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the \u003ca href=\\\"https://research.qut.edu.au/qcr/\\\"\u003eQUT Centre for Robotics\u003c/a\u003e.\u003c/p\u003e\\n\u003cp\u003eIf you use this software in your research, or for comparisons, please kindly cite our work:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003e@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n\u003c/code\u003e\u003c/pre\u003e\\n\",\"name\":\"Android App for Human Participants\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_app\",\"src\":\"/content/human_cues/abstract-map-app.md\",\"id\":\"abstract-map-app\",\"image_position\":\"center\",\"image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\",\"_image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\"}],\"feature\":0,\"src\":\"/content/human_cues/human-cues.md\",\"id\":\"human-cues\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"}"},"__N_SSG":true},"page":"/collection/[collection]","query":{"collection":"human-cues"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/collection/%5Bcollection%5D-86b3253722d3b302f18a.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>