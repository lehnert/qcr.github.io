<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H0HTWHNLPD"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-H0HTWHNLPD', {
              page_path: window.location.pathname,
            });
          </script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>QUT Centre for Robotics Open Source</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/ec58676f2add16c92212.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ec58676f2add16c92212.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e59d3ff98fad3f065f44.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e59d3ff98fad3f065f44.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.455c36b53add9c9c2736.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" as="script"/><link rel="preload" href="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" as="script"/></head><body><div id="__next"><div class="site" style="--mdc-theme-on-primary:rgba(255, 255, 255, 1);--mdc-theme-primary:#00407a"><header class="top_bar_bar__3T8Pf mdc-top-app-bar"><div class="top_bar_row__2Br8o mdc-top-app-bar__row"><section class="top_bar_logo-section__-bkhv mdc-top-app-bar__section mdc-top-app-bar__section--align-start"><img class="top_bar_logo__27Lwl" alt="QCR Logo (light)" src="/_next/static/images/qcr_logo_light-3a0967f7c1a32ca7de4713af85481529.png"/></section><section class="top_bar_pages__3emYr mdc-top-app-bar__section mdc-top-app-bar__section--align-end"><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Collections</span></button><button class="top_bar_selected-tab__2hCGV mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Code</span></button><button class="mdc-button"><div class="mdc-button__ripple"></div><span class="mdc-button__label">Datasets</span></button></section></div></header><div class="layout_space__3mcnW"></div><div class="layout_main__1OEEk layout_content__3ZRgy"><span class="code_heading__1xc27 mdc-typography--headline3">BenchBot Python API</span><a href="https://github.com/qcr/benchbot_api" target="_blank" class="focus_button_link__3dooQ"><button class="focus_button_button__MO_3J mdc-button mdc-button--raised"><div class="mdc-button__ripple"></div><span class="mdc-button__label">View the code on GitHub</span><i class="rmwc-icon rmwc-icon--url material-icons mdc-button__icon" style="background-image:url(data:image/svg+xml;base64,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)"></i></button></a><span class="code_extra__yQqAk mdc-typography--body2">qcr/benchbot_api</span><span class="markdown-body mdc-typography--body1"><div><p><strong>NOTE: this software needs to interface with a running instance of the BenchBot software stack. Unless you are running against a remote stack / robot, please install this software with the BenchBot software stack as described <a href=https://github.com/qcr/benchbot>here</a>.</strong></p>
<h1>BenchBot API</h1>
<p><video autoplay loop poster="/_next/static/gifs/566ece96917dbb84cd386109b8457390.jpg"><source src="/_next/static/gifs/566ece96917dbb84cd386109b8457390.webm" type="video/webm"/></video></p>
<p>The BenchBot API provides a simple interface for controlling a robot or simulator through actions, and receiving data through observations. As shown above, the entire code required for running an agent in a realistic 3D simulator is only a handful of simple Python commands.</p>
<p><a href="https://gym.openai.com">Open AI Gym</a> users will find the breakdown into actions, observations, and steps extremely familiar. BenchBot API allows researchers to develop and test novel algorithms with real robot systems and realistic 3D simulators, without the typical hassles arising when interfacing with complicated multi-component robot systems.</p>
<p>Running a robot through an entire environment, with your own custom agent, is as simple as one line of code with the BenchBot API:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_api <span class="token keyword">import</span> BenchBot
<span class="token keyword">from</span> my_agent <span class="token keyword">import</span> MyAgent

BenchBot<span class="token punctuation">(</span>agent<span class="token operator">=</span>MyAgent<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p>The above assumes you have created your own agent by overloading the abstract <code class="language-none">Agent</code> class provided with the API. Overloading the abstract class requires implementing 3 basic methods. Below is a basic example to spin on the spot:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_api <span class="token keyword">import</span> Agent
<span class="token keyword">import</span> json

<span class="token keyword">class</span> <span class="token class-name">MyAgent</span><span class="token punctuation">(</span>Agent<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">is_done</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> action_result<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Go forever</span>
        <span class="token keyword">return</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">pick_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> observations<span class="token punctuation">,</span> action_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Rotates on the spot indefinitely, 5 degrees at a time</span>
        <span class="token comment"># (assumes we are running in passive mode)</span>
        <span class="token keyword">return</span> <span class="token string">'move_angle'</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token string">'angle'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">save_result</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> empty_results<span class="token punctuation">,</span> results_format_fns<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Save some blank results</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>empty_results<span class="token punctuation">,</span> f<span class="token punctuation">)</span>
</code></pre>
<p>If you prefer to do things manually, a more exhaustive suite of functions are also available as part of the BenchBot API. Instead of using the <code class="language-none">BenchBot.run()</code> method, a large number of methods are available through the API. Below highlights a handful of the capabilities of BenchBot API:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_api <span class="token keyword">import</span> BenchBot<span class="token punctuation">,</span> RESULT_LOCATION
<span class="token keyword">import</span> json
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># Create a BenchBot instance &amp; reset the simulator / robot to starting state</span>
b <span class="token operator">=</span> BenchBot<span class="token punctuation">(</span><span class="token punctuation">)</span>
observations<span class="token punctuation">,</span> action_result <span class="token operator">=</span> b<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Print details of selected task &amp; environment</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>task_details<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>environment_details<span class="token punctuation">)</span>

<span class="token comment"># Visualise the current RGB image from the robot</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>observations<span class="token punctuation">[</span><span class="token string">'image_rgb'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Move to the next pose if we have a 'move_next' action available</span>
<span class="token keyword">if</span> <span class="token string">'move_next'</span> <span class="token keyword">in</span> b<span class="token punctuation">.</span>actions<span class="token punctuation">:</span>
    observations<span class="token punctuation">,</span> action_result <span class="token operator">=</span> b<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token string">'move_next'</span><span class="token punctuation">)</span>

<span class="token comment"># Save some empty results</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>RESULT_LOCATION<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>b<span class="token punctuation">.</span>empty_results<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> f<span class="token punctuation">)</span>
</code></pre>
<p>For sample solutions that use the BenchBot API, see the examples add-ons available (e.g. <a href=https://github.com/benchbot-addons/examples_base><code class="language-none">benchbot-addons/examples_base</code></a> and <a href=https://github.com/benchbot-addons/examples_ssu><code class="language-none">benchbot-addons/examples_ssu</code></a>).</p>
<h2>Installing BenchBot API</h2>
<p>BenchBot API is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:</p>
<pre class="language-none"><code class="language-none">u@pc:~$ pip install .
</code></pre>
<h2>Using the API to communicate with a robot</h2>
<p>Communication with the robot comes through a series of &quot;channels&quot; which are defined by the robot's definition file (e.g. <a href=https://github.com/benchbot-addons/robots_isaac/blob/master/robots/carter.yaml>carter</a>). A task definition file (e.g. <a href=https://github.com/benchbot-addons/tasks_ssu/blob/master/tasks/sslam_pgt.yaml>semantic_slam:passive:ground_truth</a>) then declares which of these connections are provided to the API as either sensor observations or actions to be executed by a robot actuator.</p>
<p>The API talks to the <a href=https://github.com/qcr/benchbot_supervisor>BenchBot Supervisor</a>, which handles loading and managing the different kinds of back-end configuration files. This abstracts all of the underlying communication complexities away from the user, allowing the BenchBot API to remain a simple interface that focuses on getting observations and sending actions.</p>
<p>An action is sent to the robot by calling the <code class="language-none">BenchBot.step()</code> method with a valid action (found by checking the <code class="language-none">BenchBot.actions</code> property):</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_api <span class="token keyword">import</span> BenchBot

b <span class="token operator">=</span> BenchBot<span class="token punctuation">(</span><span class="token punctuation">)</span>
available_actions <span class="token operator">=</span> b<span class="token punctuation">.</span>actions
b<span class="token punctuation">.</span>step<span class="token punctuation">(</span>b<span class="token punctuation">.</span>actions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token string">'action_arg:'</span><span class="token punctuation">,</span> arg_value<span class="token punctuation">}</span><span class="token punctuation">)</span>  <span class="token comment"># Perform the first available action</span>
</code></pre>
<p>The second parameter is a dictionary of named arguments for the selected action. For example, moving 5m forward with the <code class="language-none">'move_distance'</code> action is represented by the dictionary <code class="language-none">{'distance': 5}</code>.</p>
<p>Observations lists are received as return values from a <code class="language-none">BenchBot.step()</code> call (<code class="language-none">BenchBot.reset()</code> internally calls <code class="language-none">BenchBot.step(None)</code>, which means don't perform an action):</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> benchbot_api <span class="token keyword">import</span> BenchBot

b <span class="token operator">=</span> BenchBot<span class="token punctuation">(</span><span class="token punctuation">)</span>
observations<span class="token punctuation">,</span> action_result <span class="token operator">=</span> b<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
observations<span class="token punctuation">,</span> action_result <span class="token operator">=</span> b<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token string">'move_distance'</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token string">'distance'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre>
<p>The returned <code class="language-none">observations</code> variable holds a dictionary with key-value pairs corresponding to the name-data defined by each observation channel.</p>
<p>The <code class="language-none">action_result</code> is an enumerated value denoting the result of the action (use <code class="language-none">from benchbot_api import ActionResult</code> to access the <code class="language-none">Enum</code> class). You should use this result to guide the progression of your algorithm either manually or in the <code class="language-none">is_done()</code> method of your <code class="language-none">Agent</code>. Possible values for the returned <code class="language-none">action_result</code> are:</p>
<ul>
<li><code class="language-none">ActionResult.SUCCESS</code>: the action was carried out successfully</li>
<li><code class="language-none">ActionResult.FINISHED</code>: the action was carried out successfully, and the robot is now finished its traversal through the scene (only used in <code class="language-none">passive</code> actuation mode)</li>
<li><code class="language-none">ActionResult.COLLISION</code>: the action crashed the robot into an obstacle, and as a result it will not respond to any further actuation commands (at this point you should quit)</li>
</ul>
<h3>Standard Communication Channels</h3>
<p>Tasks and robot definition files declare actions and observations, and these files are include through <a href=https://github.com/qcr/benchbot_addons>BenchBot add-ons</a>. The add-on creator is free to add and declare channels as they please, but it is a better experience for all if channel definitions are as consistent as possible across the BenchBot ecosystem.</p>
<p>So if you're adding a robot that move between a set of poses, declare a channel called <code class="language-none">'move_next</code> with no arguments. Likewise, a robot that receives image observations should use a channel named <code class="language-none">'image_rgb'</code> with the same format as described below. Feel free to implement the channels however you please for your robot, but consistent interfaces should always be preferred.</p>
<p>If you encounter a task using non-standard channel configurations, the API has all the functionality you need as a user to handle them (<code class="language-none">actions</code>, <code class="language-none">config</code>, &amp; <code class="language-none">observations</code> properties). On the other hand, maybe the non-standard channel should be a new standard. New standard communication channels are always welcome; please open a pull request with the details!</p>
<h4>Standard action channels:</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th style="text-align:center">Required Arguments</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">'move_next'</code></td>
<td style="text-align:center"><code class="language-none">None</code></td>
<td>Moves the robot to the next pose in its list of pre-defined poses (only available in environments that declare a <code class="language-none">'trajectory_poses'</code> field).</td>
</tr>
<tr>
<td><code class="language-none">'move_distance'</code></td>
<td style="text-align:center"><pre class="language-none">{'distance': float}</pre></td>
<td>Moves the robot <code class="language-none">'distance'</code> metres directly ahead.</td>
</tr>
<tr>
<td><code class="language-none">'move_angle'</code></td>
<td style="text-align:center"><pre class="language-none">{'angle': float}</pre></td>
<td>Rotate the angle on the spot by <code class="language-none">'angle'</code> degrees.</td>
</tr>
</tbody>
</table>
<h4>Standard observation channels:</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th style="text-align:left">Data format</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">'image_depth'</code></td>
<td style="text-align:left"><pre class="language-none">numpy.ndarray(shape=(H,W),<br> dtype='float32')</pre></td>
<td>Depth image from the default image sensor with depths in meters.</td>
</tr>
<tr>
<td><code class="language-none">'image_depth_info'</code></td>
<td style="text-align:left"><pre class="language-none">{<br> 'frame_id': string<br> 'height': int<br> 'width': int<br> 'matrix_instrinsics':<br> numpy.ndarray(shape=(3,3),<br> dtype='float64')<br>'matrix_projection':<br> numpy.ndarray(shape=(3,4)<br> dtype='float64')<br>}</pre></td>
<td>Sensor information for the depth image. <code class="language-none">'matrix_instrinsics'</code> is of the format:<br><pre class="language-none">[fx 0 cx]<br>[0 fy cy]<br>[0 0 1]</pre> for a camera with focal lengths <code class="language-none">(fx,fy)</code>, &amp; principal point <code class="language-none">(cx,cy)</code>. Likewise, <code class="language-none">'matrix_projection'</code> is:<br><pre class="language-none">[fx 0 cx Tx]<br>[0 fy cy Ty]<br>[0 0 1 0]</pre>where <code class="language-none">(Tx,Ty)</code> is the translation between stereo sensors. See <a href="http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html">here</a> for further information on fields.</td>
</tr>
<tr>
<td><code class="language-none">'image_rgb'</code></td>
<td style="text-align:left"><pre class="language-none">numpy.ndarray(shape=(H,W,3),<br> dtype='uint8')</pre></td>
<td>RGB image from the default image sensor with colour values mapped to the 3 channels, in the 0-255 range.</td>
</tr>
<tr>
<td><code class="language-none">'image_rgb_info'</code></td>
<td style="text-align:left"><pre class="language-none">{<br> 'frame_id': string<br> 'height': int<br> 'width': int<br> 'matrix_instrinsics':<br> numpy.ndarray(shape=(3,3),<br> dtype='float64')<br>'matrix_projection':<br> numpy.ndarray(shape=(3,4)<br> dtype='float64')<br>}</pre></td>
<td>Sensor information for the RGB image. <code class="language-none">'matrix_instrinsics'</code> is of the format:<br><pre class="language-none">[fx 0 cx]<br>[0 fy cy]<br>[0 0 1]</pre> for a camera with focal lengths <code class="language-none">(fx,fy)</code>, &amp; principal point <code class="language-none">(cx,cy)</code>. Likewise, <code class="language-none">'matrix_projection'</code> is:<br><pre class="language-none">[fx 0 cx Tx]<br>[0 fy cy Ty]<br>[0 0 1 0]</pre>where <code class="language-none">(Tx,Ty)</code> is the translation between stereo sensors. See <a href="http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html">here</a> for further information on fields.</td>
</tr>
<tr>
<td><code class="language-none">'laser'</code></td>
<td style="text-align:left"><pre class="language-none">{<br> 'range_max': float64,<br> 'range_min': float64,<br> 'scans':<br> numpy.ndarray(shape=(N,2),<br> dtype='float64')<br>}</pre></td>
<td>Set of scan values from a laser sensor, between <code class="language-none">'range_min'</code> &amp; <code class="language-none">'range_max'</code> (in meters). The <code class="language-none">'scans'</code> array consists of <code class="language-none">N</code> scans of format <code class="language-none">[scan_angle, scan_value]</code>. For example, <code class="language-none">scans[100,0]</code> would get the angle of the 100th scan &amp; <code class="language-none">scans[100,1]</code> would get the distance value.</td>
</tr>
<tr>
<td><code class="language-none">'poses'</code></td>
<td style="text-align:left"><pre class="language-none">{<br> ...<br> 'frame_name': {<br> 'parent_frame': string<br> 'rotation_rpy':<br> numpy.ndarray(shape=(3,),<br> dtype='float64')<br> 'rotation_xyzw':<br> numpy.ndarray(shape=(4,),<br> dtype='float64')<br> 'translation_xyz':<br> numpy.ndarray(shape=(3,),<br> dtype='float64')<br> }<br> ...<br>}</pre></td>
<td>Dictionary of relative poses for the current system state. The pose of each system component is available at key <code class="language-none">'frame_name'</code>. Each pose has a <code class="language-none">'parent_frame'</code> which the pose is relative to (all poses are typically with respect to global <code class="language-none">'map'</code> frame), &amp; the pose values. <code class="language-none">'rotation_rpy'</code> is <code class="language-none">[roll,pitch,yaw]</code>, <code class="language-none">'rotation_xyzw'</code> is the equivalent quaternion <code class="language-none">[x,y,z,w]</code>, &amp; <code class="language-none">'translation_xyz'</code> is the Cartesion <code class="language-none">[x,y,z]</code> coordinates.</td>
</tr>
</tbody>
</table>
<h2>Using the API to communicate with the BenchBot system</h2>
<p>A running BenchBot system manages many other elements besides simply getting data to and from a real / simulated robot. BenchBot encapsulates not just the robot, but also the environment it is operating in (whether that be simulator or real) and task that is currently being attempted.</p>
<p>The API handles communication for all parts of the BenchBot system, including controlling the currently running environment and obtaining configuration information. Below are details for some of the more useful features of the API (all features are also documented in the <a href=https:/github.com/qcr/benchbot_api/blob/master/benchbot_api/benchbot.py><code class="language-none">benchbot.py</code></a> source code).</p>
<h3>Gathering configuration information</h3>
<table>
<thead>
<tr>
<th>API method or property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">config</code></td>
<td>Returns a <code class="language-none">dict</code> exhaustively describing the current BenchBot configuration. Most of the information returned will not be useful for general BenchBot use.</td>
</tr>
</tbody>
</table>
<h3>Interacting with the environment</h3>
<table>
<thead>
<tr>
<th>API method or property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">reset()</code></td>
<td>Resets the current environment scene. For the simulator, this means restarting the running simulator instance with the robot back at its initial position. The method returns initial <code class="language-none">observations</code>, &amp; the <code class="language-none">action_result</code> (should always be <code class="language-none">BenchBot.ActionResult.SUCCESS</code>).</td>
</tr>
<tr>
<td><code class="language-none">next_scene()</code></td>
<td>Starts the next scene in the current environment (only relevant for tasks with multiple scenes). Note there is no going back once you have moved to the next scene. Returns the same as <code class="language-none">reset()</code>.</td>
</tr>
</tbody>
</table>
<h3>Interacting with an agent</h3>
<table>
<thead>
<tr>
<th>API method or property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">actions</code></td>
<td>Returns the list of actions currently available to the agent. This will update as actions are performed in the environment (for example if the agent has collided with an obstacle this list will be empty).</td>
</tr>
<tr>
<td><code class="language-none">observations</code></td>
<td>Returns the lists of observations available to the agent.</td>
</tr>
<tr>
<td><code class="language-none">step(action, **action_args)</code></td>
<td>Performs the requested action with the provided named action arguments. See <a href=https:/github.com/qcr/benchbot_api/#using-the-api-to-communicate-with-a-robot>Using the API to communicate with a robot</a> above for further details.</td>
</tr>
</tbody>
</table>
<h3>Creating results</h3>
<table>
<thead>
<tr>
<th>API method or property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code class="language-none">empty_results()</code></td>
<td>Generates a <code class="language-none">dict</code> of with required result metadata &amp; empty results. Metadata (<code class="language-none">'task_details'</code> &amp; <code class="language-none">'environment_details'</code>) is pre-filled. To create results, all a user needs to do is fill in the empty <code class="language-none">'results'</code> field using format's results functions. These functions are available through the <code class="language-none">'results_functions()</code> method.</td>
</tr>
<tr>
<td><code class="language-none">results_functions()</code></td>
<td>Returns a <code class="language-none">dict</code> of functions defined by the task's <code class="language-none">'results_format'</code>. Example use for calling a <code class="language-none">create()</code> function is <code class="language-none">results_functions()['create']()</code>.</td>
</tr>
<tr>
<td><code class="language-none">RESULT_LOCATION</code> (outside of <code class="language-none">BenchBot</code> class)</td>
<td>A static string denoting where results should be saved (<code class="language-none">/tmp/results</code>). Using this locations ensures tools in the <a href=https://github.com/qcr/benchbot>BenchBot software stack</a> work as expected.</td>
</tr>
</tbody>
</table>
</div> </span></div><div class="bottom_bar_bar__B7RGm"><div class="site-bottom-bar bottom_bar_content__2DVtD"><div></div><div></div><div><span class="mdc-typography--body2">CRICOS No. 00213J</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"codeData":"{\"content\":\"\u003cp\u003e\u003cstrong\u003eNOTE: this software needs to interface with a running instance of the BenchBot software stack. Unless you are running against a remote stack / robot, please install this software with the BenchBot software stack as described \u003ca href=https://github.com/qcr/benchbot\u003ehere\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\\n\u003ch1\u003eBenchBot API\u003c/h1\u003e\\n\u003cp\u003e\u003cvideo autoplay loop poster=\\\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.jpg\\\"\u003e\u003csource src=\\\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.webm\\\" type=\\\"video/webm\\\"/\u003e\u003c/video\u003e\u003c/p\u003e\\n\u003cp\u003eThe BenchBot API provides a simple interface for controlling a robot or simulator through actions, and receiving data through observations. As shown above, the entire code required for running an agent in a realistic 3D simulator is only a handful of simple Python commands.\u003c/p\u003e\\n\u003cp\u003e\u003ca href=\\\"https://gym.openai.com\\\"\u003eOpen AI Gym\u003c/a\u003e users will find the breakdown into actions, observations, and steps extremely familiar. BenchBot API allows researchers to develop and test novel algorithms with real robot systems and realistic 3D simulators, without the typical hassles arising when interfacing with complicated multi-component robot systems.\u003c/p\u003e\\n\u003cp\u003eRunning a robot through an entire environment, with your own custom agent, is as simple as one line of code with the BenchBot API:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_api \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e BenchBot\\n\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e my_agent \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e MyAgent\\n\\nBenchBot\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eagent\u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003eMyAgent\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003erun\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe above assumes you have created your own agent by overloading the abstract \u003ccode class=\\\"language-none\\\"\u003eAgent\u003c/code\u003e class provided with the API. Overloading the abstract class requires implementing 3 basic methods. Below is a basic example to spin on the spot:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_api \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e Agent\\n\u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e json\\n\\n\u003cspan class=\\\"token keyword\\\"\u003eclass\u003c/span\u003e \u003cspan class=\\\"token class-name\\\"\u003eMyAgent\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eAgent\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n\\n    \u003cspan class=\\\"token keyword\\\"\u003edef\u003c/span\u003e \u003cspan class=\\\"token function\\\"\u003eis_done\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eself\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_result\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n        \u003cspan class=\\\"token comment\\\"\u003e# Go forever\u003c/span\u003e\\n        \u003cspan class=\\\"token keyword\\\"\u003ereturn\u003c/span\u003e \u003cspan class=\\\"token boolean\\\"\u003eFalse\u003c/span\u003e\\n\\n    \u003cspan class=\\\"token keyword\\\"\u003edef\u003c/span\u003e \u003cspan class=\\\"token function\\\"\u003epick_action\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eself\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e observations\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_list\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n        \u003cspan class=\\\"token comment\\\"\u003e# Rotates on the spot indefinitely, 5 degrees at a time\u003c/span\u003e\\n        \u003cspan class=\\\"token comment\\\"\u003e# (assumes we are running in passive mode)\u003c/span\u003e\\n        \u003cspan class=\\\"token keyword\\\"\u003ereturn\u003c/span\u003e \u003cspan class=\\\"token string\\\"\u003e'move_angle'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token punctuation\\\"\u003e{\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'angle'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e \u003cspan class=\\\"token number\\\"\u003e5\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e}\u003c/span\u003e\\n\\n    \u003cspan class=\\\"token keyword\\\"\u003edef\u003c/span\u003e \u003cspan class=\\\"token function\\\"\u003esave_result\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eself\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e filename\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e empty_results\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e results_format_fns\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n        \u003cspan class=\\\"token comment\\\"\u003e# Save some blank results\u003c/span\u003e\\n        \u003cspan class=\\\"token keyword\\\"\u003ewith\u003c/span\u003e \u003cspan class=\\\"token builtin\\\"\u003eopen\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003efilename\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token string\\\"\u003e'w'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e \u003cspan class=\\\"token keyword\\\"\u003eas\u003c/span\u003e f\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n            json\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003edump\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eempty_results\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e f\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eIf you prefer to do things manually, a more exhaustive suite of functions are also available as part of the BenchBot API. Instead of using the \u003ccode class=\\\"language-none\\\"\u003eBenchBot.run()\u003c/code\u003e method, a large number of methods are available through the API. Below highlights a handful of the capabilities of BenchBot API:\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_api \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e BenchBot\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e RESULT_LOCATION\\n\u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e json\\n\u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e matplotlib\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003epyplot \u003cspan class=\\\"token keyword\\\"\u003eas\u003c/span\u003e plt\\n\\n\u003cspan class=\\\"token comment\\\"\u003e# Create a BenchBot instance \u0026amp; reset the simulator / robot to starting state\u003c/span\u003e\\nb \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e BenchBot\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\nobservations\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_result \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003ereset\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\\n\u003cspan class=\\\"token comment\\\"\u003e# Print details of selected task \u0026amp; environment\u003c/span\u003e\\n\u003cspan class=\\\"token keyword\\\"\u003eprint\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eb\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003etask_details\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003cspan class=\\\"token keyword\\\"\u003eprint\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eb\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eenvironment_details\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\\n\u003cspan class=\\\"token comment\\\"\u003e# Visualise the current RGB image from the robot\u003c/span\u003e\\nplt\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eimshow\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eobservations\u003cspan class=\\\"token punctuation\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'image_rgb'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e]\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\\n\u003cspan class=\\\"token comment\\\"\u003e# Move to the next pose if we have a 'move_next' action available\u003c/span\u003e\\n\u003cspan class=\\\"token keyword\\\"\u003eif\u003c/span\u003e \u003cspan class=\\\"token string\\\"\u003e'move_next'\u003c/span\u003e \u003cspan class=\\\"token keyword\\\"\u003ein\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eactions\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n    observations\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_result \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003estep\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'move_next'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\\n\u003cspan class=\\\"token comment\\\"\u003e# Save some empty results\u003c/span\u003e\\n\u003cspan class=\\\"token keyword\\\"\u003ewith\u003c/span\u003e \u003cspan class=\\\"token builtin\\\"\u003eopen\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eRESULT_LOCATION\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token string\\\"\u003e'w'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e \u003cspan class=\\\"token keyword\\\"\u003eas\u003c/span\u003e f\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e\\n    json\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003edump\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eb\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eempty_results\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e f\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eFor sample solutions that use the BenchBot API, see the examples add-ons available (e.g. \u003ca href=https://github.com/benchbot-addons/examples_base\u003e\u003ccode class=\\\"language-none\\\"\u003ebenchbot-addons/examples_base\u003c/code\u003e\u003c/a\u003e and \u003ca href=https://github.com/benchbot-addons/examples_ssu\u003e\u003ccode class=\\\"language-none\\\"\u003ebenchbot-addons/examples_ssu\u003c/code\u003e\u003c/a\u003e).\u003c/p\u003e\\n\u003ch2\u003eInstalling BenchBot API\u003c/h2\u003e\\n\u003cp\u003eBenchBot API is a Python package, installable with pip. Run the following in the root directory of where this repository was cloned:\u003c/p\u003e\\n\u003cpre class=\\\"language-none\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eu@pc:~$ pip install .\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch2\u003eUsing the API to communicate with a robot\u003c/h2\u003e\\n\u003cp\u003eCommunication with the robot comes through a series of \u0026quot;channels\u0026quot; which are defined by the robot's definition file (e.g. \u003ca href=https://github.com/benchbot-addons/robots_isaac/blob/master/robots/carter.yaml\u003ecarter\u003c/a\u003e). A task definition file (e.g. \u003ca href=https://github.com/benchbot-addons/tasks_ssu/blob/master/tasks/sslam_pgt.yaml\u003esemantic_slam:passive:ground_truth\u003c/a\u003e) then declares which of these connections are provided to the API as either sensor observations or actions to be executed by a robot actuator.\u003c/p\u003e\\n\u003cp\u003eThe API talks to the \u003ca href=https://github.com/qcr/benchbot_supervisor\u003eBenchBot Supervisor\u003c/a\u003e, which handles loading and managing the different kinds of back-end configuration files. This abstracts all of the underlying communication complexities away from the user, allowing the BenchBot API to remain a simple interface that focuses on getting observations and sending actions.\u003c/p\u003e\\n\u003cp\u003eAn action is sent to the robot by calling the \u003ccode class=\\\"language-none\\\"\u003eBenchBot.step()\u003c/code\u003e method with a valid action (found by checking the \u003ccode class=\\\"language-none\\\"\u003eBenchBot.actions\u003c/code\u003e property):\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_api \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e BenchBot\\n\\nb \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e BenchBot\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\navailable_actions \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eactions\\nb\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003estep\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003eb\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003eactions\u003cspan class=\\\"token punctuation\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"token number\\\"\u003e0\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e]\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token punctuation\\\"\u003e{\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'action_arg:'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e arg_value\u003cspan class=\\\"token punctuation\\\"\u003e}\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e  \u003cspan class=\\\"token comment\\\"\u003e# Perform the first available action\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe second parameter is a dictionary of named arguments for the selected action. For example, moving 5m forward with the \u003ccode class=\\\"language-none\\\"\u003e'move_distance'\u003c/code\u003e action is represented by the dictionary \u003ccode class=\\\"language-none\\\"\u003e{'distance': 5}\u003c/code\u003e.\u003c/p\u003e\\n\u003cp\u003eObservations lists are received as return values from a \u003ccode class=\\\"language-none\\\"\u003eBenchBot.step()\u003c/code\u003e call (\u003ccode class=\\\"language-none\\\"\u003eBenchBot.reset()\u003c/code\u003e internally calls \u003ccode class=\\\"language-none\\\"\u003eBenchBot.step(None)\u003c/code\u003e, which means don't perform an action):\u003c/p\u003e\\n\u003cpre class=\\\"language-python\\\"\u003e\u003ccode class=\\\"language-python\\\"\u003e\u003cspan class=\\\"token keyword\\\"\u003efrom\u003c/span\u003e benchbot_api \u003cspan class=\\\"token keyword\\\"\u003eimport\u003c/span\u003e BenchBot\\n\\nb \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e BenchBot\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\nobservations\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_result \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003ereset\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\nobservations\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e action_result \u003cspan class=\\\"token operator\\\"\u003e=\u003c/span\u003e b\u003cspan class=\\\"token punctuation\\\"\u003e.\u003c/span\u003estep\u003cspan class=\\\"token punctuation\\\"\u003e(\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'move_distance'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e,\u003c/span\u003e \u003cspan class=\\\"token punctuation\\\"\u003e{\u003c/span\u003e\u003cspan class=\\\"token string\\\"\u003e'distance'\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e:\u003c/span\u003e \u003cspan class=\\\"token number\\\"\u003e5\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e}\u003c/span\u003e\u003cspan class=\\\"token punctuation\\\"\u003e)\u003c/span\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003eThe returned \u003ccode class=\\\"language-none\\\"\u003eobservations\u003c/code\u003e variable holds a dictionary with key-value pairs corresponding to the name-data defined by each observation channel.\u003c/p\u003e\\n\u003cp\u003eThe \u003ccode class=\\\"language-none\\\"\u003eaction_result\u003c/code\u003e is an enumerated value denoting the result of the action (use \u003ccode class=\\\"language-none\\\"\u003efrom benchbot_api import ActionResult\u003c/code\u003e to access the \u003ccode class=\\\"language-none\\\"\u003eEnum\u003c/code\u003e class). You should use this result to guide the progression of your algorithm either manually or in the \u003ccode class=\\\"language-none\\\"\u003eis_done()\u003c/code\u003e method of your \u003ccode class=\\\"language-none\\\"\u003eAgent\u003c/code\u003e. Possible values for the returned \u003ccode class=\\\"language-none\\\"\u003eaction_result\u003c/code\u003e are:\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003eActionResult.SUCCESS\u003c/code\u003e: the action was carried out successfully\u003c/li\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003eActionResult.FINISHED\u003c/code\u003e: the action was carried out successfully, and the robot is now finished its traversal through the scene (only used in \u003ccode class=\\\"language-none\\\"\u003epassive\u003c/code\u003e actuation mode)\u003c/li\u003e\\n\u003cli\u003e\u003ccode class=\\\"language-none\\\"\u003eActionResult.COLLISION\u003c/code\u003e: the action crashed the robot into an obstacle, and as a result it will not respond to any further actuation commands (at this point you should quit)\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003ch3\u003eStandard Communication Channels\u003c/h3\u003e\\n\u003cp\u003eTasks and robot definition files declare actions and observations, and these files are include through \u003ca href=https://github.com/qcr/benchbot_addons\u003eBenchBot add-ons\u003c/a\u003e. The add-on creator is free to add and declare channels as they please, but it is a better experience for all if channel definitions are as consistent as possible across the BenchBot ecosystem.\u003c/p\u003e\\n\u003cp\u003eSo if you're adding a robot that move between a set of poses, declare a channel called \u003ccode class=\\\"language-none\\\"\u003e'move_next\u003c/code\u003e with no arguments. Likewise, a robot that receives image observations should use a channel named \u003ccode class=\\\"language-none\\\"\u003e'image_rgb'\u003c/code\u003e with the same format as described below. Feel free to implement the channels however you please for your robot, but consistent interfaces should always be preferred.\u003c/p\u003e\\n\u003cp\u003eIf you encounter a task using non-standard channel configurations, the API has all the functionality you need as a user to handle them (\u003ccode class=\\\"language-none\\\"\u003eactions\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003econfig\u003c/code\u003e, \u0026amp; \u003ccode class=\\\"language-none\\\"\u003eobservations\u003c/code\u003e properties). On the other hand, maybe the non-standard channel should be a new standard. New standard communication channels are always welcome; please open a pull request with the details!\u003c/p\u003e\\n\u003ch4\u003eStandard action channels:\u003c/h4\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eName\u003c/th\u003e\\n\u003cth style=\\\"text-align:center\\\"\u003eRequired Arguments\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'move_next'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:center\\\"\u003e\u003ccode class=\\\"language-none\\\"\u003eNone\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eMoves the robot to the next pose in its list of pre-defined poses (only available in environments that declare a \u003ccode class=\\\"language-none\\\"\u003e'trajectory_poses'\u003c/code\u003e field).\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'move_distance'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:center\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{'distance': float}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eMoves the robot \u003ccode class=\\\"language-none\\\"\u003e'distance'\u003c/code\u003e metres directly ahead.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'move_angle'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:center\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{'angle': float}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eRotate the angle on the spot by \u003ccode class=\\\"language-none\\\"\u003e'angle'\u003c/code\u003e degrees.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003ch4\u003eStandard observation channels:\u003c/h4\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eName\u003c/th\u003e\\n\u003cth style=\\\"text-align:left\\\"\u003eData format\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'image_depth'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003enumpy.ndarray(shape=(H,W),\u003cbr\u003e dtype='float32')\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eDepth image from the default image sensor with depths in meters.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'image_depth_info'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{\u003cbr\u003e 'frame_id': string\u003cbr\u003e 'height': int\u003cbr\u003e 'width': int\u003cbr\u003e 'matrix_instrinsics':\u003cbr\u003e numpy.ndarray(shape=(3,3),\u003cbr\u003e dtype='float64')\u003cbr\u003e'matrix_projection':\u003cbr\u003e numpy.ndarray(shape=(3,4)\u003cbr\u003e dtype='float64')\u003cbr\u003e}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eSensor information for the depth image. \u003ccode class=\\\"language-none\\\"\u003e'matrix_instrinsics'\u003c/code\u003e is of the format:\u003cbr\u003e\u003cpre class=\\\"language-none\\\"\u003e[fx 0 cx]\u003cbr\u003e[0 fy cy]\u003cbr\u003e[0 0 1]\u003c/pre\u003e for a camera with focal lengths \u003ccode class=\\\"language-none\\\"\u003e(fx,fy)\u003c/code\u003e, \u0026amp; principal point \u003ccode class=\\\"language-none\\\"\u003e(cx,cy)\u003c/code\u003e. Likewise, \u003ccode class=\\\"language-none\\\"\u003e'matrix_projection'\u003c/code\u003e is:\u003cbr\u003e\u003cpre class=\\\"language-none\\\"\u003e[fx 0 cx Tx]\u003cbr\u003e[0 fy cy Ty]\u003cbr\u003e[0 0 1 0]\u003c/pre\u003ewhere \u003ccode class=\\\"language-none\\\"\u003e(Tx,Ty)\u003c/code\u003e is the translation between stereo sensors. See \u003ca href=\\\"http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html\\\"\u003ehere\u003c/a\u003e for further information on fields.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'image_rgb'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003enumpy.ndarray(shape=(H,W,3),\u003cbr\u003e dtype='uint8')\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eRGB image from the default image sensor with colour values mapped to the 3 channels, in the 0-255 range.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'image_rgb_info'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{\u003cbr\u003e 'frame_id': string\u003cbr\u003e 'height': int\u003cbr\u003e 'width': int\u003cbr\u003e 'matrix_instrinsics':\u003cbr\u003e numpy.ndarray(shape=(3,3),\u003cbr\u003e dtype='float64')\u003cbr\u003e'matrix_projection':\u003cbr\u003e numpy.ndarray(shape=(3,4)\u003cbr\u003e dtype='float64')\u003cbr\u003e}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eSensor information for the RGB image. \u003ccode class=\\\"language-none\\\"\u003e'matrix_instrinsics'\u003c/code\u003e is of the format:\u003cbr\u003e\u003cpre class=\\\"language-none\\\"\u003e[fx 0 cx]\u003cbr\u003e[0 fy cy]\u003cbr\u003e[0 0 1]\u003c/pre\u003e for a camera with focal lengths \u003ccode class=\\\"language-none\\\"\u003e(fx,fy)\u003c/code\u003e, \u0026amp; principal point \u003ccode class=\\\"language-none\\\"\u003e(cx,cy)\u003c/code\u003e. Likewise, \u003ccode class=\\\"language-none\\\"\u003e'matrix_projection'\u003c/code\u003e is:\u003cbr\u003e\u003cpre class=\\\"language-none\\\"\u003e[fx 0 cx Tx]\u003cbr\u003e[0 fy cy Ty]\u003cbr\u003e[0 0 1 0]\u003c/pre\u003ewhere \u003ccode class=\\\"language-none\\\"\u003e(Tx,Ty)\u003c/code\u003e is the translation between stereo sensors. See \u003ca href=\\\"http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html\\\"\u003ehere\u003c/a\u003e for further information on fields.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'laser'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{\u003cbr\u003e 'range_max': float64,\u003cbr\u003e 'range_min': float64,\u003cbr\u003e 'scans':\u003cbr\u003e numpy.ndarray(shape=(N,2),\u003cbr\u003e dtype='float64')\u003cbr\u003e}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eSet of scan values from a laser sensor, between \u003ccode class=\\\"language-none\\\"\u003e'range_min'\u003c/code\u003e \u0026amp; \u003ccode class=\\\"language-none\\\"\u003e'range_max'\u003c/code\u003e (in meters). The \u003ccode class=\\\"language-none\\\"\u003e'scans'\u003c/code\u003e array consists of \u003ccode class=\\\"language-none\\\"\u003eN\u003c/code\u003e scans of format \u003ccode class=\\\"language-none\\\"\u003e[scan_angle, scan_value]\u003c/code\u003e. For example, \u003ccode class=\\\"language-none\\\"\u003escans[100,0]\u003c/code\u003e would get the angle of the 100th scan \u0026amp; \u003ccode class=\\\"language-none\\\"\u003escans[100,1]\u003c/code\u003e would get the distance value.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003e'poses'\u003c/code\u003e\u003c/td\u003e\\n\u003ctd style=\\\"text-align:left\\\"\u003e\u003cpre class=\\\"language-none\\\"\u003e{\u003cbr\u003e ...\u003cbr\u003e 'frame_name': {\u003cbr\u003e 'parent_frame': string\u003cbr\u003e 'rotation_rpy':\u003cbr\u003e numpy.ndarray(shape=(3,),\u003cbr\u003e dtype='float64')\u003cbr\u003e 'rotation_xyzw':\u003cbr\u003e numpy.ndarray(shape=(4,),\u003cbr\u003e dtype='float64')\u003cbr\u003e 'translation_xyz':\u003cbr\u003e numpy.ndarray(shape=(3,),\u003cbr\u003e dtype='float64')\u003cbr\u003e }\u003cbr\u003e ...\u003cbr\u003e}\u003c/pre\u003e\u003c/td\u003e\\n\u003ctd\u003eDictionary of relative poses for the current system state. The pose of each system component is available at key \u003ccode class=\\\"language-none\\\"\u003e'frame_name'\u003c/code\u003e. Each pose has a \u003ccode class=\\\"language-none\\\"\u003e'parent_frame'\u003c/code\u003e which the pose is relative to (all poses are typically with respect to global \u003ccode class=\\\"language-none\\\"\u003e'map'\u003c/code\u003e frame), \u0026amp; the pose values. \u003ccode class=\\\"language-none\\\"\u003e'rotation_rpy'\u003c/code\u003e is \u003ccode class=\\\"language-none\\\"\u003e[roll,pitch,yaw]\u003c/code\u003e, \u003ccode class=\\\"language-none\\\"\u003e'rotation_xyzw'\u003c/code\u003e is the equivalent quaternion \u003ccode class=\\\"language-none\\\"\u003e[x,y,z,w]\u003c/code\u003e, \u0026amp; \u003ccode class=\\\"language-none\\\"\u003e'translation_xyz'\u003c/code\u003e is the Cartesion \u003ccode class=\\\"language-none\\\"\u003e[x,y,z]\u003c/code\u003e coordinates.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003ch2\u003eUsing the API to communicate with the BenchBot system\u003c/h2\u003e\\n\u003cp\u003eA running BenchBot system manages many other elements besides simply getting data to and from a real / simulated robot. BenchBot encapsulates not just the robot, but also the environment it is operating in (whether that be simulator or real) and task that is currently being attempted.\u003c/p\u003e\\n\u003cp\u003eThe API handles communication for all parts of the BenchBot system, including controlling the currently running environment and obtaining configuration information. Below are details for some of the more useful features of the API (all features are also documented in the \u003ca href=https:/github.com/qcr/benchbot_api/blob/master/benchbot_api/benchbot.py\u003e\u003ccode class=\\\"language-none\\\"\u003ebenchbot.py\u003c/code\u003e\u003c/a\u003e source code).\u003c/p\u003e\\n\u003ch3\u003eGathering configuration information\u003c/h3\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eAPI method or property\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003econfig\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eReturns a \u003ccode class=\\\"language-none\\\"\u003edict\u003c/code\u003e exhaustively describing the current BenchBot configuration. Most of the information returned will not be useful for general BenchBot use.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003ch3\u003eInteracting with the environment\u003c/h3\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eAPI method or property\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003ereset()\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eResets the current environment scene. For the simulator, this means restarting the running simulator instance with the robot back at its initial position. The method returns initial \u003ccode class=\\\"language-none\\\"\u003eobservations\u003c/code\u003e, \u0026amp; the \u003ccode class=\\\"language-none\\\"\u003eaction_result\u003c/code\u003e (should always be \u003ccode class=\\\"language-none\\\"\u003eBenchBot.ActionResult.SUCCESS\u003c/code\u003e).\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003enext_scene()\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eStarts the next scene in the current environment (only relevant for tasks with multiple scenes). Note there is no going back once you have moved to the next scene. Returns the same as \u003ccode class=\\\"language-none\\\"\u003ereset()\u003c/code\u003e.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003ch3\u003eInteracting with an agent\u003c/h3\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eAPI method or property\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003eactions\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eReturns the list of actions currently available to the agent. This will update as actions are performed in the environment (for example if the agent has collided with an obstacle this list will be empty).\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003eobservations\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eReturns the lists of observations available to the agent.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003estep(action, **action_args)\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003ePerforms the requested action with the provided named action arguments. See \u003ca href=https:/github.com/qcr/benchbot_api/#using-the-api-to-communicate-with-a-robot\u003eUsing the API to communicate with a robot\u003c/a\u003e above for further details.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003ch3\u003eCreating results\u003c/h3\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003cth\u003eAPI method or property\u003c/th\u003e\\n\u003cth\u003eDescription\u003c/th\u003e\\n\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003eempty_results()\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eGenerates a \u003ccode class=\\\"language-none\\\"\u003edict\u003c/code\u003e of with required result metadata \u0026amp; empty results. Metadata (\u003ccode class=\\\"language-none\\\"\u003e'task_details'\u003c/code\u003e \u0026amp; \u003ccode class=\\\"language-none\\\"\u003e'environment_details'\u003c/code\u003e) is pre-filled. To create results, all a user needs to do is fill in the empty \u003ccode class=\\\"language-none\\\"\u003e'results'\u003c/code\u003e field using format's results functions. These functions are available through the \u003ccode class=\\\"language-none\\\"\u003e'results_functions()\u003c/code\u003e method.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003eresults_functions()\u003c/code\u003e\u003c/td\u003e\\n\u003ctd\u003eReturns a \u003ccode class=\\\"language-none\\\"\u003edict\u003c/code\u003e of functions defined by the task's \u003ccode class=\\\"language-none\\\"\u003e'results_format'\u003c/code\u003e. Example use for calling a \u003ccode class=\\\"language-none\\\"\u003ecreate()\u003c/code\u003e function is \u003ccode class=\\\"language-none\\\"\u003eresults_functions()['create']()\u003c/code\u003e.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003ccode class=\\\"language-none\\\"\u003eRESULT_LOCATION\u003c/code\u003e (outside of \u003ccode class=\\\"language-none\\\"\u003eBenchBot\u003c/code\u003e class)\u003c/td\u003e\\n\u003ctd\u003eA static string denoting where results should be saved (\u003ccode class=\\\"language-none\\\"\u003e/tmp/results\u003c/code\u003e). Using this locations ensures tools in the \u003ca href=https://github.com/qcr/benchbot\u003eBenchBot software stack\u003c/a\u003e work as expected.\u003c/td\u003e\\n\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\",\"name\":\"BenchBot Python API\",\"type\":\"code\",\"url\":\"https://github.com/qcr/benchbot_api\",\"image_position\":\"center 100%\",\"src\":\"/content/benchbot/benchbot-api.md\",\"id\":\"benchbot-api\",\"image\":\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.jpg\",\"_image\":\"/_next/static/gifs/566ece96917dbb84cd386109b8457390.webm\"}"},"__N_SSG":true},"page":"/code/[code]","query":{"code":"benchbot-api"},"buildId":"1nuS-y2A2y9fnoeaTjLIs","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c439d75cfca1ce6a0f7f.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.a6402fb70cc88f6f61b0.js" async=""></script><script src="/_next/static/chunks/commons.455c36b53add9c9c2736.js" async=""></script><script src="/_next/static/chunks/pages/_app-d07085bfa8b88c39a473.js" async=""></script><script src="/_next/static/chunks/3d04e185781834a6bdd2cdc78a14cbdede4fee55.e5e850c413858c1cae6e.js" async=""></script><script src="/_next/static/chunks/pages/code/%5Bcode%5D-855919f7a7c9635db067.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_buildManifest.js" async=""></script><script src="/_next/static/1nuS-y2A2y9fnoeaTjLIs/_ssgManifest.js" async=""></script></body></html>