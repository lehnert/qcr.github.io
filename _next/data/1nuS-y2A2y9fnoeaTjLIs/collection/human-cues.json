{"pageProps":{"collectionData":"{\"content\":\"<p>The Human Cues for Robot Navigation ARC Discovery Project (DP140103216) investigated how a robot can navigate using the same navigation cues humans use when navigating built environments. Types of navigation cues targeted include labels, directional signs, signboards, maps &amp; floor plans, navigational gestures, and spoken directions &amp; descriptions. The main contribution from this work is the abstract map, a navigational tool that allows a robot to employ symbolic spatial information in its navigation of unseen spaces.</p>\\n\",\"name\":\"Human Cues for Robot Navigation\",\"type\":\"collection\",\"url\":\"https://btalb.github.io/abstract_map\",\"code\":[{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>The Abstract Map - using symbols to navigate</h1>\\n<p><img src=\\\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\\\" alt=\\\"The abstract map in action\\\"></p>\\n<p>This repository provides the implementation of the abstract map used in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The implementation, done in Python, includes the following features:</p>\\n<ul>\\n<li>a novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, &amp; collision models)</li>\\n<li>a visualiser &amp; text-based commentator for introspection of your navigation system (both shown in videos on the <a href=\\\"https://btalb.github.io/abstract_map/\\\">repository website</a>)</li>\\n<li>easy ROS bindings for getting up &amp; running in simulation or on a real robot</li>\\n<li>tag readers &amp; interpreters for extracting symbolic spatial information from <a href=\\\"http://wiki.ros.org/apriltag_ros\\\">AprilTags</a></li>\\n<li>configuration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">the paper</a> for further details)</li>\\n<li>serialisation methods for passing an entire abstract map state between machines, or saving to file</li>\\n</ul>\\n<p>Please see our other related repositories for further resources, and related parts of the abstract map studies:</p>\\n<ul>\\n<li><strong><a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a>:</strong> all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT</li>\\n<li><strong><a href=https://github.com/btalb/abstract_map_app>abstract_map_app</a>:</strong> mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags &amp; present the mapped symbolic spatial information in real time)</li>\\n</ul>\\n<h2>Getting up &amp; running with the abstract map</h2>\\n<p><em>Note: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the <a href=https://github.com/btalb/abstract_map_simulator>abstract_map_simulator</a> package</em></p>\\n<h3>Setting up your environment</h3>\\n<p>Clone the repo &amp; install all Python dependencies:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/btalb/abstract_map\\npip install -r abstract_map/requirements.txt\\n</code></pre>\\n<p>Add the new package to your ROS workspace at <code class=\\\"language-none\\\">&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/\\n</code></pre>\\n<p>Install all of the listed ROS dependencies, and build the package:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">cd &lt;ROS_WS&gt;/src/\\nrosdep install abstract_map\\ncd &lt;ROS_WS&gt;\\ncatkin_make\\n</code></pre>\\n<h3>Running the Zoo experiments</h3>\\n<p>Start the experiment (this will try &amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">roslaunch abstract_map experiment.launch\\n</code></pre>\\n<p><em>(please see <a href=https://github.com/btalb/abstract_map_simulator/issues/1>this issue</a> for details if you get the spam of TF based errors... which probably shouldn't even be errors... )</em></p>\\n<p>In another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map hierarchy_publisher\\n</code></pre>\\n<p>This will use the hierarchy available in <code class=\\\"language-none\\\">./experiments/zoo_hierarchy.xml</code> by default. Feel free to make your own if you would like to do different experiments.</p>\\n<p>Start the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map visualiser\\n</code></pre>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">rosrun abstract_map visualiser --dark\\n</code></pre>\\n<p><img src=\\\"/_next/static/images/abstract_map_light_vs_dark-ae93c3e7b8419b56719b5d876dd150f4.png\\\" alt=\\\"Visualise the abstract map with dark or light colours\\\"></p>\\n<p>Finally, start the abstract map with a goal, and watch it attempt to complete the navigation task:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">roslaunch abstract_map abstract_map.launch goal:=Lion\\n</code></pre>\\n<p>If you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in &quot;observe mode&quot;.</p>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"Abstract Map (Python)\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map\",\"src\":\"/content/human_cues/abstract-map.md\",\"id\":\"abstract-map\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"},{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>Using the Abstract Map in a 2D Stage simulation</h1>\\n<p><img src=\\\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\\\" alt=\\\"2D Stage simulation with with simulated tags\\\"></p>\\n<p>Package contains everything needed to simulate the zoo experiments performed in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The package includes:</p>\\n<ul>\\n<li>world &amp; launch files for a stage simulation of the GP-S11 environment on QUT's Gardens Point campus</li>\\n<li>a tool for creating simulated tags in an environment &amp; saving them to file,</li>\\n<li>launch &amp; config files for using the move_base navigation stack with gmapping to explore unseen simulated environments</li>\\n</ul>\\n<h2>Installing the abstract map simulator</h2>\\n<p><em>Note: this is just the simulator; to use the abstract map with the simulator please make sure you use the <a href=https://github.com/btalb/abstract_map>abstract_map</a> package</em></p>\\n<p>Clone the repo &amp; install all Python dependencies:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">git clone https://github.com/btalb/abstract_map_simulator\\npip install -r abstract_map_simulator/requirements.txt\\n</code></pre>\\n<p>Add the new package to your ROS workspace at <code class=\\\"language-none\\\">&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/\\n</code></pre>\\n<p>Install all of the listed ROS dependencies, and build the package:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">cd &lt;ROS_WS&gt;/src/\\nrosdep install abstract_map_simulator\\ncd &lt;ROS_WS&gt;\\ncatkin_make\\n</code></pre>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"2D Simulator for Zoo Experiments\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_simulator\",\"src\":\"/content/human_cues/abstract-map-simulator.md\",\"id\":\"abstract-map-simulator\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_simulation-8e1275a3c88423d73d8d661443eeefdf.png\"},{\"content\":\"<p align=center><strong>~ Please see the <a href=\\\"https://btalb.github.io/abstract_map/\\\">abstract map site</a> for further details about the research publication ~</strong></p>\\n<h1>App for the Human vs Abstract Map Zoo Experiments</h1>\\n<p><video autoplay loop poster=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\\\"><source src=\\\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\\\" type=\\\"video/webm\\\"/></video></p>\\n<p>This repository contains the mobile application used by human participants in the zoo experiments described in our <a href=\\\"https://doi.org/10.1109/TCDS.2020.2993855\\\">IEEE TCDS journal</a>. The app, created with Android Studio, includes the following:</p>\\n<ul>\\n<li>opening screen for users to select experiment name &amp; goal location</li>\\n<li>live display of the camera to help users correctly capture a tag</li>\\n<li>instant visual feedback when a tag is detected, with colouring to denote whether symbolic spatial information is not the goal (red), navigation information (orange), or the goal (green)</li>\\n<li>experiment definitions &amp; tag mappings are creatable via the same XML style used in the <a href=https://github.com/btalb/abstract_map>abstract_map</a> package</li>\\n<li>integration with the <a href=https://github.com/AprilRobotics/apriltag>native C AprilTags</a> using the Android NDK</li>\\n</ul>\\n<h2>Developing &amp; producing the app</h2>\\n<p>The project should be directly openable using Android Studio.</p>\\n<p>Please keep in mind that this app was last developed in 2019, and Android Studio often introduces minor breaking changes with new versions. Often you will have to tweak things like Gradle versions / syntax etc. to get a project working with newer versions. Android Studio is very good though with pointing out where it sees errors and offering suggestions for how to resolve them.</p>\\n<p>Once you have the project open, you should be able to compile the app and load it directly onto a device without issues.</p>\\n<h2>Acknowledgements &amp; Citing our work</h2>\\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\\\"https://research.qut.edu.au/qcr/\\\">QUT Centre for Robotics</a>.</p>\\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\\n<pre class=\\\"language-none\\\"><code class=\\\"language-none\\\">@ARTICLE{9091567,  \\n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \\n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \\n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \\n    year={2020},  \\n    volume={},  \\n    number={},  \\n    pages={1-1},\\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\\n    doi={10.1109/TCDS.2020.2993855},\\n    ISSN={2379-8939},\\n    month={},}\\n}\\n</code></pre>\\n\",\"name\":\"Android App for Human Participants\",\"type\":\"code\",\"url\":\"https://github.com/btalb/abstract_map_app\",\"src\":\"/content/human_cues/abstract-map-app.md\",\"id\":\"abstract-map-app\",\"image_position\":\"center\",\"image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.jpg\",\"_image\":\"/_next/static/gifs/1e64d3716d81982074296f38479c3ae2.webm\"}],\"feature\":0,\"src\":\"/content/human_cues/human-cues.md\",\"id\":\"human-cues\",\"image_position\":\"center\",\"image\":\"/_next/static/images/abstract_map_in_action-261a2f7eea1f79411b48203e72995f14.png\"}"},"__N_SSG":true}